Folder: D:\Documents\E_Plus_2030_py\aiaia

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cali_1

  File: calibration_manager.py
  --- File Contents Start ---
# calibration_manager.py

import random
import numpy as np
import pandas as pd
from typing import Callable, Dict, Any, List, Union, Tuple

# Attempt to import scikit-optimize
try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer
    HAVE_SKOPT = True
except ImportError:
    gp_minimize = None
    Real = None
    Integer = None
    HAVE_SKOPT = False


# -----------------------------------------------------------------------------
# A) Data Structures
# -----------------------------------------------------------------------------
class ParameterSpec:
    """
    Holds metadata for a single parameter:
      - name: the parameter identifier
      - min_value, max_value: numeric range
      - is_active: if this parameter is used in calibration
      - is_integer: whether the parameter should be integer-valued
      - categories: for categorical parameters (not fully implemented here)
    """
    def __init__(self, 
                 name: str, 
                 min_value: float,
                 max_value: float,
                 is_active: bool = True,
                 is_integer: bool = False,
                 categories: List[str] = None):
        self.name = name
        self.min_value = min_value
        self.max_value = max_value
        self.is_active = is_active
        self.is_integer = is_integer
        self.categories = categories  # For categorical or enumerated values

    def sample_random(self) -> float:
        """
        Returns a random sample in [min_value, max_value].
        If is_integer=True, cast the result to int.
        If categories exist, picks from categories (not fully used here).
        """
        if self.categories is not None:
            return random.choice(self.categories)

        val = random.uniform(self.min_value, self.max_value)
        if self.is_integer:
            val = int(round(val))
        return val


class ParameterSet:
    """
    Represents a set of parameter values, stored as {param_name: numeric_value}.
    """
    def __init__(self, values: Dict[str, float]):
        self.values = values  # e.g. {"infiltration": 1.2, "occupancy": 25.0, ...}

    def copy(self):
        return ParameterSet(dict(self.values))

    def __repr__(self):
        return f"ParameterSet({self.values})"


# -----------------------------------------------------------------------------
# B) Helper Functions
# -----------------------------------------------------------------------------
def create_param_specs_from_df(df_params: pd.DataFrame,
                               active_only: bool = True) -> List[ParameterSpec]:
    """
    Convert a DataFrame with columns:
      'param_name', 'min_value', 'max_value', 'is_active', 'is_integer'
    into a list of ParameterSpec objects.
    """
    specs = []
    for _, row in df_params.iterrows():
        if active_only and (("is_active" in row) and (not row["is_active"])):
            continue

        name = row["param_name"]
        mn = row["min_value"]
        mx = row["max_value"]
        is_int = bool(row.get("is_integer", False))
        is_act = row.get("is_active", True)

        spec = ParameterSpec(
            name=name,
            min_value=mn,
            max_value=mx,
            is_active=is_act,
            is_integer=is_int
        )
        specs.append(spec)
    return specs


def sample_random_param_set(param_specs: List[ParameterSpec]) -> ParameterSet:
    """Generate a random ParameterSet from a list of ParameterSpec."""
    values = {}
    for spec in param_specs:
        if not spec.is_active:
            continue
        values[spec.name] = spec.sample_random()
    return ParameterSet(values)


def clamp_param_set(param_set: ParameterSet,
                    param_specs: List[ParameterSpec]) -> ParameterSet:
    """
    Ensure each value in param_set is within [min_value, max_value].
    If is_integer, round or cast to int.
    """
    new_values = dict(param_set.values)
    spec_dict = {s.name: s for s in param_specs}
    for p_name, p_value in new_values.items():
        spec = spec_dict.get(p_name, None)
        if spec:
            clamped = max(spec.min_value, min(spec.max_value, p_value))
            if spec.is_integer:
                clamped = int(round(clamped))
            new_values[p_name] = clamped
    return ParameterSet(new_values)


def evaluate_param_set(param_set: ParameterSet, 
                       evaluation_func: Callable[[Dict[str, float]], float]) -> float:
    """
    Evaluate a ParameterSet by passing its dict to user-defined 'evaluation_func',
    which returns a scalar error or cost (to MINIMIZE). e.g. CV(RMSE).
    """
    return evaluation_func(param_set.values)


# -----------------------------------------------------------------------------
# C) Random Search
# -----------------------------------------------------------------------------
def random_search(param_specs: List[ParameterSpec],
                  evaluation_func: Callable[[dict], float],
                  n_iterations: int = 50) -> Tuple[ParameterSet, float]:
    """
    Simple random search calibration:
      - Sample n_iterations random param sets
      - Evaluate objective
      - Return best
    """
    best_score = float('inf')
    best_set = None

    for _ in range(n_iterations):
        pset = sample_random_param_set(param_specs)
        score = evaluate_param_set(pset, evaluation_func)
        if score < best_score:
            best_score = score
            best_set = pset

    return best_set, best_score


# -----------------------------------------------------------------------------
# D) Genetic Algorithm (GA)
# -----------------------------------------------------------------------------
def ga_optimization(param_specs: List[ParameterSpec],
                    evaluation_func: Callable[[dict], float],
                    pop_size: int = 20,
                    max_generations: int = 10,
                    mutation_prob: float = 0.1) -> Tuple[ParameterSet, float]:
    """
    Basic GA approach:
      1. Initialize pop_size random param sets
      2. Evaluate each
      3. Repeat for max_generations:
         - Sort by score
         - Keep some elites
         - Reproduce via tournament selection & crossover
         - Mutate
      4. Return best
    """

    # 1) Initialize population
    population = []
    for _ in range(pop_size):
        pset = sample_random_param_set(param_specs)
        score = evaluate_param_set(pset, evaluation_func)
        population.append((pset, score))

    # 2) GA main loop
    for gen in range(max_generations):
        # Sort population by ascending score
        population.sort(key=lambda x: x[1])
        best_score = population[0][1]
        best_pset = population[0][0]
        print(f"[GA Generation {gen}] Best score: {best_score:.4f}")

        # Reproduce
        new_pop = []
        # keep top 2
        new_pop.extend(population[:2])

        # fill rest
        while len(new_pop) < pop_size:
            parent1 = tournament_select(population)
            parent2 = tournament_select(population)
            child_pset = crossover(parent1[0], parent2[0])
            child_pset = mutate(child_pset, param_specs, mutation_prob, evaluation_func)
            child_score = evaluate_param_set(child_pset, evaluation_func)
            new_pop.append((child_pset, child_score))

        population = new_pop

    # final best
    population.sort(key=lambda x: x[1])
    return population[0][0], population[0][1]


def tournament_select(population: List[Tuple[ParameterSet, float]], k: int = 3) -> Tuple[ParameterSet, float]:
    """Pick k random individuals, return best (lowest score)."""
    contenders = random.sample(population, k)
    contenders.sort(key=lambda x: x[1])
    return contenders[0]


def crossover(pset_a: ParameterSet, pset_b: ParameterSet) -> ParameterSet:
    """Uniform crossover: for each param, pick from parent A or B at 50% chance."""
    child_vals = {}
    for key in pset_a.values.keys():
        if random.random() < 0.5:
            child_vals[key] = pset_a.values[key]
        else:
            child_vals[key] = pset_b.values[key]
    return ParameterSet(child_vals)


def mutate(pset: ParameterSet,
           param_specs: List[ParameterSpec],
           mutation_prob: float,
           eval_func: Callable[[Dict[str, float]], float]) -> ParameterSet:
    """
    For each param, with probability mutation_prob, re-sample.
    Then clamp. 
    """
    new_vals = dict(pset.values)
    spec_dict = {s.name: s for s in param_specs}

    for key in new_vals.keys():
        if random.random() < mutation_prob:
            spec = spec_dict[key]
            new_vals[key] = spec.sample_random()

    child = ParameterSet(new_vals)
    child = clamp_param_set(child, param_specs)
    return child


# -----------------------------------------------------------------------------
# E) Bayesian (scikit-optimize)
# -----------------------------------------------------------------------------
def bayes_optimization(param_specs: List[ParameterSpec],
                       evaluation_func: Callable[[dict], float],
                       n_calls: int = 30) -> Tuple[ParameterSet, float]:
    """
    If scikit-optimize is installed, uses gp_minimize; else fallback to random_search.
    :return: (best_param_set, best_score)
    """
    if not HAVE_SKOPT:
        print("[WARN] scikit-optimize not installed. Fallback to random search.")
        return random_search(param_specs, evaluation_func, n_iterations=n_calls)

    # Build skopt space
    dimensions = []
    param_names = []
    for spec in param_specs:
        if spec.categories:
            raise NotImplementedError("Categorical not implemented in this snippet.")
        else:
            if spec.is_integer:
                dimensions.append(Integer(spec.min_value, spec.max_value, name=spec.name))
            else:
                dimensions.append(Real(spec.min_value, spec.max_value, name=spec.name))
        param_names.append(spec.name)

    def objective(params):
        # params is a list in same order as dimensions
        param_dict = {}
        for i, val in enumerate(params):
            param_dict[param_names[i]] = val

        pset = ParameterSet(param_dict)
        pset = clamp_param_set(pset, param_specs)
        score = evaluate_param_set(pset, evaluation_func)
        return score

    res = gp_minimize(objective, dimensions, n_calls=n_calls, random_state=42)
    best_score = res.fun
    best_vals = res.x
    best_dict = {}
    for i, val in enumerate(best_vals):
        best_dict[param_names[i]] = val
    best_pset = ParameterSet(best_dict)

    return best_pset, best_score


# -----------------------------------------------------------------------------
# F) Example Usage (Test)
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    # Example with 2D param space
    # f(x,y) = x^2 + y^2 -> minimum at (0,0)
    def test_eval_func(p: Dict[str, float]) -> float:
        return p["x"]**2 + p["y"]**2

    specs_demo = [
        ParameterSpec("x", -5, 5, True, False),
        ParameterSpec("y", -3, 3, True, False)
    ]

    # Random
    best_p_rand, best_score_rand = random_search(specs_demo, test_eval_func, n_iterations=30)
    print("[Random] best:", best_p_rand, "score:", best_score_rand)

    # GA
    best_p_ga, best_score_ga = ga_optimization(specs_demo, test_eval_func, pop_size=15, max_generations=5)
    print("[GA] best:", best_p_ga, "score:", best_score_ga)

    # Bayesian
    best_p_bayes, best_score_bayes = bayes_optimization(specs_demo, test_eval_func, n_calls=20)
    print("[Bayesian] best:", best_p_bayes, "score:", best_score_bayes)













  --- File Contents End ---

  File: main_calibration.py
  --- File Contents Start ---
# main_calibration.py

import pandas as pd

# Import from your calibration_manager module:
from cali_1.calibration_manager import (
    create_param_specs_from_df,
    random_search,
    ga_optimization,
    bayes_optimization
)

# Suppose you have a function that runs a Surrogate or direct E+ sim:
def evaluate_param_dict(param_dict):
    """
    This is your 'calibration' objective function.
    In practice, you'd do:
      1) Create/modify IDF from param_dict
      2) Run EnergyPlus or call surrogate
      3) Compute difference vs. real data => e.g. CV(RMSE)
      4) Return that scalar (lower is better)
    For demonstration, let's do a dummy function like sphere_obj.
    """
    x = param_dict.get("x", 0)
    y = param_dict.get("y", 0)
    # Suppose we want to minimize (x^2 + y^2)
    return x**2 + y**2


def main():
    # 1) Load or define a DataFrame describing your calibration params
    #    e.g. from "calibration_params.csv" with columns:
    #      param_name, min_value, max_value, is_active, is_integer
    data = {
        "param_name": ["x", "y"],
        "min_value": [-5, -3],
        "max_value": [5, 3],
        "is_active": [True, True],
        "is_integer": [False, False]
    }
    df_params = pd.DataFrame(data)

    # 2) Create param specs
    param_specs = create_param_specs_from_df(df_params, active_only=True)

    # 3) Choose which calibration approach:
    #    (A) random_search
    #    (B) ga_optimization
    #    (C) bayes_optimization
    #    We'll do random_search as an example:
    best_pset, best_score = random_search(param_specs, evaluate_param_dict, n_iterations=50)

    print("[Calibration - Random] Best:", best_pset, "Score:", best_score)

    # 4) If you prefer GA:
    best_pset_ga, best_score_ga = ga_optimization(param_specs, evaluate_param_dict, pop_size=20, max_generations=10)
    print("[Calibration - GA] Best:", best_pset_ga, "Score:", best_score_ga)

    # 5) Or Bayesian:
    best_pset_bayes, best_score_bayes = bayes_optimization(param_specs, evaluate_param_dict, n_calls=30)
    print("[Calibration - Bayesian] Best:", best_pset_bayes, "Score:", best_score_bayes)

    # 6) Optionally, store the best parameter set in CSV or JSON
    # e.g. save "calibrated_params.csv"
    best_params_dict = best_pset_bayes.values
    df_calibrated = pd.DataFrame([best_params_dict])
    df_calibrated.to_csv("calibrated_params.csv", index=False)
    print("[INFO] Saved best Bayesian calibration to calibrated_params.csv")


if __name__ == "__main__":
    main()

  --- File Contents End ---

  File: man.py
  --- File Contents Start ---
import os
from cali_1.sensitivity_main import run_sensitivity_analysis

def main():
    """
    This script shows how to call the `run_sensitivity_analysis` function
    from sensitivity_main.py. Update the scenario folder and methods as needed.
    """
    scenario_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"

    # Make sure the folder exists or adjust the path
    if not os.path.isdir(scenario_folder):
        print(f"[ERROR] Scenario folder does not exist: {scenario_folder}")
        return

    # Example: Running Morris analysis
    run_sensitivity_analysis(
        scenario_folder=scenario_folder,
        method="morris",
        output_csv="morris_sensitivity_results.csv"
    )

    # Example: Running Sobol analysis
    run_sensitivity_analysis(
        scenario_folder=scenario_folder,
        method="sobol",
        output_csv="sobol_sensitivity_results.csv"
    )

if __name__ == "__main__":
    main()



# man.py

import pandas as pd
from cali_1.surrogate_main import (
    train_aggregate_surrogate,
    load_aggregate_surrogate
)

if __name__ == "__main__":
    # Example usage

    # 1) Define paths
    scenario_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    results_csv = r"D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv"
    sensitivity_csv = "morris_sensitivity_results.csv"  # or None
    top_n = 5
    output_model_path = "my_aggregate_surrogate.pkl"

    # 2) Train & save
    train_aggregate_surrogate(
        scenario_folder=scenario_folder,
        results_csv=results_csv,
        sensitivity_csv=sensitivity_csv,
        top_n=top_n,
        output_model_path=output_model_path
    )

    # 3) Load the trained model
    model = load_aggregate_surrogate(output_model_path)

    # 4) Predict on new data (columns must match training columns!)
    new_params = pd.DataFrame({
        "exterior_wall_U_value": [0.38],
        "infiltration_base": [1.2],
        "occupant_density_m2_per_person": [28.0],
        "default_heater_capacity_w": [4000.0],
        "ground_floor_window_construction_name": [0]
    })

    predictions = model.predict(new_params)
    print("Predictions:", predictions)

  --- File Contents End ---

  File: sensitivity_main.py
  --- File Contents Start ---
import os
import numpy as np
import pandas as pd
from typing import Dict, Any

# SALib imports
from SALib.sample import saltelli, morris
from SALib.analyze import sobol, morris as morris_analyze

##############################################################################
# 1) Read and Merge Scenario Parameter CSVs
##############################################################################

def load_scenario_params(scenario_folder: str) -> pd.DataFrame:
    """
    Reads scenario CSV files (dhw, elec, fenez, hvac, vent, etc.)
    from the specified folder, merges them into one DataFrame
    with columns: [scenario_index, ogc_fid, param_name, assigned_value].
    
    Returns the merged DataFrame.
    """
    scenario_files = [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv",
        "scenario_params_fenez.csv",
        "scenario_params_hvac.csv",
        "scenario_params_vent.csv"
    ]

    dfs = []
    for fname in scenario_files:
        fpath = os.path.join(scenario_folder, fname)
        if os.path.isfile(fpath):
            df = pd.read_csv(fpath)
            df["source_file"] = fname  # keep track of which file or system
            dfs.append(df)
        else:
            print(f"[WARNING] File not found: {fpath}")

    if not dfs:
        raise FileNotFoundError("No scenario parameter CSVs found in the folder.")

    merged_df = pd.concat(dfs, ignore_index=True)
    return merged_df


##############################################################################
# 2) Extract Param Ranges (Enforce param_min < param_max)
##############################################################################

def extract_parameter_ranges(merged_df: pd.DataFrame,
                             param_min_col: str = "param_min",
                             param_max_col: str = "param_max") -> pd.DataFrame:
    """
    Creates a DataFrame with columns ['name', 'min_value', 'max_value'].
    If param_min/param_max do not exist (or are NaN / invalid),
    fallback logic uses assigned_value with +/- 20% or a small epsilon
    to ensure min < max.

    This is where we fix the "Bounds are not legal" error by forcing:
       min_value < max_value
    """
    # Check if the user-defined columns exist in the DataFrame
    has_param_min = (param_min_col in merged_df.columns)
    has_param_max = (param_max_col in merged_df.columns)

    # If param_min / param_max columns do NOT exist, fallback to assigned_value
    if not has_param_min or not has_param_max:
        print("[INFO] param_min/param_max columns not found. Using fallback approach...")

        param_names = merged_df["param_name"].unique()
        rows = []
        for p in param_names:
            subset = merged_df[merged_df["param_name"] == p]
            if subset.empty:
                continue

            # Just pick the first assigned_value we find
            val = subset.iloc[0].get("assigned_value", 0)
            if pd.isna(val):
                # If it's NaN, fallback to 1.0
                val = 1.0

            base_val = float(val)
            # 20% buffer
            minv = base_val * 0.8
            maxv = base_val * 1.2

            # If they end up equal (e.g. base_val=0 => both 0), ensure minv < maxv
            if minv >= maxv:
                maxv = minv + 1e-4  # enforce strictly less

            rows.append({"name": p, "min_value": minv, "max_value": maxv})

        return pd.DataFrame(rows)

    # If param_min/param_max DO exist, we group them by param_name
    grouped = merged_df.groupby("param_name")[[param_min_col, param_max_col]].first().reset_index()
    grouped.rename(columns={
        "param_name": "name",
        param_min_col: "min_value",
        param_max_col: "max_value"
    }, inplace=True)

    out_rows = []
    for _, row in grouped.iterrows():
        name = row["name"]
        minv = row["min_value"]
        maxv = row["max_value"]

        # Check validity
        if pd.isna(minv) or pd.isna(maxv) or (minv >= maxv):
            # fallback
            sample = merged_df.loc[merged_df["param_name"] == name, "assigned_value"]
            if not sample.empty and not pd.isna(sample.iloc[0]):
                val = float(sample.iloc[0])
                minv = val * 0.8
                maxv = val * 1.2
                if minv >= maxv:
                    maxv = minv + 1e-4
            else:
                # If we can't fix it, default to [0,1]
                minv, maxv = 0, 1

        out_rows.append({"name": name, "min_value": minv, "max_value": maxv})

    return pd.DataFrame(out_rows)


##############################################################################
# 3) Build SALib 'problem' dict
##############################################################################

def build_problem_dict(params_meta: pd.DataFrame) -> Dict[str, Any]:
    """
    Convert a DataFrame [name, min_value, max_value] into SALib 'problem' dict.
    """
    problem = {
        'num_vars': len(params_meta),
        'names': params_meta['name'].tolist(),
        'bounds': []
    }
    for _, row in params_meta.iterrows():
        problem['bounds'].append([row['min_value'], row['max_value']])

    return problem


##############################################################################
# 4) Mock or Real Simulation
##############################################################################

def eplus_simulation_func(param_set: Dict[str, float]) -> float:
    """
    Replace this mock with your real E+ run or scenario-based IDF generation.
    param_set: { 'infiltration_base': 1.2, 'occupant_density': 25, ... }

    For demonstration, we do:
      energy = infiltration_base * occupant_density * (1/wall_u_value)
    """
    infiltration = param_set.get("infiltration_base", 1.0)
    occupant_density = param_set.get("occupant_density_m2_per_person", 30.0)
    wall_u = param_set.get("exterior_wall_U_value", 0.5)

    # If wall_u = 0 => avoid div by zero
    energy = infiltration * occupant_density * (1.0 / max(0.001, wall_u))
    return energy


def model_function(sample_values: np.ndarray,
                   problem_dict: Dict[str, Any],
                   simulate_func) -> np.ndarray:
    """
    Evaluate simulate_func for each row in sample_values.
    
    sample_values: 2D array of shape (N, D)
    problem_dict: SALib problem dict
    simulate_func: user-defined function returning a float (energy usage, etc.)
    """
    results = []
    param_names = problem_dict['names']

    for row in sample_values:
        param_set = {}
        for i, p_name in enumerate(param_names):
            param_set[p_name] = row[i]

        out_val = simulate_func(param_set)
        results.append(out_val)

    return np.array(results)


##############################################################################
# 5) Morris & Sobol routines
##############################################################################

def run_morris_sensitivity(params_meta: pd.DataFrame,
                           simulate_func,
                           n_trajectories: int = 10,
                           num_levels: int = 4):
    """
    Perform a Morris (Elementary Effects) sensitivity analysis using SALib.

    returns: (morris_result, sample_values, Y)
    """
    problem_dict = build_problem_dict(params_meta)

    # Generate Morris samples
    sample_values = morris.sample(
        problem_dict,
        N=n_trajectories,
        num_levels=num_levels,
        optimal_trajectories=None,
        local_optimization=False
    )

    # Evaluate model
    Y = model_function(sample_values, problem_dict, simulate_func)

    # Morris analysis
    morris_result = morris_analyze.analyze(
        problem_dict,
        sample_values,
        Y,
        conf_level=0.95,
        print_to_console=False
    )

    return morris_result, sample_values, Y


def run_sobol_sensitivity(params_meta: pd.DataFrame,
                          simulate_func,
                          n_samples: int = 256):
    """
    Perform a Sobol (global) sensitivity analysis with SALib's Saltelli sampler.

    returns: (sobol_result, sample_values, Y)
    """
    problem_dict = build_problem_dict(params_meta)

    # Generate Sobol samples
    sample_values = saltelli.sample(problem_dict,
                                    n_samples,
                                    calc_second_order=True)

    # Evaluate model
    Y = model_function(sample_values, problem_dict, simulate_func)

    # Sobol analysis
    sobol_result = sobol.analyze(
        problem_dict,
        Y,
        calc_second_order=True,
        print_to_console=False
    )

    return sobol_result, sample_values, Y


##############################################################################
# 6) Main Orchestrator for Sensitivity
##############################################################################

def run_sensitivity_analysis(
    scenario_folder: str,
    method: str = "morris",
    output_csv: str = "sensitivity_results.csv"
):
    """
    Orchestrates the entire sensitivity process:
      1) Load scenario params from CSVs
      2) Extract valid param ranges
      3) Run Morris or Sobol
      4) Print & save results to CSV
    """
    print(f"=== Running Sensitivity Analysis with method={method} ===")

    # 1) Read scenario CSVs
    merged_df = load_scenario_params(scenario_folder)

    # 2) Extract param ranges with fallback
    params_meta = extract_parameter_ranges(merged_df)

    # 3) Decide method
    if method.lower() == "morris":
        morris_res, X, Y = run_morris_sensitivity(params_meta, eplus_simulation_func)
        # Morris has keys: 'mu', 'mu_star', 'sigma', 'mu_star_conf'
        print("[Morris] mu_star =>", morris_res["mu_star"])
        print("[Morris] sigma   =>", morris_res["sigma"])
        param_list = params_meta["name"].tolist()
        print("Parameters:", param_list)

        # Save to CSV
        df_out = pd.DataFrame({
            "param": param_list,
            "mu_star": morris_res["mu_star"],
            "mu_star_conf": morris_res["mu_star_conf"],
            "sigma": morris_res["sigma"]
        })
        df_out.to_csv(output_csv, index=False)
        print(f"[INFO] Morris results saved => {output_csv}")

    elif method.lower() == "sobol":
        sobol_res, X, Y = run_sobol_sensitivity(params_meta, eplus_simulation_func)
        s1 = sobol_res["S1"]
        st = sobol_res["ST"]
        s2 = sobol_res["S2"]
        param_list = params_meta["name"].tolist()

        print("[Sobol] S1 =>", s1)
        print("[Sobol] ST =>", st)
        print("[Sobol] S2 =>", s2)
        print("Parameters:", param_list)

        # Save to CSV
        df_out = pd.DataFrame({
            "param": param_list,
            "S1": s1,
            "ST": st
        })
        # s2 is a matrix, you can flatten it or omit it
        df_out.to_csv(output_csv, index=False)
        print(f"[INFO] Sobol results saved => {output_csv}")

    else:
        raise ValueError(f"Unknown method={method}, must be 'morris' or 'sobol'")

    print("=== Sensitivity Analysis Complete ===")


##############################################################################
# 7) Example Usage (If run directly)
##############################################################################

if __name__ == "__main__":
    # Provide the path to your scenario CSVs folder
    scenario_path = r"D:\Documents\E_Plus_2030_py\output\scenarios"

    # Example 1: Morris analysis
    run_sensitivity_analysis(
        scenario_folder=scenario_path,
        method="morris",
        output_csv="morris_sensitivity_results.csv"
    )

    # Example 2: Sobol analysis (uncomment to run)
    run_sensitivity_analysis(
         scenario_folder=scenario_path,
         method="sobol",
         output_csv="sobol_sensitivity_results.csv"
    )

  --- File Contents End ---

  File: surrogate_main.py
  --- File Contents Start ---
# surrogate_main.py

import os
import re
import joblib
import numpy as np
import pandas as pd
from typing import Optional

from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor

# -----------------------------------------------------------------------------
# A) AggregateSurrogate CLASS
#    - Stores the training columns in self.feature_columns
#    - Reorders new X in predict() to match training order
# -----------------------------------------------------------------------------
class AggregateSurrogate:
    """
    Surrogate for predicting a single scalar (e.g. total annual consumption).
    Uses RandomForestRegressor by default.
    """

    def __init__(self, model=None):
        if model is None:
            self.model = RandomForestRegressor(
                n_estimators=50,
                random_state=42
            )
        else:
            self.model = model

        self.is_fitted = False
        self.feature_columns = None  # will store the list of columns used for training

    def fit(self, X: pd.DataFrame, y: np.ndarray):
        """
        Train the surrogate on X,y. Also stores X columns for consistent prediction.
        """
        # Make sure X is a DataFrame so we can get column names
        if not isinstance(X, pd.DataFrame):
            raise ValueError("X must be a pandas DataFrame so we can store column names.")

        self.feature_columns = X.columns.tolist()  # store the training columns
        self.model.fit(X, y)
        self.is_fitted = True

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict using the trained model. Automatically reorders X columns
        to match the training order.
        """
        if not self.is_fitted:
            raise RuntimeError("Model not fitted yet.")

        if not isinstance(X, pd.DataFrame):
            raise ValueError("X must be a pandas DataFrame.")

        # Check that all training columns exist in new X
        missing_cols = [col for col in self.feature_columns if col not in X.columns]
        if missing_cols:
            raise ValueError(f"Missing columns in new data: {missing_cols}")

        # Also check if X has extra columns that were not in training
        extra_cols = [col for col in X.columns if col not in self.feature_columns]
        if extra_cols:
            print(f"[WARNING] New data has extra columns not seen in training: {extra_cols}")

        # Reorder X to match the training columns exactly
        X_reordered = X[self.feature_columns]

        return self.model.predict(X_reordered)


# (Optional) If you need a multi-output/time-series version
class TimeSeriesSurrogate:
    """
    Surrogate for predicting multiple outputs (e.g. 24-hour, 8760-hour).
    Wraps a RandomForestRegressor in MultiOutputRegressor by default,
    and also tracks feature columns for consistent prediction.
    """

    def __init__(self, model=None):
        if model is None:
            base = RandomForestRegressor(n_estimators=50, random_state=42)
            self.model = MultiOutputRegressor(base)
        else:
            self.model = model

        self.is_fitted = False
        self.feature_columns = None

    def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
        if not isinstance(X, pd.DataFrame):
            raise ValueError("X must be a pandas DataFrame.")
        self.feature_columns = X.columns.tolist()

        self.model.fit(X, Y)
        self.is_fitted = True

    def predict(self, X: pd.DataFrame):
        if not self.is_fitted:
            raise RuntimeError("Model not fitted yet.")
        if not isinstance(X, pd.DataFrame):
            raise ValueError("X must be a pandas DataFrame.")

        missing_cols = [c for c in self.feature_columns if c not in X.columns]
        if missing_cols:
            raise ValueError(f"Missing columns in new data: {missing_cols}")

        extra_cols = [c for c in X.columns if c not in self.feature_columns]
        if extra_cols:
            print(f"[WARNING] Extra columns in new data: {extra_cols}")

        X_reordered = X[self.feature_columns]
        return self.model.predict(X_reordered)


# -----------------------------------------------------------------------------
# B) Loading Scenario CSVs
# -----------------------------------------------------------------------------
def load_scenario_params(scenario_folder: str) -> pd.DataFrame:
    """
    Reads scenario CSV files (dhw, elec, fenez, hvac, vent)
    from `scenario_folder`. Merges them into a single DataFrame.
    """
    scenario_files = [
        "scenario_params_dhw.csv",
        "scenario_params_elec.csv",
        "scenario_params_fenez.csv",
        "scenario_params_hvac.csv",
        "scenario_params_vent.csv"
    ]

    dfs = []
    for fname in scenario_files:
        fpath = os.path.join(scenario_folder, fname)
        if os.path.isfile(fpath):
            df = pd.read_csv(fpath)
            df["source_file"] = fname
            dfs.append(df)
        else:
            print(f"[WARNING] Not found: {fpath}")

    if not dfs:
        raise FileNotFoundError(f"No scenario CSVs found in {scenario_folder}.")
    merged = pd.concat(dfs, ignore_index=True)
    return merged


# -----------------------------------------------------------------------------
# C) Optionally Filter by Sensitivity
# -----------------------------------------------------------------------------
def filter_top_parameters(all_params_df: pd.DataFrame,
                          sensitivity_csv: str,
                          top_n: int = 5) -> pd.DataFrame:
    """
    Reads a sensitivity CSV, picks the top N parameters, returns filtered scenario data.
    Must have a param column (e.g. 'param_name') and a numeric metric to sort by (like 'mu_star').
    """
    if not os.path.isfile(sensitivity_csv):
        print(f"[INFO] No sensitivity file found: {sensitivity_csv}, skipping filter.")
        return all_params_df

    sens_df = pd.read_csv(sensitivity_csv)

    possible_param_cols = ["param", "param_name", "ParameterName"]
    param_col = None
    for c in possible_param_cols:
        if c in sens_df.columns:
            param_col = c
            break
    if param_col is None:
        raise ValueError(f"No param column found in {sensitivity_csv}. "
                         f"Columns: {sens_df.columns.tolist()}")

    # Decide which metric
    if "mu_star" in sens_df.columns:
        sort_col = "mu_star"
        print("[INFO] Sorting by mu_star (Morris).")
    elif "ST" in sens_df.columns:
        sort_col = "ST"
        print("[INFO] Sorting by ST (Sobol).")
    elif "S1" in sens_df.columns:
        sort_col = "S1"
        print("[INFO] Sorting by S1 (Sobol).")
    else:
        cand = [col for col in sens_df.columns if col != param_col]
        if not cand:
            raise ValueError("No numeric column to sort by in sensitivity CSV.")
        sort_col = cand[0]
        print(f"[WARNING] Fallback sorting by {sort_col}.")

    # Sort, pick top
    sens_df_sorted = sens_df.sort_values(by=sort_col, ascending=False)
    top_params = sens_df_sorted.head(top_n)[param_col].tolist()
    print(f"[INFO] Top {top_n} parameters by {sort_col}: {top_params}")

    # Filter
    filtered = all_params_df[all_params_df["param_name"].isin(top_params)].copy()
    return filtered


# -----------------------------------------------------------------------------
# D) Build (X, y) Summing Daily Columns => 'AnnualTotal'
# -----------------------------------------------------------------------------
def build_training_data(
    all_params_df: pd.DataFrame,
    results_csv: str,
    scenario_index_col: str = "scenario_index",
    date_col_pattern: str = r"^\d{2}/\d{2}$",
    new_target_col: str = "AnnualTotal"
):
    """
    1) pivot scenario => wide form
    2) read results, sum columns matching date_col_pattern => new_target_col
    3) merge => (X,y)
    """
    # pivot
    pivot_df = all_params_df.pivot_table(
        index=scenario_index_col,
        columns="param_name",
        values="assigned_value",
        aggfunc="first"
    ).reset_index()

    # read results
    if not os.path.isfile(results_csv):
        raise FileNotFoundError(f"Results CSV not found: {results_csv}")
    res_df = pd.read_csv(results_csv)

    # rename if needed
    if "BuildingID" in res_df.columns and scenario_index_col != "BuildingID":
        res_df.rename(columns={"BuildingID": scenario_index_col}, inplace=True)

    # sum daily columns
    pattern = re.compile(date_col_pattern)
    daily_cols = [c for c in res_df.columns if pattern.match(c)]
    if not daily_cols:
        print(f"[WARNING] No daily columns found via pattern={date_col_pattern}")
        # If we donâ€™t have daily cols, we expect new_target_col to already exist
        if new_target_col not in res_df.columns:
            raise ValueError(f"{new_target_col} not found, no daily columns to sum.")
    else:
        # sum them
        res_df[new_target_col] = res_df[daily_cols].sum(axis=1)

    if new_target_col not in res_df.columns:
        raise ValueError(f"{new_target_col} not found in results. "
                         f"Columns: {res_df.columns.tolist()}")

    # merge
    merged = pd.merge(pivot_df, res_df, on=scenario_index_col, how="inner")

    y = merged[new_target_col].values
    param_cols = pivot_df.columns.drop(scenario_index_col).tolist()
    X = merged[[scenario_index_col] + param_cols].copy()

    if scenario_index_col in X.columns:
        X.drop(columns=[scenario_index_col], inplace=True)

    return X, y


# -----------------------------------------------------------------------------
# E) Orchestrator: Train & Save
# -----------------------------------------------------------------------------
def train_aggregate_surrogate(
    scenario_folder: str,
    results_csv: str,
    sensitivity_csv: Optional[str] = None,
    top_n: Optional[int] = None,
    output_model_path: str = "aggregate_surrogate.pkl"
):
    """
    Steps:
      1) load scenario CSV
      2) filter top N if needed
      3) build (X,y) => sum daily columns => AnnualTotal
      4) train AggregateSurrogate
      5) save model
    """
    # 1) load scenario data
    all_params_df = load_scenario_params(scenario_folder)

    # 2) optionally filter
    if sensitivity_csv and top_n is not None:
        all_params_df = filter_top_parameters(all_params_df, sensitivity_csv, top_n)

    # 3) build training data
    X, y = build_training_data(all_params_df, results_csv)
    print(f"[INFO] Training data shape => X: {X.shape}, y: {y.shape}")

    # 4) train
    model = AggregateSurrogate()
    model.fit(X, y)
    print("[INFO] Surrogate training complete.")

    # 5) save
    joblib.dump(model, output_model_path)
    print(f"[INFO] Model saved => {output_model_path}")


# -----------------------------------------------------------------------------
# F) Load Surrogate
# -----------------------------------------------------------------------------
def load_aggregate_surrogate(model_path: str) -> AggregateSurrogate:
    """
    Loads an AggregateSurrogate from the same environment.
    """
    if not os.path.isfile(model_path):
        raise FileNotFoundError(f"File not found: {model_path}")

    loaded_obj = joblib.load(model_path)

    # We'll do a gentler check that it has 'predict' and 'fit'
    if not hasattr(loaded_obj, "predict") or not hasattr(loaded_obj, "fit"):
        raise TypeError("Loaded object does not look like a surrogate model (missing predict/fit).")

    # If you want to confirm it's the EXACT class, uncomment:
    # if not isinstance(loaded_obj, AggregateSurrogate):
    #     raise TypeError("Loaded object is not an AggregateSurrogate.")

    return loaded_obj


# -----------------------------------------------------------------------------
# G) Example Main (Optional)
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    scenario_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    results_csv = r"D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv"

    sensitivity_csv = "morris_sensitivity_results.csv"
    top_n = 5
    output_model_path = "my_aggregate_surrogate.pkl"

    # 1) Train & save
    train_aggregate_surrogate(
        scenario_folder=scenario_folder,
        results_csv=results_csv,
        sensitivity_csv=sensitivity_csv,
        top_n=top_n,
        output_model_path=output_model_path
    )

    # 2) Load the model
    loaded_model = load_aggregate_surrogate(output_model_path)

    # 3) Example new param set in a DataFrame with EXACT same columns as training
    new_params = pd.DataFrame({
        # Must have the same columns as X in the same or any order:
        "exterior_wall_U_value": [0.38],
        "infiltration_base": [1.2],
        "occupant_density_m2_per_person": [28.0],
        "default_heater_capacity_w": [4000.0],
        "ground_floor_window_construction_name": [0]
    })

    # .predict() will reorder columns internally to match training
    pred = loaded_model.predict(new_params)
    print("[INFO] Surrogate prediction =>", pred)

  --- File Contents End ---

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cali_1\__pycache__

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cali_2

  File: build_surrogate_model.py
  --- File Contents Start ---
"""
surrogate_model.py

FULL CODE EXAMPLE: Surrogate Model Building with 
 - scenario-based parameters (from multiple CSVs),
 - EnergyPlus results (merged_daily_mean_mocked.csv),
 - Hyperparameter tuning (RandomizedSearchCV),
 - Feature list saving & re-loading,
 - Handling of fillna + infer_objects to avoid FutureWarning,
 - Potential improvement in test R^2 by tuning parameters or collecting more data.
"""

import os
import numpy as np
import pandas as pd
import joblib

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_absolute_error


# ---------------------------------------------------------------------
# 1) LOADING AND AGGREGATION UTILITIES
# ---------------------------------------------------------------------

def load_scenario_file(filepath):
    """
    Loads a single scenario CSV with columns like:
      [scenario_index, ogc_fid, param_name, assigned_value].
    """
    return pd.read_csv(filepath)


def combine_scenarios(scenarios_dict):
    """
    Merges scenario DataFrames from multiple subsystems (dhw, hvac, fenez, etc.)
    into one combined DataFrame, then pivots param_name -> columns.

    Returns a pivot_df with columns like:
    [scenario_index, ogc_fid, param1, param2, ...].
    """
    all_scenarios = []
    for scenario_name, df_scenario in scenarios_dict.items():
        df_temp = df_scenario.copy()
        df_temp["scenario_type"] = scenario_name  # optional
        all_scenarios.append(df_temp)

    combined = pd.concat(all_scenarios, ignore_index=True)

    # Pivot param_name -> columns
    pivot_df = combined.pivot_table(
        index=["scenario_index", "ogc_fid"],
        columns="param_name",
        values="assigned_value",
        aggfunc="first"
    ).reset_index()

    return pivot_df


def filter_top_parameters(full_param_df, top_params_list):
    """
    OPTIONAL: If you only want to keep the top parameters from sensitivity,
    plus scenario_index and ogc_fid. Otherwise, skip this step.
    """
    keep_cols = ["scenario_index", "ogc_fid"] + top_params_list
    exist_cols = [c for c in keep_cols if c in full_param_df.columns]
    return full_param_df[exist_cols].copy()


def load_sim_results(results_csv):
    """
    Load the merged E+ results file, e.g. 'merged_daily_mean_mocked.csv'.
    Columns typically: [BuildingID, VariableName, 01-Jan, 02-Jan, 03-Jan, ...].
    """
    return pd.read_csv(results_csv)


def aggregate_results(df_sim):
    """
    Convert wide daily columns to a single numeric measure (sum across days),
    returning a DataFrame with [BuildingID, VariableName, TotalEnergy_J].
    """
    melted = df_sim.melt(
        id_vars=["BuildingID", "VariableName"],
        var_name="Day",
        value_name="Value"
    )
    # sum across days
    agg_df = (
        melted.groupby(["BuildingID", "VariableName"])["Value"]
        .sum()
        .reset_index()
        .rename(columns={"Value": "TotalEnergy_J"})
    )
    return agg_df


def merge_params_with_results(pivot_df, df_agg, target_variable=None):
    """
    - rename pivot_df's 'scenario_index' to 'BuildingID' for merging
    - merge with df_agg on BuildingID
    - (optionally) filter rows to target_variable
    """
    merged = pivot_df.copy()
    merged.rename(columns={"scenario_index": "BuildingID"}, inplace=True)

    # Join param data with aggregated simulation results
    merged = pd.merge(
        merged,
        df_agg,
        on="BuildingID",
        how="inner"  # or 'left' if you want all param rows
    )

    if target_variable is not None:
        merged = merged[merged["VariableName"] == target_variable].copy()

    return merged


# ---------------------------------------------------------------------
# 2) MODEL BUILDING & PREDICTION
# ---------------------------------------------------------------------

def build_and_save_surrogate(
    df_data,
    target_col="TotalEnergy_J",
    model_out_path="surrogate_model.joblib",
    columns_out_path="surrogate_model_columns.joblib",
    test_size=0.3,
    random_state=42
):
    """
    Train a RandomForest with hyperparameter search. Then save:
      - The model (model_out_path)
      - The list of columns used for training (columns_out_path)
    """
    # Step A: Exclude rows missing target
    df_data = df_data.dropna(subset=[target_col])

    # Step B: Exclude ID columns or any that are definitely not features
    excluded_cols = ["BuildingID", "ogc_fid", "VariableName", "scenario_type", target_col]

    # potential feature set
    candidate_cols = [c for c in df_data.columns if c not in excluded_cols]

    # Keep only numeric columns
    numeric_cols = []
    for c in candidate_cols:
        if pd.api.types.is_numeric_dtype(df_data[c]):
            numeric_cols.append(c)

    # Prepare X, y
    X = df_data[numeric_cols].copy()
    y = df_data[target_col].copy()

    # Drop rows that have NaNs in X
    mask = ~X.isna().any(axis=1)
    X = X[mask]
    y = y[mask]

    if len(X) < 5:
        print("[ERROR] Not enough data to train a surrogate (fewer than 5 rows).")
        return None, None

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    # Step C: Hyperparameter Tuning with RandomizedSearchCV
    param_distributions = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 40],
        'max_features': ['auto', 'sqrt', 0.5],
    }
    base_rf = RandomForestRegressor(random_state=random_state)

    search = RandomizedSearchCV(
        base_rf,
        param_distributions,
        n_iter=10,
        cv=3,
        random_state=random_state,
        n_jobs=-1  # use all cores
    )
    search.fit(X_train, y_train)
    rf = search.best_estimator_

    # Evaluate performance
    y_pred_train = rf.predict(X_train)
    y_pred_test = rf.predict(X_test)

    r2_train = r2_score(y_train, y_pred_train)
    r2_test = r2_score(y_test, y_pred_test)
    mae_test = mean_absolute_error(y_test, y_pred_test)

    print("\n=== Surrogate Model Performance ===")
    print(f"Best Hyperparams: {search.best_params_}")
    print(f"Train R^2: {r2_train:.3f}")
    print(f"Test  R^2: {r2_test:.3f}")
    print(f"Test  MAE: {mae_test:.3f}")

    # Save the model
    joblib.dump(rf, model_out_path)
    print(f"[INFO] Surrogate model saved to: {model_out_path}")

    # Save the list of numeric columns used at training
    joblib.dump(numeric_cols, columns_out_path)
    print(f"[INFO] Model columns saved to: {columns_out_path}")

    return rf, numeric_cols


def load_and_predict_sample(
    model_path,
    columns_path,
    sample_series
):
    """
    Re-load the RandomForest + the feature column list. 
    Build a 1-row DataFrame from sample_series, ensuring we have the same columns in the same order.
    Fill missing columns with 0.0, then .infer_objects() to avoid the future warning.
    Predict and return result.
    """
    # Load model
    rf = joblib.load(model_path)
    # Load columns
    trained_columns = joblib.load(columns_path)

    # Convert sample_series -> DataFrame(1 row)
    sample_df = sample_series.to_frame().T

    # Guarantee that all 'trained_columns' are present
    for col in trained_columns:
        if col not in sample_df.columns:
            sample_df[col] = np.nan

    # Drop columns not in trained_columns
    sample_df = sample_df[trained_columns]

    # Fill missing numeric fields
    sample_df = sample_df.fillna(0.0)

    # Convert object dtypes to numeric if possible, to avoid future warnings
    sample_df = sample_df.infer_objects(copy=False)

    # Predict
    y_pred = rf.predict(sample_df)
    return y_pred


# ---------------------------------------------------------------------
# 3) MAIN SCRIPT: GATHER, TRAIN, SAVE, DEMO
# ---------------------------------------------------------------------

def main():
    """
    Full pipeline demonstration:
    1) Load scenario CSVs (dhw, hvac, etc.)
    2) Combine -> param pivot
    3) Load sim results & sum daily -> df_agg
    4) Merge param pivot w/ results for a chosen variable => merged_df
    5) build_and_save_surrogate => random forest
    6) load_and_predict_sample => single-sample prediction
    """
    # A) Load scenario CSVs
    scenarios_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    scenario_files = {
        "dhw":   "scenario_params_dhw.csv",
        "elec":  "scenario_params_elec.csv",
        "fenez": "scenario_params_fenez.csv",
        "hvac":  "scenario_params_hvac.csv",
        "vent":  "scenario_params_vent.csv",
    }

    scenarios_dict = {}
    for name, fname in scenario_files.items():
        fpath = os.path.join(scenarios_folder, fname)
        if os.path.exists(fpath):
            df_scenario = load_scenario_file(fpath)
            scenarios_dict[name] = df_scenario
        else:
            print(f"[WARN] Missing scenario file: {fpath}")

    # Combine into pivot
    pivot_df = combine_scenarios(scenarios_dict)

    # (OPTIONAL) If you want only top parameters from sensitivity:
    # top_params_list = ["infiltration_base", "occupant_density_m2_per_person", "heating_day_setpoint"]
    # pivot_df = filter_top_parameters(pivot_df, top_params_list)

    # B) Load & aggregate sim results
    results_csv = r"D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv"
    df_sim = load_sim_results(results_csv)
    df_agg = aggregate_results(df_sim)

    # C) Merge param pivot with results for a chosen variable (Heating, e.g.)
    target_variable = "Heating:EnergyTransfer [J](Hourly)"
    merged_df = merge_params_with_results(pivot_df, df_agg, target_variable=target_variable)

    # We'll rename 'TotalEnergy_J' -> 'target' for clarity
    merged_df.rename(columns={"TotalEnergy_J": "target"}, inplace=True)

    # D) Build & Save Surrogate 
    model_path = "heating_surrogate_model.joblib"
    columns_path = "heating_surrogate_columns.joblib"
    rf_model, col_list = build_and_save_surrogate(
        df_data=merged_df,
        target_col="target",
        model_out_path=model_path,
        columns_out_path=columns_path,
        test_size=0.3,  # 30% test
        random_state=42
    )
    if rf_model is None:
        print("[ERROR] Surrogate model training unsuccessful.")
        return

    # E) Demo: Re-load & predict a single row
    if not merged_df.empty:
        sample_row = merged_df.iloc[0].copy()  # pick 1 row from the DF
        y_pred_sample = load_and_predict_sample(
            model_path,
            columns_path,
            sample_row
        )
        print(f"\n[SAMPLE PREDICTION] Heating Energy = {y_pred_sample[0]:.2f} J (approx)")
    else:
        print("[WARN] Merged DataFrame is empty - no row to sample for prediction.")


# Keep the following if you want to run this directly:
if __name__ == "__main__":
    main()

  --- File Contents End ---

  File: calibration_code.py
  --- File Contents Start ---
import random
import copy
import csv
import pandas as pd
from typing import List, Dict, Tuple

# For Bayesian Optimization
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args


# -------------------------------------------------------------------------
# 1) DEFINE PARAM SPACE AND SIMULATION/ERROR FUNCTION
# -------------------------------------------------------------------------

def simulate_or_surrogate(params: Dict[str, float]) -> float:
    """
    Placeholder function:
      1) Takes a param dictionary, e.g. {'infiltration': 0.9, 'occupant_density': 30, ...}
      2) Runs either a direct E+ simulation OR calls your surrogate
      3) Compares results to real data (MBE, CV(RMSE), or custom objective)
      4) Returns a scalar error (lower is better)

    TODO: Implement your actual logic here.
    """
    # For demonstration, produce a random error:
    return random.uniform(0, 100)


# Example param space
PARAM_SPACE = [
    {"name": "infiltration_base", "type": "float", "low": 0.5, "high": 1.3},
    {"name": "occupant_density",  "type": "float", "low": 20.0, "high": 40.0},
    {"name": "heating_setpoint",  "type": "float", "low": 19.0, "high": 24.0},
]


# -------------------------------------------------------------------------
# 2) RANDOM SEARCH
# -------------------------------------------------------------------------

def random_search_calibration(
    param_space: List[Dict],
    n_iterations: int = 50,
) -> Tuple[Dict[str, float], float, list]:
    """
    1) Randomly sample N sets of parameters from the param_space
    2) Evaluate error for each
    3) Return best param set, best error, and a history list

    :param param_space: list of dicts describing each param's name, type, low/high
    :param n_iterations: how many random draws
    :return: (best_params, best_error, history)
             history is a list of (param_dict, error)
    """
    best_params = None
    best_error = float("inf")
    history = []

    for i in range(n_iterations):
        # sample a random param set
        candidate = {}
        for pspec in param_space:
            if pspec["type"] == "float":
                val = random.uniform(pspec["low"], pspec["high"])
            elif pspec["type"] == "int":
                val = random.randint(pspec["low"], pspec["high"])
            else:
                # Handle categories or fallback
                val = pspec["low"]  
            candidate[pspec["name"]] = val

        # simulate
        error = simulate_or_surrogate(candidate)
        history.append((candidate, error))

        if error < best_error:
            best_error = error
            best_params = candidate

    return best_params, best_error, history


# -------------------------------------------------------------------------
# 3) GENETIC ALGORITHM (GA)
# -------------------------------------------------------------------------

def ga_calibration(
    param_space: List[Dict],
    pop_size: int = 20,
    generations: int = 10,
    crossover_prob: float = 0.7,
    mutation_prob: float = 0.2,
) -> Tuple[Dict[str, float], float, list]:
    """
    Basic GA:
     - Initialize population of random solutions
     - Evaluate fitness (1/error)
     - Selection (tournament)
     - Crossover
     - Mutation
     - Return best solution
    Also keep a history of tested individuals across generations.

    :return: (best_params, best_error, history)
             where history is list of (param_dict, error)
    """

    def random_individual():
        ind = {}
        for pspec in param_space:
            if pspec["type"] == "float":
                val = random.uniform(pspec["low"], pspec["high"])
            elif pspec["type"] == "int":
                val = random.randint(pspec["low"], pspec["high"])
            else:
                val = pspec["low"]
            ind[pspec["name"]] = val
        return ind

    def evaluate(ind):
        err = simulate_or_surrogate(ind)
        return 1.0 / (1.0 + err), err  # (fitness, error)

    def tournament_select(pop, k=3):
        # pick k random
        contenders = random.sample(pop, k)
        # highest fitness
        best = max(contenders, key=lambda x: x["fitness"])
        return copy.deepcopy(best)

    def crossover(parent1, parent2):
        child1 = {}
        child2 = {}
        for key in parent1["params"].keys():
            if random.random() < 0.5:
                child1[key] = parent1["params"][key]
                child2[key] = parent2["params"][key]
            else:
                child1[key] = parent2["params"][key]
                child2[key] = parent1["params"][key]
        return child1, child2

    def mutate(ind_params):
        for pspec in param_space:
            if random.random() < mutation_prob:
                if pspec["type"] == "float":
                    ind_params[pspec["name"]] = random.uniform(pspec["low"], pspec["high"])
                elif pspec["type"] == "int":
                    ind_params[pspec["name"]] = random.randint(pspec["low"], pspec["high"])

    # INITIAL POP
    population = []
    history = []
    for _ in range(pop_size):
        params = random_individual()
        fit, err = evaluate(params)
        population.append({"params": params, "fitness": fit, "error": err})
        history.append((params, err))

    # EVOLVE
    for gen in range(generations):
        new_pop = []
        while len(new_pop) < pop_size:
            parent_a = tournament_select(population)
            parent_b = tournament_select(population)
            if random.random() < crossover_prob:
                c1, c2 = crossover(parent_a, parent_b)
            else:
                c1 = parent_a["params"]
                c2 = parent_b["params"]
            mutate(c1)
            mutate(c2)
            f1, e1 = evaluate(c1)
            f2, e2 = evaluate(c2)
            new_pop.append({"params": c1, "fitness": f1, "error": e1})
            new_pop.append({"params": c2, "fitness": f2, "error": e2})
            # record to history
            history.append((c1, e1))
            history.append((c2, e2))

        # keep best pop_size
        new_pop.sort(key=lambda x: x["fitness"], reverse=True)
        population = new_pop[:pop_size]

        best_ind = max(population, key=lambda x: x["fitness"])
        print(f"GA gen={gen}, best error={best_ind['error']:.3f}")

    best_ind = max(population, key=lambda x: x["fitness"])
    return best_ind["params"], best_ind["error"], history


# -------------------------------------------------------------------------
# 4) BAYESIAN OPTIMIZATION
# -------------------------------------------------------------------------

def bayes_calibration(
    param_space: List[Dict],
    n_calls: int = 30,
) -> Tuple[Dict[str, float], float, list]:
    """
    Bayesian Optimization with gp_minimize. 
    We'll keep track of 'history' by hooking into res.x_iters and res.func_vals.

    :return: (best_params, best_error, history)
    """
    # Convert param_space -> skopt
    skopt_dims = []
    param_names = []
    for pspec in param_space:
        param_names.append(pspec["name"])
        if pspec["type"] == "float":
            skopt_dims.append(Real(pspec["low"], pspec["high"], name=pspec["name"]))
        elif pspec["type"] == "int":
            skopt_dims.append(Integer(pspec["low"], pspec["high"], name=pspec["name"]))
        else:
            # handle categories or fallback
            skopt_dims.append(Real(pspec["low"], pspec["high"], name=pspec["name"]))

    @use_named_args(skopt_dims)
    def objective(**kwargs):
        err = simulate_or_surrogate(kwargs)
        return err

    res = gp_minimize(
        func=objective,
        dimensions=skopt_dims,
        n_calls=n_calls,
        n_initial_points=5,
        random_state=0
    )

    best_error = res.fun
    best_x = res.x

    # convert best_x -> dict
    best_params = {}
    for i, val in enumerate(best_x):
        best_params[param_names[i]] = val

    # Build a 'history' from x_iters + func_vals
    history = []
    for i, xlist in enumerate(res.x_iters):
        param_dict = {}
        for j, val in enumerate(xlist):
            param_dict[param_names[j]] = val
        err = res.func_vals[i]
        history.append((param_dict, err))

    return best_params, best_error, history


# -------------------------------------------------------------------------
# 5) CALIBRATION MANAGER
# -------------------------------------------------------------------------

class CalibrationManager:
    """
    A manager to unify the interface. 
    usage:

        manager = CalibrationManager(param_space=PARAM_SPACE)
        best_params, best_err, history = manager.run_calibration(method="random")
    """
    def __init__(self, param_space: List[Dict]):
        self.param_space = param_space

    def run_calibration(
        self,
        method: str = "random",
        **kwargs
    ) -> Tuple[Dict[str, float], float, list]:
        """
        method can be: "random", "ga", or "bayes"
        returns (best_params, best_error, history)
        """
        if method == "random":
            n_iter = kwargs.get("n_iterations", 50)
            bp, be, hist = random_search_calibration(self.param_space, n_iter)
            return bp, be, hist

        elif method == "ga":
            pop_size = kwargs.get("pop_size", 20)
            generations = kwargs.get("generations", 10)
            crossover_prob = kwargs.get("crossover_prob", 0.7)
            mutation_prob = kwargs.get("mutation_prob", 0.2)
            bp, be, hist = ga_calibration(
                self.param_space,
                pop_size=pop_size,
                generations=generations,
                crossover_prob=crossover_prob,
                mutation_prob=mutation_prob,
            )
            return bp, be, hist

        elif method == "bayes":
            n_calls = kwargs.get("n_calls", 30)
            bp, be, hist = bayes_calibration(self.param_space, n_calls)
            return bp, be, hist

        else:
            raise ValueError(f"Unknown method: {method}")


# -------------------------------------------------------------------------
# 6) CSV LOGGING UTILITY
# -------------------------------------------------------------------------

def save_history_to_csv(history: list, filename: str):
    """
    Save a calibration history to CSV.
    `history` is a list of (param_dict, error).
    We'll create columns for each param + 'error'.
    """
    if not history:
        print(f"[WARN] No history to save for {filename}.")
        return

    # Build a list of dicts
    rows = []
    for (pdict, err) in history:
        row = dict(**pdict)
        row["error"] = err
        rows.append(row)

    # Convert to DataFrame for easy CSV
    df = pd.DataFrame(rows)
    df.to_csv(filename, index=False)
    print(f"[INFO] Saved calibration history to {filename}")

  --- File Contents End ---

  File: calibration_man.py
  --- File Contents Start ---
from cali_2.calibration_code import (
    CalibrationManager,
    PARAM_SPACE,
    save_history_to_csv
)

def main():
    manager = CalibrationManager(PARAM_SPACE)

    # 1) Random Search
    print("=== Calibrating with Random Search ===")
    best_params_random, best_err_random, hist_random = manager.run_calibration(
        method="random",
        n_iterations=30
    )
    print(f"[RANDOM] best error = {best_err_random:.3f}")
    print(f"[RANDOM] best params = {best_params_random}")
    save_history_to_csv(hist_random, "calibration_random.csv")

    # 2) Genetic Algorithm
    print("\n=== Calibrating with GA ===")
    best_params_ga, best_err_ga, hist_ga = manager.run_calibration(
        method="ga",
        pop_size=10,
        generations=5,
        crossover_prob=0.7,
        mutation_prob=0.2
    )
    print(f"[GA] best error = {best_err_ga:.3f}")
    print(f"[GA] best params = {best_params_ga}")
    save_history_to_csv(hist_ga, "calibration_ga.csv")

    # 3) Bayesian Optimization
    print("\n=== Calibrating with Bayesian Optimization ===")
    best_params_bayes, best_err_bayes, hist_bayes = manager.run_calibration(
        method="bayes",
        n_calls=20
    )
    print(f"[BAYES] best error = {best_err_bayes:.3f}")
    print(f"[BAYES] best params = {best_params_bayes}")
    save_history_to_csv(hist_bayes, "calibration_bayes.csv")


if __name__ == "__main__":
    main()

  --- File Contents End ---

  File: man.py
  --- File Contents Start ---
"""
A simple runner script that imports the main sensitivity analysis and executes it.
"""

import cali_2.sensitivity_analysis

if __name__ == "__main__":
    cali_2.sensitivity_analysis.main()



"""
man.py

A simple entry-point script to run the surrogate model pipeline 
from the 'surrogate_model.py' module.
"""

import cali_2.build_surrogate_model

if __name__ == "__main__":
    cali_2.build_surrogate_model.main()

  --- File Contents End ---

  File: sensitivity_analysis.py
  --- File Contents Start ---
"""
Example script for running a sensitivity analysis on scenario-based simulations.

Workflow:
1) Collect scenario definitions from CSV files in output/scenarios/.
2) For each scenario, (optionally) generate an IDF & run EnergyPlus, or skip if already done.
3) Load the simulation results from 'merged_daily_mean_mocked.csv' (or your actual file).
4) Merge scenario parameters with results.
5) Perform a simple correlation-based sensitivity analysis (or partial correlation).
6) Save a 'sensitivity_report.csv' summarizing which parameters most affect the outputs.
7) Optionally feed top parameters into calibration/next steps.

Author: Example
"""
import os
import numpy as np
import pandas as pd
from pathlib import Path

# If you want advanced sensitivity, you might do:
# from SALib.analyze import sobol
# from SALib.sample import saltelli
# etc.


def load_scenario_file(filepath):
    """
    Load a scenario CSV (e.g. 'scenario_params_dhw.csv') into a pandas DataFrame.
    Expects columns: [scenario_index, ogc_fid, param_name, assigned_value].
    """
    df = pd.read_csv(filepath)
    return df


def generate_idf_and_run_simulations(all_scenarios_df, scenario_name):
    """
    Placeholder function that:
    1. Groups scenario data by scenario_index.
    2. For each scenario, merges all assigned parameters into an IDF.
    3. Runs E+ or calls the existing 'modification/' pipeline, saving results.

    In a real pipeline, you'd call:
      - modification.main_modifi.apply_... for each scenario
      - epw.run_epw_sims.simulate_all(...) 
    Here we just print and assume the user runs them or has results already.
    """
    grouped = all_scenarios_df.groupby("scenario_index")
    for scenario_idx, subdf in grouped:
        print(f"\n[INFO] Running scenario: {scenario_name}, idx={scenario_idx}")
        # 1. Merge param sets from subdf
        # 2. Generate IDF file -> "scenario_{scenario_idx}.idf"
        # 3. Run E+ or skip if done
        # ...
        # We'll just demonstrate logging:
        param_summary = subdf[["param_name", "assigned_value"]].head(5).to_dict(orient="records")
        print(f"[DEBUG] Example param subset: {param_summary}")
    print(f"[INFO] Completed sim for all scenarios in {scenario_name}")


def load_sim_results(results_csv):
    """
    Load your merged E+ results from CSV, e.g. 'merged_daily_mean_mocked.csv'.
    Expected columns: [BuildingID, VariableName, 01-Jan, 02-Jan, ...].
    We'll pivot to wide format or keep as is for correlation.
    """
    df = pd.read_csv(results_csv)
    return df


def prepare_sensitivity_data(scenarios_dict, sim_results):
    """
    Merge scenario parameters with simulation results to build a dataset for correlation analysis.
    Steps:
      1) Convert scenario parameters to wide form (param columns).
      2) Link scenario_index -> BuildingID or some identifying key in sim_results.
      3) Extract or aggregate the relevant output metrics (e.g. total consumption).
    """

    # ------------- Part A: Combine all scenario CSVs into one DataFrame -------------
    all_scenarios = []
    for scenario_name, df_scenario in scenarios_dict.items():
        df_temp = df_scenario.copy()
        df_temp["scenario_type"] = scenario_name
        all_scenarios.append(df_temp)
    combined_df = pd.concat(all_scenarios, ignore_index=True)

    # Pivot so each param_name becomes a column, scenario_index is the row
    pivot_df = combined_df.pivot_table(
        index=["scenario_index", "ogc_fid"],
        columns="param_name",
        values="assigned_value",
        aggfunc="first"  # if multiple, just take the first
    ).reset_index()

    # ------------- Part B: Link to simulation results -------------
    # Assume scenario_index matches BuildingID in results
    pivot_df.rename(columns={"scenario_index": "BuildingID"}, inplace=True)

    # Melt and sum daily results in sim_results
    melted = sim_results.melt(
        id_vars=["BuildingID", "VariableName"],
        var_name="Day",
        value_name="Value"
    )
    daily_sum = melted.groupby(["BuildingID", "VariableName"])["Value"].sum().reset_index()
    daily_sum.rename(columns={"Value": "TotalEnergy_J"}, inplace=True)

    # Merge param pivot with daily sums
    merged_sens_df = pd.merge(
        pivot_df, daily_sum,
        on="BuildingID",
        how="inner"
    )
    return merged_sens_df


def compute_sensitivity(merged_sens_df, target_variable="Heating:EnergyTransfer [J](Hourly)"):
    """
    Example: Filter to the target_variable, then compute correlation
    of each parameter with 'TotalEnergy_J'. 
    Returns a DataFrame with 'Parameter' and 'Correlation'.
    """
    df_var = merged_sens_df[merged_sens_df["VariableName"] == target_variable].copy()
    if df_var.empty:
        print(f"[WARN] No rows found for variable={target_variable}, cannot compute sensitivity.")
        return pd.DataFrame()

    exclude_cols = ["BuildingID", "ogc_fid", "VariableName", "TotalEnergy_J"]
    param_cols = [c for c in df_var.columns if c not in exclude_cols]

    correlations = []
    for col in param_cols:
        if pd.api.types.is_numeric_dtype(df_var[col]):
            corr = df_var[[col, "TotalEnergy_J"]].corr().iloc[0, 1]
            correlations.append((col, corr))
        else:
            correlations.append((col, np.nan))

    corr_df = pd.DataFrame(correlations, columns=["Parameter", "Correlation"])
    corr_df["AbsCorrelation"] = corr_df["Correlation"].abs()
    corr_df.sort_values("AbsCorrelation", ascending=False, inplace=True)
    return corr_df


def main():
    """
    Main function to illustrate a sensitivity analysis pipeline.
    """
    # 1) Load scenario CSVs from 'output/scenarios' folder
    scenarios_folder = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    scenario_files = {
        "dhw": "scenario_params_dhw.csv",
        "elec": "scenario_params_elec.csv",
        "fenez": "scenario_params_fenez.csv",
        "hvac": "scenario_params_hvac.csv",
        "vent": "scenario_params_vent.csv",
    }

    scenarios_dict = {}
    for name, fname in scenario_files.items():
        fpath = os.path.join(scenarios_folder, fname)
        if os.path.exists(fpath):
            print(f"[INFO] Loading scenario file for {name}: {fpath}")
            df_scenario = load_scenario_file(fpath)
            scenarios_dict[name] = df_scenario
        else:
            print(f"[WARN] File not found: {fpath} - skipping.")
    
    # 2) (Optionally) Generate IDFs & Run E+ for each scenario set
    for scenario_name, df_scenario in scenarios_dict.items():
        generate_idf_and_run_simulations(df_scenario, scenario_name)

    # 3) Load aggregated simulation results
    results_csv = r"D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv"
    sim_results = load_sim_results(results_csv)

    # 4) Prepare DataFrame merging param values with results
    merged_sens_df = prepare_sensitivity_data(scenarios_dict, sim_results)

    # 5) Perform correlation-based sensitivity for multiple target variables
    target_vars = [
        "Heating:EnergyTransfer [J](Hourly)",
        "Electricity:Facility [J](Hourly)",
        "MYDHW_0_WATERHEATER:Water Heater Heating Energy [J](Daily)"
    ]
    final_report = []
    for var in target_vars:
        corr_df = compute_sensitivity(merged_sens_df, target_variable=var)
        corr_df["TargetVariable"] = var
        final_report.append(corr_df)

    if final_report:
        final_report_df = pd.concat(final_report, ignore_index=True)
    else:
        final_report_df = pd.DataFrame()

    # 6) Save sensitivity report
    out_csv = "sensitivity_report.csv"
    final_report_df.to_csv(out_csv, index=False)
    print(f"\n[INFO] Sensitivity report saved to: {out_csv}")

    # 7) Identify top 3 parameters by absolute correlation for each variable
    top_params = (
        final_report_df
        .sort_values("AbsCorrelation", ascending=False)
        .groupby("TargetVariable")
        .head(3)
    )
    print("\n=== Top 3 Parameters (by |Correlation|) for Each Variable ===")
    print(top_params)

    # 8) Additional steps like calibration/optimization could go here.


if __name__ == "__main__":
    main()

  --- File Contents End ---

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cali_2\__pycache__

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cal_1

  File: calibration_main.py
  --- File Contents Start ---
"""
calibration_main.py

Demonstrates a calibration workflow using 3 optimization approaches:
  1) Random Search
  2) Genetic Algorithm (GA)
  3) Bayesian Optimization (with scikit-optimize)

Integration points:
  - We load parameter specs from a CSV (similar to df_param_ranges).
  - We define an 'evaluation function' that returns a mismatch vs. real data.
    (This could call a surrogate model or the real E+ simulation.)
  - Then we run the chosen optimization method until we find the best match.

Dependencies:
  pip install numpy pandas scikit-optimize (if you want Bayesian) joblib
"""

import os
import numpy as np
import pandas as pd
import random
from typing import List, Dict, Any, Callable

try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer
    SKOPT_AVAILABLE = True
except ImportError:
    gp_minimize = None
    Real = None
    Integer = None
    SKOPT_AVAILABLE = False


class ParameterSpec:
    def __init__(self,
                 name: str,
                 min_value: float,
                 max_value: float,
                 is_active: bool = True,
                 is_integer: bool = False,
                 categories: List[str] = None):
        self.name = name
        self.min_value = min_value
        self.max_value = max_value
        self.is_active = is_active
        self.is_integer = is_integer
        self.categories = categories

    def sample_random(self) -> float:
        if self.categories is not None:
            return random.choice(self.categories)
        val = random.uniform(self.min_value, self.max_value)
        if self.is_integer:
            val = int(round(val))
        return val


class ParameterSet:
    def __init__(self, values: Dict[str, float]):
        self.values = values

    def copy(self):
        return ParameterSet(dict(self.values))

    def __repr__(self):
        return f"ParameterSet({self.values})"


def clamp_param_set(param_set: ParameterSet,
                    param_specs: List[ParameterSpec]) -> ParameterSet:
    new_values = dict(param_set.values)
    spec_dict = {s.name: s for s in param_specs}
    for p_name, p_value in new_values.items():
        spec = spec_dict[p_name]
        clamped = max(spec.min_value, min(spec.max_value, p_value))
        if spec.is_integer:
            clamped = int(round(clamped))
        new_values[p_name] = clamped
    return ParameterSet(new_values)


def sample_random_param_set(param_specs: List[ParameterSpec]) -> ParameterSet:
    values = {}
    for spec in param_specs:
        if not spec.is_active:
            continue
        values[spec.name] = spec.sample_random()
    return ParameterSet(values)


def evaluate_param_set(param_set: ParameterSet,
                       evaluation_func: Callable[[Dict[str, float]], float]) -> float:
    return evaluation_func(param_set.values)


def random_search(param_specs: List[ParameterSpec],
                  evaluation_func: Callable[[dict], float],
                  n_iterations: int = 50):
    best_score = float('inf')
    best_pset = None

    for _ in range(n_iterations):
        pset = sample_random_param_set(param_specs)
        score = evaluation_func(pset.values)
        if score < best_score:
            best_score = score
            best_pset = pset

    return best_pset, best_score


def ga_optimization(param_specs: List[ParameterSpec],
                    evaluation_func: Callable[[dict], float],
                    pop_size: int = 20,
                    max_generations: int = 10,
                    mutation_prob: float = 0.1):
    """
    Basic Genetic Algorithm:
      - Initialize population
      - Evaluate
      - Reproduce (selection, crossover, mutation)
      - Return best
    """
    # 1) Init population
    population = []
    for _ in range(pop_size):
        pset = sample_random_param_set(param_specs)
        score = evaluation_func(pset.values)
        population.append((pset, score))

    # 2) GA loop
    for gen in range(max_generations):
        # Sort by ascending score
        population.sort(key=lambda x: x[1])
        best_score = population[0][1]
        print(f"[GEN {gen}] best_score = {best_score:.4f}")

        # Reproduce new pop
        new_pop = []
        # Elitism: keep top 2
        new_pop.extend(population[:2])

        # Fill up population
        while len(new_pop) < pop_size:
            parent1 = tournament_select(population)
            parent2 = tournament_select(population)
            child_pset = crossover(parent1[0], parent2[0])
            child_pset = mutate(child_pset, param_specs, mutation_prob)
            child_score = evaluation_func(child_pset.values)
            new_pop.append((child_pset, child_score))

        population = new_pop

    population.sort(key=lambda x: x[1])
    return population[0][0], population[0][1]


def tournament_select(population: List[tuple], k: int = 3):
    contenders = random.sample(population, k)
    contenders.sort(key=lambda x: x[1])
    return contenders[0]


def crossover(psetA: ParameterSet, psetB: ParameterSet) -> ParameterSet:
    child_vals = {}
    for k in psetA.values.keys():
        if random.random() < 0.5:
            child_vals[k] = psetA.values[k]
        else:
            child_vals[k] = psetB.values[k]
    return ParameterSet(child_vals)


def mutate(pset: ParameterSet,
           param_specs: List[ParameterSpec],
           mutation_prob: float) -> ParameterSet:
    new_vals = dict(pset.values)
    spec_dict = {s.name: s for s in param_specs}

    for key in new_vals.keys():
        if random.random() < mutation_prob:
            # resample
            spec = spec_dict[key]
            new_vals[key] = spec.sample_random()

    child = ParameterSet(new_vals)
    child = clamp_param_set(child, param_specs)
    return child


def bayes_optimization(param_specs: List[ParameterSpec],
                       evaluation_func: Callable[[dict], float],
                       n_calls: int = 30):
    """
    If scikit-optimize is not installed, this defaults to random search.
    """
    if not SKOPT_AVAILABLE or gp_minimize is None:
        print("[WARN] scikit-optimize not installed. Fallback to random_search.")
        return random_search(param_specs, evaluation_func, n_iterations=n_calls)

    from skopt.space import Space, Categorical  # only if installed

    dims = []
    param_names = []
    for spec in param_specs:
        if spec.categories:
            # Not implemented in detail
            raise NotImplementedError("Categorical example not implemented.")
        else:
            if spec.is_integer:
                dims.append(Integer(spec.min_value, spec.max_value, name=spec.name))
            else:
                dims.append(Real(spec.min_value, spec.max_value, name=spec.name))
        param_names.append(spec.name)

    # Objective
    def objective(params_list):
        param_dict = {}
        for i, val in enumerate(params_list):
            param_dict[param_names[i]] = val
        pset = ParameterSet(param_dict)
        pset = clamp_param_set(pset, param_specs)
        return evaluation_func(pset.values)

    # run gp_minimize
    res = gp_minimize(
        objective,
        dims,
        n_calls=n_calls,
        random_state=42
    )

    best_score = res.fun
    best_solution = res.x
    best_vals = {}
    for i, val in enumerate(best_solution):
        best_vals[param_names[i]] = val
    best_pset = ParameterSet(best_vals)

    return best_pset, best_score


def load_param_specs(csv_file: str) -> List[ParameterSpec]:
    """
    Loads a CSV with columns like [param_name, min_value, max_value, is_active, is_integer].
    Returns a list of ParameterSpec.
    """
    df = pd.read_csv(csv_file)
    specs = []
    for _, row in df.iterrows():
        param_name = row["param_name"]
        mn = float(row["min_value"])
        mx = float(row["max_value"])
        is_act = row.get("is_active", True)
        is_int = row.get("is_integer", False)
        spec = ParameterSpec(
            name=param_name,
            min_value=mn,
            max_value=mx,
            is_active=is_act,
            is_integer=is_int
        )
        specs.append(spec)
    return specs


def mock_eplus_or_surrogate(param_dict: Dict[str, float]) -> float:
    """
    Example calibration objective function that returns
    a mismatch between model and real data.
    We'll do a contrived formula for demonstration.
    """
    total = sum(param_dict.values())
    noise = np.random.uniform(-0.1, 0.1)
    return abs(10.0 - total) + noise


def main():
    """
    Run a demo of the calibration workflow. 
    1) Create or load param specs from CSV
    2) Define an objective function
    3) Run three optimization strategies
    4) Compare results and save the best
    """
    param_specs_csv = "my_param_specs.csv"
    if not os.path.isfile(param_specs_csv):
        # Create a demo CSV if it doesn't exist
        data_demo = {
            "param_name": ["infiltration", "occupancy", "wall_u"],
            "min_value": [0.5, 10, 0.3],
            "max_value": [2.0, 40, 1.2],
            "is_active": [True, True, True],
            "is_integer": [False, True, False]
        }
        df_demo = pd.DataFrame(data_demo)
        df_demo.to_csv(param_specs_csv, index=False)
        print(f"[INFO] Created a demo param specs CSV => {param_specs_csv}")

    # 1) Load param specs
    specs = load_param_specs(param_specs_csv)
    print(f"[INFO] Loaded {len(specs)} parameter specs from {param_specs_csv}")

    # 2) Define objective function
    def calibration_objective(param_values: Dict[str, float]) -> float:
        return mock_eplus_or_surrogate(param_values)

    # 3) Run each method
    print("\n=== Running Random Search Calibration ===")
    best_set_rand, best_score_rand = random_search(specs, calibration_objective, n_iterations=20)
    print("[RANDOM] Best ParamSet:", best_set_rand, "Score:", best_score_rand)

    print("\n=== Running GA Calibration ===")
    best_set_ga, best_score_ga = ga_optimization(specs, calibration_objective,
                                                 pop_size=10, max_generations=5, mutation_prob=0.2)
    print("[GA] Best ParamSet:", best_set_ga, "Score:", best_score_ga)

    print("\n=== Running Bayesian Calibration ===")
    best_set_bayes, best_score_bayes = bayes_optimization(specs, calibration_objective, n_calls=15)
    print("[Bayes] Best ParamSet:", best_set_bayes, "Score:", best_score_bayes)

    # 4) Compare/choose the best
    results = [
        ("Random", best_set_rand, best_score_rand),
        ("GA", best_set_ga, best_score_ga),
        ("Bayesian", best_set_bayes, best_score_bayes)
    ]
    results.sort(key=lambda x: x[2])  # sort by score ascending
    best_method, best_params, best_score = results[0]
    print(f"\n=== Overall Best Among 3 ===")
    print(f"Method: {best_method}, Score: {best_score:.4f}, Params: {best_params}")

    out_csv = "calibration_best_params.csv"
    df_out = pd.DataFrame(list(best_params.values.items()), columns=["param_name", "param_value"])
    df_out["best_score"] = best_score
    df_out["method"] = best_method
    df_out.to_csv(out_csv, index=False)
    print(f"[INFO] Saved best param set to {out_csv}")


if __name__ == "__main__":
    main()

  --- File Contents End ---

  File: man.py
  --- File Contents Start ---
"""
man.py

Simple driver script that uses functions from sensitivity_analysis_main.py.
"""

import sys
import os

# Adjust the import path if needed
# If `sensitivity_analysis_main.py` is in the same folder, this should work:
from cal_1.sensitivity_analysis_main import run_sensitivity_analyses

if __name__ == "__main__":
    # Adjust the paths below to point to your scenario CSV files
    dhw_csv   = r"D:\Documents\E_Plus_2030_py\output\scenarios\scenario_params_dhw.csv"
    elec_csv  = r"D:\Documents\E_Plus_2030_py\output\scenarios\scenario_params_elec.csv"
    fenez_csv = r"D:\Documents\E_Plus_2030_py\output\scenarios\scenario_params_fenez.csv"
    hvac_csv  = r"D:\Documents\E_Plus_2030_py\output\scenarios\scenario_params_hvac.csv"
    vent_csv  = r"D:\Documents\E_Plus_2030_py\output\scenarios\scenario_params_vent.csv"

    scenario_index = 0    # Example
    # You can customize these:
    n_morris_trajectories = 10
    n_sobol_samples = 128

    # Run the main sensitivity analyses
    run_sensitivity_analyses(
        dhw_csv,
        elec_csv,
        fenez_csv,
        hvac_csv,
        vent_csv,
        scenario_index=scenario_index,
        n_morris_trajectories=n_morris_trajectories,
        n_sobol_samples=n_sobol_samples
    )







    """
man.py

This file imports 'surrogate_model_main' and runs the demo.
Usage:
    python man.py
"""

import cal_1.surrogate_model

if __name__ == "__main__":
    cal_1.surrogate_model.run_demo()


  --- File Contents End ---

  File: man_cal.py
  --- File Contents Start ---
# man.py

from cal_1.calibration_main import main

if __name__ == "__main__":
    main()

  --- File Contents End ---

  File: sensitivity_analysis_main.py
  --- File Contents Start ---
"""
sensitivity_analysis_main.py

A full example script that:
  1) Reads scenario parameter CSV files (DHW, Elec, Fenez, HVAC, Vent).
  2) Merges them into a single DataFrame of param_name => assigned_value.
  3) Creates SALib parameter ranges for each param_name (fixing or bounding
     zero or negative values to avoid illegal bounds).
  4) Runs SALib's Morris and Sobol sensitivity analyses using the routines below.
  5) Outputs results for further analysis or calibration steps.
"""

import os
import numpy as np
import pandas as pd

# SALib imports
from SALib.sample import saltelli, morris
from SALib.analyze import sobol, morris as morris_analyze
from typing import Dict, Any


###############################################################################
# 1) SALib Helper Functions
###############################################################################
def build_problem_dict(params_meta: pd.DataFrame) -> Dict[str, Any]:
    """
    Convert a DataFrame describing parameter ranges into
    a SALib 'problem' dictionary.

    Expected columns in params_meta:
      - name: str (unique param name)
      - min_value: float
      - max_value: float
    """
    problem = {
        'num_vars': len(params_meta),
        'names': params_meta['name'].tolist(),
        'bounds': []
    }

    for _, row in params_meta.iterrows():
        problem['bounds'].append([row['min_value'], row['max_value']])

    return problem


def model_function(
    param_values: np.ndarray,
    problem_dict: Dict[str, Any],
    simulate_func
) -> np.ndarray:
    """
    Evaluate a user-provided 'simulate_func' for each row in param_values.

    :param param_values: 2D array (N x D) of parameter samples
    :param problem_dict: SALib problem dict with param names
    :param simulate_func: function taking {param_name: value} => float
    :return: np.array of shape (N,) with the output for each row
    """
    results = []
    param_names = problem_dict['names']

    for row in param_values:
        # Build the {param_name: value} dict
        param_set = {}
        for i, p_name in enumerate(param_names):
            param_set[p_name] = row[i]

        # Evaluate user simulation or mock function
        val = simulate_func(param_set)
        results.append(val)

    return np.array(results)


def run_morris_sensitivity(
    params_meta: pd.DataFrame,
    simulate_func,
    n_trajectories: int = 10,
    num_levels: int = 4
):
    """
    Perform a Morris (Elementary Effects) sensitivity analysis using SALib.
    """
    problem_dict = build_problem_dict(params_meta)

    # 1) Generate Morris samples
    sample_values = morris.sample(
        problem_dict,
        N=n_trajectories,
        num_levels=num_levels,
        optimal_trajectories=None,
        local_optimization=False
    )

    # 2) Evaluate
    Y = model_function(sample_values, problem_dict, simulate_func)

    # 3) Analyze
    morris_result = morris_analyze.analyze(
        problem_dict,
        sample_values,
        Y,
        conf_level=0.95,
        print_to_console=False
    )
    return morris_result, sample_values, Y


def run_sobol_sensitivity(
    params_meta: pd.DataFrame,
    simulate_func,
    n_samples: int = 1000
):
    """
    Perform a Sobol (Saltelli) sensitivity analysis using SALib.
    """
    problem_dict = build_problem_dict(params_meta)

    # 1) Generate Sobol samples
    sample_values = saltelli.sample(
        problem_dict,
        n_samples,
        calc_second_order=True
    )

    # 2) Evaluate
    Y = model_function(sample_values, problem_dict, simulate_func)

    # 3) Analyze
    sobol_result = sobol.analyze(
        problem_dict,
        Y,
        calc_second_order=True,
        print_to_console=False
    )
    return sobol_result, sample_values, Y


###############################################################################
# 2) Code that reads scenario CSVs and merges them
###############################################################################
def load_scenario_params(
    dhw_csv: str,
    elec_csv: str,
    fenez_csv: str,
    hvac_csv: str,
    vent_csv: str,
    scenario_index_filter=0
) -> pd.DataFrame:
    """
    Reads multiple scenario parameter CSVs (DHW, Elec, Fenez, HVAC, Vent).
    Filters by scenario_index if desired.
    Returns a single DataFrame with columns:
      [param_name, assigned_value]
    We'll define min_value/max_value in another function.
    """
    # 1) Load each CSV
    df_dhw   = pd.read_csv(dhw_csv)
    df_elec  = pd.read_csv(elec_csv)
    df_fenez = pd.read_csv(fenez_csv)
    df_hvac  = pd.read_csv(hvac_csv)
    df_vent  = pd.read_csv(vent_csv)

    # 2) Filter by scenario_index if provided
    if scenario_index_filter is not None:
        df_dhw   = df_dhw[df_dhw["scenario_index"] == scenario_index_filter]
        df_elec  = df_elec[df_elec["scenario_index"] == scenario_index_filter]
        df_fenez = df_fenez[df_fenez["scenario_index"] == scenario_index_filter]
        df_hvac  = df_hvac[df_hvac["scenario_index"] == scenario_index_filter]
        df_vent  = df_vent[df_vent["scenario_index"] == scenario_index_filter]

    # 3) Concatenate them
    df_all = pd.concat([df_dhw, df_elec, df_fenez, df_hvac, df_vent],
                       ignore_index=True)

    # We'll keep only relevant columns: "param_name", "assigned_value"
    df_all = df_all[["param_name", "assigned_value"]]

    return df_all


def create_parameter_ranges(
    df_params: pd.DataFrame,
    default_min: float = 0.5,
    default_max: float = 2.0
) -> pd.DataFrame:
    """
    Given a DataFrame of param_name => assigned_value from scenario CSVs,
    define a 'min_value' and 'max_value' for each unique param_name.

    **Fix**: ensure min_value < max_value to avoid SALib "Bounds are not legal".
    If assigned_value <= 0, we fallback to default_min..default_max.

    Returns a DataFrame with columns:
      [name, min_value, max_value]
    """
    param_names = df_params["param_name"].unique()
    rows = []

    # Group by param_name and take mean assigned_value
    grouped = df_params.groupby("param_name")["assigned_value"].mean().reset_index()

    for _, row_grp in grouped.iterrows():
        pname = row_grp["param_name"]
        val   = row_grp["assigned_value"]

        # If val is NaN or <= 0 => fallback
        if pd.isna(val) or val <= 0:
            mn, mx = default_min, default_max
        else:
            # e.g. Â±50% around 'val'
            mn = val * 0.5
            mx = val * 1.5

            # If that yields something invalid (mn >= mx), fix it
            if mn >= mx:
                # We'll bump up mx slightly
                mx = mn + abs(mn) * 0.1 + 0.001  # e.g. 10% more than mn

            # If either is negative or zero (in case val < 1):
            if mn <= 0:
                mn = 0.01
            if mx <= mn:
                mx = mn + 0.1

            # Also, as a final check, if somehow mn >= mx, do fallback
            if mn >= mx:
                mn, mx = default_min, default_max

        rows.append({"name": pname, "min_value": mn, "max_value": mx})

    df_ranges = pd.DataFrame(rows)
    return df_ranges


###############################################################################
# 3) Mock simulation function
###############################################################################
def mock_simulation_function(param_dict: Dict[str, float]) -> float:
    """
    Placeholder for your real or surrogate simulation.
    We'll create a random "energy" = sum(param_dict.values()) + noise.
    In reality, you'd:
      - Write param_dict to an IDF, run EnergyPlus, parse results => single float
      OR
      - Evaluate a surrogate model that approximates E+ results
    """
    base_sum = sum(param_dict.values())
    noise = np.random.uniform(-0.5, 0.5)
    return base_sum + noise


###############################################################################
# 4) Main logic function (so we can call it from elsewhere)
###############################################################################
def run_sensitivity_analyses(
    dhw_csv: str,
    elec_csv: str,
    fenez_csv: str,
    hvac_csv: str,
    vent_csv: str,
    scenario_index: int = 0,
    n_morris_trajectories: int = 10,
    n_sobol_samples: int = 128
):
    """
    Runs both Morris and Sobol sensitivity analyses and prints or saves results.

    :param dhw_csv: CSV path for DHW scenario parameters
    :param elec_csv: CSV path for Elec scenario parameters
    :param fenez_csv: CSV path for Fenestration scenario parameters
    :param hvac_csv: CSV path for HVAC scenario parameters
    :param vent_csv: CSV path for Vent scenario parameters
    :param scenario_index: which scenario_index to filter
    :param n_morris_trajectories: N for Morris sampling
    :param n_sobol_samples: N for Sobol sampling
    """
    # ------------------------------------------------------------------------
    # A) Load scenario params & create param ranges
    # ------------------------------------------------------------------------
    df_scen = load_scenario_params(
        dhw_csv,
        elec_csv,
        fenez_csv,
        hvac_csv,
        vent_csv,
        scenario_index_filter=scenario_index
    )
    print("[INFO] Merged scenario param count:", len(df_scen))
    print(df_scen.head(10))

    df_param_ranges = create_parameter_ranges(df_scen)
    print("\n[INFO] Parameter Ranges for Sensitivity:")
    print(df_param_ranges)

    # ------------------------------------------------------------------------
    # B) MORRIS Sensitivity
    # ------------------------------------------------------------------------
    print("\n=== Running MORRIS Sensitivity Analysis ===")
    morris_res, X_morris, Y_morris = run_morris_sensitivity(
        df_param_ranges,
        mock_simulation_function,
        n_trajectories=n_morris_trajectories,
        num_levels=4
    )

    print("[MORRIS] mu_star:", morris_res['mu_star'])
    print("[MORRIS] sigma:", morris_res['sigma'])
    print("[MORRIS] param order:", df_param_ranges["name"].tolist())

    # (Optional) Save Morris results
    df_morris = pd.DataFrame({
        "param_name": df_param_ranges["name"],
        "mu_star": morris_res["mu_star"],
        "sigma": morris_res["sigma"]
    })
    df_morris.to_csv("morris_sensitivity_results.csv", index=False)
    print("[MORRIS] Results saved to morris_sensitivity_results.csv")

    # ------------------------------------------------------------------------
    # C) SOBOL Sensitivity
    # ------------------------------------------------------------------------
    print("\n=== Running SOBOL Sensitivity Analysis ===")
    sobol_res, X_sobol, Y_sobol = run_sobol_sensitivity(
        df_param_ranges,
        mock_simulation_function,
        n_samples=n_sobol_samples
    )
    print("[SOBOL] S1:", sobol_res["S1"])
    print("[SOBOL] ST:", sobol_res["ST"])
    print("[SOBOL] param order:", df_param_ranges["name"].tolist())

    # (Optional) Save Sobol results
    df_sobol = pd.DataFrame({
        "param_name": df_param_ranges["name"],
        "S1": sobol_res["S1"],
        "ST": sobol_res["ST"]
    })
    df_sobol.to_csv("sobol_sensitivity_results.csv", index=False)
    print("[SOBOL] Results saved to sobol_sensitivity_results.csv")

    print("\n[INFO] Done with sensitivity analysis!")

  --- File Contents End ---

  File: surrogate_model.py
  --- File Contents Start ---
"""
surrogate_model_main.py

Demonstration of:
  1) Loading parameter data and target data (single scalar or time-series).
  2) Optionally filtering to top N most sensitive parameters (based on prior SALib results).
  3) Training either an AggregateSurrogate (single-output) or TimeSeriesSurrogate (multi-output).
  4) Saving and re-loading the trained model for future usage.

Dependencies:
    pip install numpy pandas scikit-learn joblib
"""

import os
import numpy as np
import pandas as pd
from typing import List

# For saving/loading the model
import joblib

from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor

class AggregateSurrogate:
    """
    A simple class that:
      1. Accepts training data in X (param sets) and y (scalar target).
      2. Trains a single-output regression model (RandomForest by default).
      3. Predicts the scalar output for new param sets.
    """
    def __init__(self, model=None):
        if model is None:
            # default model
            self.model = RandomForestRegressor(n_estimators=50, random_state=42)
        else:
            self.model = model
        self.is_fitted = False

    def fit(self, X: pd.DataFrame, y: np.ndarray):
        self.model.fit(X, y)
        self.is_fitted = True

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        if not self.is_fitted:
            raise RuntimeError("Model must be fitted before calling predict.")
        return self.model.predict(X)


class TimeSeriesSurrogate:
    """
    A class that predicts multiple outputs for each param set.
    For example, predicting a 24-hour load profile or 8760-hour annual load.

    By default, uses a RandomForestRegressor wrapped in sklearn's MultiOutputRegressor.
    That ensures the model can handle multi-output arrays (Y shape: (n_samples, n_times)).
    """
    def __init__(self, model=None):
        if model is None:
            base_model = RandomForestRegressor(n_estimators=50, random_state=42)
            self.model = MultiOutputRegressor(base_model)
        else:
            self.model = model
        self.is_fitted = False

    def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
        self.model.fit(X, Y)
        self.is_fitted = True

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        if not self.is_fitted:
            raise RuntimeError("Model must be fitted before calling predict.")
        return self.model.predict(X)


def load_data_for_surrogate(
    param_csv: str,
    target_csv: str,
    top_params_csv: str = None,
    top_n: int = None,
    multi_output: bool = False
) -> (pd.DataFrame, pd.DataFrame):
    """
    Reads 'param_csv' which should contain columns for parameter features
    (one column per parameter). Also reads 'target_csv' for the outputs.

    If 'top_params_csv' is provided, we read it to get a list of the top
    N parameter names (e.g. from a SALib sensitivity analysis).
    Then we filter param_csv to those columns only.

    :param param_csv: Path to CSV with shape (n_samples, n_params).
                     Each row is one sample's parameter values.
    :param target_csv: Path to CSV with shape (n_samples, ?).
                     If multi_output=False => single column with scalar target.
                     If multi_output=True  => multiple columns for time-series or multi-target.
    :param top_params_csv: Optional path to a CSV with columns: [param_name, some_sensitivity_metric]
    :param top_n: If provided, we keep only the top N param_names based on that metric.
    :param multi_output: If True, we assume target_csv has multiple columns => multi-output.
    :return: (df_X, df_Y)
    """
    # 1) Read parameter data
    df_X = pd.read_csv(param_csv)
    # e.g. columns: "infiltration_base", "occupant_density", "wall_u_value", etc.

    # 2) If we have a CSV of top parameters, filter
    if top_params_csv and top_n:
        df_top = pd.read_csv(top_params_csv)
        # Suppose df_top has columns: param_name, mu_star (or S1), etc.
        # We'll sort by the sensitivity metric descending
        # and keep the top N param_names
        df_top_sorted = df_top.sort_values(by="mu_star", ascending=False)
        top_params = df_top_sorted["param_name"].head(top_n).tolist()

        # Now keep only those columns in df_X that are in top_params
        existing_cols = [c for c in df_X.columns if c in top_params]
        df_X = df_X[existing_cols]
        print(f"[INFO] Filtered parameter columns to top {len(existing_cols)} from {top_params_csv}.")
    else:
        print("[INFO] Using all parameter columns from param_csv; no top-parameter filter applied.")

    # 3) Read target data
    df_Y = pd.read_csv(target_csv)

    # Ensure shapes align: same number of rows, etc.
    if len(df_X) != len(df_Y):
        raise ValueError(f"Mismatch in row counts: df_X={len(df_X)}, df_Y={len(df_Y)}")

    return df_X, df_Y


def save_model(model, filename: str):
    """
    Saves the fitted surrogate model to a .pkl file using joblib.
    :param model: An instance of AggregateSurrogate or TimeSeriesSurrogate
    :param filename: output path
    """
    joblib.dump(model, filename)
    print(f"[INFO] Model saved to {filename}")


def load_model(filename: str):
    """
    Loads a previously saved surrogate model.
    """
    print(f"[INFO] Loading model from {filename}")
    model = joblib.load(filename)
    return model


def run_demo():
    """
    Runs two demonstration examples:
      - Example 1: Single scalar target
      - Example 2: Multi-output (time-series) target
    """
    print("=== Example 1: Single Scalar Surrogate ===")
    np.random.seed(42)
    n_samples = 40
    param_names = ["paramA", "paramB", "paramC", "paramD"]
    df_param_demo = pd.DataFrame(np.random.rand(n_samples, 4), columns=param_names)
    # Let's define a random scalar target => "energy"
    target_vals = (1000
                   + 300 * df_param_demo["paramA"]
                   + 200 * df_param_demo["paramB"]
                   + 50  * df_param_demo["paramC"]
                   + 10  * df_param_demo["paramD"]
                   + np.random.randn(n_samples) * 10)
    df_target_demo = pd.DataFrame({"energy": target_vals})

    # Save them to CSV (mock)
    df_param_demo.to_csv("demo_params_single.csv", index=False)
    df_target_demo.to_csv("demo_target_single.csv", index=False)

    # Load them with the load_data_for_surrogate function
    df_X_single, df_Y_single = load_data_for_surrogate(
        param_csv="demo_params_single.csv",
        target_csv="demo_target_single.csv"
    )

    # Now we train a single-output surrogate
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        df_X_single, df_Y_single, test_size=0.2, random_state=42
    )

    y_train_np = y_train.values.ravel()
    y_test_np  = y_test.values.ravel()

    agg_model = AggregateSurrogate()
    agg_model.fit(X_train, y_train_np)

    preds = agg_model.predict(X_test)
    mse = np.mean((preds - y_test_np)**2)
    print(f"[SingleOutput] Test MSE: {mse:.2f}")

    # Save the model
    save_model(agg_model, "my_single_output_model.pkl")

    print("\n=== Example 2: Multi-Output Time-Series Surrogate ===")
    param_names_2 = ["param1", "param2", "param3"]
    n_hours = 24
    n_samples_2 = 30

    df_param2 = pd.DataFrame(np.random.rand(n_samples_2, 3), columns=param_names_2)

    # Create a synthetic time-series: shape (n_samples, 24)
    Y_data_2 = []
    for i in range(n_samples_2):
        p1 = df_param2.loc[i, "param1"]
        p2 = df_param2.loc[i, "param2"]
        p3 = df_param2.loc[i, "param3"]
        amplitude = 5 + 10 * p1
        offset    = 20 + 5 * p2
        freqShift = 0.5 + p3
        profile_i = [
            offset + amplitude * np.sin((h * np.pi) / (12 * freqShift))
            for h in range(n_hours)
        ]
        Y_data_2.append(profile_i)

    df_target2 = pd.DataFrame(Y_data_2, columns=[f"H{i}" for i in range(n_hours)])

    df_param2.to_csv("demo_params_multi.csv", index=False)
    df_target2.to_csv("demo_target_multi.csv", index=False)

    # Load them
    df_X_multi, df_Y_multi = load_data_for_surrogate(
        param_csv="demo_params_multi.csv",
        target_csv="demo_target_multi.csv",
        multi_output=True
    )

    X_train2, X_test2, Y_train2, Y_test2 = train_test_split(
        df_X_multi, df_Y_multi, test_size=0.2, random_state=42
    )

    ts_model = TimeSeriesSurrogate()
    ts_model.fit(X_train2, Y_train2)

    preds2 = ts_model.predict(X_test2)
    mse2 = np.mean((preds2 - Y_test2.values)**2)
    print(f"[MultiOutput] Test MSE over 24-hour profile: {mse2:.2f}")

    # Save the multi-output model
    save_model(ts_model, "my_multi_output_model.pkl")

    # Example of re-loading the single-output model and predicting
    loaded_agg = load_model("my_single_output_model.pkl")
    new_preds = loaded_agg.predict(X_test.iloc[:2])
    print("[Loaded single-output model] Predictions for first 2 rows:", new_preds)


if __name__ == "__main__":
    # If you run this file directly, it calls run_demo().
    run_demo()

  --- File Contents End ---

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cal_1\__pycache__

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cal_2

  File: main_calibration.py
  --- File Contents Start ---
#!/usr/bin/env python3
# main_calibration.py

"""
Calibration script with three optimization methods:
  1) Random Search
  2) Genetic Algorithm (GA)
  3) Bayesian Optimization (via scikit-optimize)

Additionally:
  - Saves the best parameters + score to a .joblib file
  - Also saves them to a CSV file

Instructions:
  1) Modify 'scenario_dir' to point to your scenario_params_*.csv location.
  2) Adjust 'objective_function' with your real mismatch or calibration logic.
  3) Set 'method' = "random", "ga", or "bayes" to choose the algorithm.
  4) Run script directly or via man.py: python man.py
"""

import os
import csv
import random
import joblib
import numpy as np
import pandas as pd

from typing import List, Tuple, Dict, Callable

# Bayesian optimization tools (scikit-optimize)
try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer
except ImportError:
    gp_minimize = None
    Real = None
    Integer = None


##############################################################################
# 1) LOAD SCENARIO PARAMS
##############################################################################

def load_scenario_parameter_csvs(scenario_dir: str) -> pd.DataFrame:
    """
    Reads scenario_params_*.csv from scenario_dir, merges them into one DataFrame.
    Expects columns: [scenario_index, param_name, assigned_value].
    """
    dfs = []
    for fname in os.listdir(scenario_dir):
        if fname.startswith("scenario_params_") and fname.endswith(".csv"):
            fpath = os.path.join(scenario_dir, fname)
            if not os.path.isfile(fpath):
                continue
            df_temp = pd.read_csv(fpath)
            required = {"scenario_index", "param_name", "assigned_value"}
            missing = required - set(df_temp.columns)
            if missing:
                print(f"[WARNING] {fname} missing {missing}, skipping.")
                continue
            dfs.append(df_temp)
    if not dfs:
        raise FileNotFoundError(f"No scenario_params_*.csv found in {scenario_dir}.")
    df_merged = pd.concat(dfs, ignore_index=True)
    return df_merged


def pivot_scenario_params(df_params: pd.DataFrame) -> pd.DataFrame:
    """
    Pivot => one row per scenario_index, columns=param_name, values=assigned_value
    """
    df_pivot = df_params.pivot_table(
        index="scenario_index",
        columns="param_name",
        values="assigned_value",
        aggfunc="first"
    ).reset_index()
    df_pivot.columns.name = None
    return df_pivot


##############################################################################
# 2) DEFINE PARAMETER SPEC + HELPER
##############################################################################

class ParameterSpec:
    """
    Holds metadata for a single calibration parameter:
      - name
      - min_value, max_value
      - is_integer
    """
    def __init__(self, name: str, min_value: float, max_value: float,
                 is_integer: bool = False):
        self.name = name
        self.min_value = min_value
        self.max_value = max_value
        self.is_integer = is_integer

    def sample_random(self) -> float:
        val = random.uniform(self.min_value, self.max_value)
        if self.is_integer:
            val = int(round(val))
        return val


def build_param_specs_from_pivot(
    df_pivot: pd.DataFrame,
    exclude_cols: List[str] = None
) -> List[ParameterSpec]:
    """
    For each numeric column (except exclude_cols), create a ParameterSpec.
    We guess a range from (col.min, col.max). If min==max, expand slightly.
    """
    if exclude_cols is None:
        exclude_cols = ["scenario_index"]

    specs = []
    for col in df_pivot.columns:
        if col in exclude_cols:
            continue
        # Convert to numeric
        series = pd.to_numeric(df_pivot[col], errors="coerce").dropna()
        if series.empty:
            continue

        cmin = series.min()
        cmax = series.max()
        if cmin == cmax:
            # forcibly expand
            if cmin == 0.0:
                cmin = -0.001
                cmax = 0.001
            else:
                cmin *= 0.95
                cmax *= 1.05

        is_int = False  # Adjust if you need integer parameters
        specs.append(ParameterSpec(col, float(cmin), float(cmax), is_int))
    return specs


##############################################################################
# 3) DEFINE OBJECTIVE (MISMATCH)
##############################################################################

def objective_function(param_dict: Dict[str, float]) -> float:
    """
    A placeholder objective for calibration.
    In a real scenario, you'd:
      - Build an IDF from param_dict
      - Run E+ 
      - Compare vs. real data => compute CV(RMSE) or MBE
    Here, we do a contrived formula:
       mismatch = sum( (param_value - 10)^2 ).
    Return the scalar "score" to MINIMIZE.
    """
    mismatch = 0.0
    for val in param_dict.values():
        mismatch += (val - 10.0) ** 2
    return mismatch


##############################################################################
# 4) CALIBRATION ALGORITHMS
##############################################################################

# 4A) Random Search
def random_search(
    param_specs: List[ParameterSpec],
    objective_func: Callable[[Dict[str, float]], float],
    n_iterations: int = 50
) -> Tuple[Dict[str, float], float]:
    """
    Randomly sample param sets within [min, max], evaluate objective.
    Returns best param_dict + best_score.
    """
    best_score = float("inf")
    best_params = None
    for _ in range(n_iterations):
        p_dict = {}
        for spec in param_specs:
            p_dict[spec.name] = spec.sample_random()
        score = objective_func(p_dict)
        if score < best_score:
            best_score = score
            best_params = p_dict
    return best_params, best_score


# 4B) Genetic Algorithm
def ga_optimize(
    param_specs: List[ParameterSpec],
    objective_func: Callable[[Dict[str, float]], float],
    pop_size: int = 20,
    max_generations: int = 10,
    mutation_prob: float = 0.1
) -> Tuple[Dict[str, float], float]:
    """
    Basic GA approach:
      - Initialize population randomly
      - Evaluate
      - Tournament selection, crossover, mutate
      - Repeat for max_generations
      - Return best param_dict + best_score
    """

    # Initialize population
    population = []
    for _ in range(pop_size):
        p_dict = {}
        for spec in param_specs:
            p_dict[spec.name] = spec.sample_random()
        score = objective_func(p_dict)
        population.append((p_dict, score))

    def tournament_select(pop: List[Tuple[Dict[str, float], float]], k=3):
        contenders = random.sample(pop, k)
        contenders.sort(key=lambda x: x[1])
        return contenders[0]  # best is lowest score

    def crossover(pA: Dict[str, float], pB: Dict[str, float]) -> Dict[str, float]:
        child = {}
        for k in pA.keys():
            if random.random() < 0.5:
                child[k] = pA[k]
            else:
                child[k] = pB[k]
        return child

    def mutate(p: Dict[str, float]) -> Dict[str, float]:
        c = dict(p)
        for spec in param_specs:
            if random.random() < mutation_prob:
                c[spec.name] = spec.sample_random()
        return c

    for gen in range(max_generations):
        # sort by ascending score
        population.sort(key=lambda x: x[1])
        print(f"[GA Gen {gen}] best score = {population[0][1]:.4f}")

        # elitism: keep top 2
        new_pop = population[:2]

        # fill up new_pop
        while len(new_pop) < pop_size:
            pA = tournament_select(population)
            pB = tournament_select(population)
            child_params = crossover(pA[0], pB[0])
            child_params = mutate(child_params)
            child_score = objective_func(child_params)
            new_pop.append((child_params, child_score))

        population = new_pop

    population.sort(key=lambda x: x[1])
    best_params, best_score = population[0]
    return best_params, best_score


# 4C) Bayesian Optimization
def bayes_optimize(
    param_specs: List[ParameterSpec],
    objective_func: Callable[[Dict[str, float]], float],
    n_calls: int = 30
) -> Tuple[Dict[str, float], float]:
    """
    Use scikit-optimize's gp_minimize for Bayesian optimization.
    If it's unavailable, fallback to random_search.
    """
    if gp_minimize is None or Real is None:
        print("[WARN] scikit-optimize not installed. Fallback to random search.")
        return random_search(param_specs, objective_func, n_iterations=n_calls)

    # Build dimension space
    param_names = []
    dimensions = []
    for spec in param_specs:
        param_names.append(spec.name)
        if spec.is_integer:
            dimensions.append(Integer(spec.min_value, spec.max_value, name=spec.name))
        else:
            dimensions.append(Real(spec.min_value, spec.max_value, name=spec.name))

    def skopt_obj(x):
        p_dict = {}
        for i, nm in enumerate(param_names):
            p_dict[nm] = x[i]
        return objective_func(p_dict)

    res = gp_minimize(
        skopt_obj,
        dimensions,
        n_calls=n_calls,
        random_state=42
    )
    best_score = res.fun
    best_x = res.x
    best_params = {nm: bx for nm, bx in zip(param_names, best_x)}
    return best_params, best_score


##############################################################################
# 5) MAIN DEMO
##############################################################################

def main():
    # 1) load scenario params
    scenario_dir = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    df_params_long = load_scenario_parameter_csvs(scenario_dir)
    df_params_pivot = pivot_scenario_params(df_params_long)
    print("[INFO] pivot shape:", df_params_pivot.shape)
    print(df_params_pivot.head(3))

    # 2) build param specs
    param_specs = build_param_specs_from_pivot(df_params_pivot, exclude_cols=["scenario_index"])
    print("[INFO] Number of param specs:", len(param_specs))
    for s in param_specs[:5]:
        print(f"  {s.name}: min={s.min_value:.4f}, max={s.max_value:.4f}, int={s.is_integer}")

    # 3) choose method: "random", "ga", or "bayes"
    method = "bayes"

    # 4) run the chosen method
    if method == "random":
        best_params, best_score = random_search(param_specs, objective_function, n_iterations=50)
    elif method == "ga":
        best_params, best_score = ga_optimize(param_specs, objective_function, pop_size=20, max_generations=5)
    elif method == "bayes":
        best_params, best_score = bayes_optimize(param_specs, objective_function, n_calls=20)
    else:
        raise ValueError(f"Unknown method: {method}")

    print(f"\n=== Calibration via {method} finished ===")
    print("Best score:", best_score)
    print("Best params:")
    for k, v in best_params.items():
        print(f"  {k} = {v:.4f}")

    # 5) Save results to joblib
    best_params_path = f"best_params_{method}.joblib"
    joblib.dump(best_params, best_params_path)
    print(f"[INFO] Saved best params to {best_params_path}")

    # 6) Also save to CSV
    best_params_csv = f"best_params_{method}.csv"
    with open(best_params_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Method", "ParamName", "ParamValue", "BestScore"])
        for param_name, param_value in best_params.items():
            writer.writerow([method, param_name, f"{param_value:.4f}", f"{best_score:.4f}"])

    print(f"[INFO] Also saved best params to {best_params_csv}")


if __name__ == "__main__":
    main()

  --- File Contents End ---

  File: main_sensitivity.py
  --- File Contents Start ---
#!/usr/bin/env python3
# main_sensitivity.py

"""
Example script to demonstrate sensitivity analysis using SALib,
integrating scenario-based parameters and simulation/real data.

Requirements:
    pip install SALib
    pip install pandas
    pip install numpy

Data folder references:
    D:\Documents\E_Plus_2030_py\output\scenarios\scenario_params_*.csv
    D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv
    D:\Documents\E_Plus_2030_py\output\results\mock_merged_daily_mean.csv

Workflow:
  1) Load & merge scenario parameter CSVs
  2) Pivot to get columns for each param
  3) Build a SALib problem (skip or expand zero-variance params)
  4) Load sim & real data
  5) Run Morris or Sobol from SALib
  6) Print sensitivity indices
"""

import os
import numpy as np
import pandas as pd

# SALib (for Morris / Sobol)
from SALib.sample import saltelli, morris
from SALib.analyze import sobol, morris as morris_analyze


###############################################################################
# 1) LOAD & MERGE SCENARIO PARAMETERS
###############################################################################

def load_scenario_parameter_csvs(scenario_dir: str) -> pd.DataFrame:
    """
    Reads multiple scenario_params_*.csv files (dhw, elec, fenez, hvac, vent)
    from 'scenario_dir' and merges them into one DataFrame.

    Expects columns: [scenario_index, ogc_fid, param_name, assigned_value].
    Returns a single DataFrame with those columns merged from all files.
    """
    dfs = []
    for fname in os.listdir(scenario_dir):
        if fname.startswith("scenario_params_") and fname.endswith(".csv"):
            fpath = os.path.join(scenario_dir, fname)
            df_temp = pd.read_csv(fpath)
            # Quick check for columns
            required_cols = {"scenario_index", "ogc_fid", "param_name", "assigned_value"}
            missing = required_cols - set(df_temp.columns)
            if missing:
                raise ValueError(f"File '{fname}' is missing columns: {missing}")
            dfs.append(df_temp)

    if not dfs:
        raise FileNotFoundError(f"No scenario_params_*.csv files found in {scenario_dir}.")

    df_merged = pd.concat(dfs, ignore_index=True)
    return df_merged


def pivot_scenario_params(df_params: pd.DataFrame) -> pd.DataFrame:
    """
    Convert from long format (one row per param) into wide format,
    so each (scenario_index, ogc_fid) becomes a single row,
    with columns for each param_name and values = assigned_value.

    :return: pivoted DataFrame with columns:
             ['scenario_index', 'ogc_fid', 'param1', 'param2', ...]
    """
    df_pivot = df_params.pivot_table(
        index=["scenario_index", "ogc_fid"],
        columns="param_name",
        values="assigned_value",
        aggfunc="first"
    ).reset_index()

    df_pivot.columns.name = None  # remove pivot naming
    return df_pivot

###############################################################################
# 2) LOAD SIMULATION & REAL DATA
###############################################################################

def load_sim_results(sim_csv: str) -> pd.DataFrame:
    """
    Example: load 'merged_daily_mean_mocked.csv'
    which has columns like:
      BuildingID | VariableName | 01-Jan | 02-Jan | 03-Jan | ...
    """
    return pd.read_csv(sim_csv)

def load_real_data(real_csv: str) -> pd.DataFrame:
    """
    Example: load 'mock_merged_daily_mean.csv'
    which has columns like:
      BuildingID | VariableName | 01-Jan | 02-Jan | 03-Jan | ...
    """
    return pd.read_csv(real_csv)

###############################################################################
# 3) SALib 'PROBLEM' & ZERO-VARIANCE HANDLING
###############################################################################

def build_salib_problem_from_pivoted(
    df_pivot: pd.DataFrame,
    exclude_cols: list = None
) -> dict:
    """
    Build a SALib 'problem' from the pivoted scenario DataFrame.
    We'll skip columns in exclude_cols and skip any parameter that is
    non-numeric or has zero variance (min == max).

    If min==max but not zero, we forcibly expand by Â±5% for demonstration.
    If it's exactly 0.0 for all scenarios, we skip it or forcibly expand Â±0.001.

    :param df_pivot: pivoted DataFrame with shape (n_scenarios, many_params)
    :param exclude_cols: e.g. ["scenario_index", "ogc_fid"]
    :return: SALib problem dict with keys: num_vars, names, bounds
    """
    if exclude_cols is None:
        exclude_cols = ["scenario_index", "ogc_fid"]

    param_cols = []
    bounds = []

    for col in df_pivot.columns:
        if col in exclude_cols:
            continue

        # Convert to numeric if possible, drop NaNs
        col_data = pd.to_numeric(df_pivot[col], errors="coerce").dropna()
        if col_data.empty:
            # No valid numeric data => skip
            print(f"[DEBUG] Skipping '{col}' - no numeric data.")
            continue

        col_min = col_data.min()
        col_max = col_data.max()

        # If exactly the same value
        if col_min == col_max:
            # Option: skip entirely
            # print(f"[INFO] Param '{col}' is constant at {col_min}. Skipping it.")
            # continue

            # Or forcibly expand
            if col_min == 0.0:
                col_min = -0.001
                col_max = 0.001
                print(f"[WARNING] Param '{col}' is 0.0 everywhere; forcing range Â±0.001.")
            else:
                col_min *= 0.95
                col_max *= 1.05
                print(f"[WARNING] Param '{col}' was constant at {col_data.iloc[0]:.4f}; expanded Â±5% to [{col_min:.4f}, {col_max:.4f}].")

        # Double-check legal range
        if col_min >= col_max:
            print(f"[WARNING] Param '{col}' has invalid range [{col_min}, {col_max}] -> skipping.")
            continue

        param_cols.append(col)
        bounds.append([float(col_min), float(col_max)])

    problem = {
        "num_vars": len(param_cols),
        "names": param_cols,
        "bounds": bounds
    }

    # Debug: Print them
    print("\n[SALib PROBLEM BOUNDS]")
    for pname, (low, high) in zip(problem["names"], problem["bounds"]):
        print(f"  {pname}: {low} -> {high}")

    return problem

###############################################################################
# 4) THE MISMATCH / MODEL FUNCTION
###############################################################################
def simulate_scenario_mismatch(param_dict: dict,
                               df_sim: pd.DataFrame,
                               df_real: pd.DataFrame) -> float:
    bldg_id = 0
    varname = "Electricity:Facility [J](Hourly)"

    # Filter to building 0 & that variable
    df_sim_sel = df_sim[
        (df_sim["BuildingID"] == bldg_id) &
        (df_sim["VariableName"] == varname)
    ]
    df_real_sel = df_real[
        (df_real["BuildingID"] == bldg_id) &
        (df_real["VariableName"] == varname)
    ]

    # If no rows, return penalty
    if df_sim_sel.empty or df_real_sel.empty:
        return 999999.0

    # Option B: Pick the first data column that isn't ID/VarName
    possible_day_cols = [c for c in df_sim_sel.columns if c not in ["BuildingID", "VariableName"]]
    if not possible_day_cols:
        return 999999.0

    day_col = possible_day_cols[0]  # e.g. "01-Jan" if it exists
    if day_col not in df_sim_sel.columns:
        return 999999.0

    sim_val = df_sim_sel[day_col].values[0]
    real_val = df_real_sel[day_col].values[0]

    diff = abs(sim_val - real_val)

    infiltration = param_dict.get("infiltration_base", 1.0)
    occupant_dens = param_dict.get("occupant_density_m2_per_person", 30.0)
    mismatch = diff * (infiltration / 1.0) * (occupant_dens / 30.0)

    return mismatch

###############################################################################
# 5) RUN SENSITIVITY (MORRIS OR SOBOL)
###############################################################################

def run_morris_sensitivity(problem: dict,
                           n_trajectories: int,
                           num_levels: int,
                           simulate_func,
                           df_sim: pd.DataFrame,
                           df_real: pd.DataFrame):
    """
    Perform Morris (Elementary Effects) with SALib on the 'problem' definition.
    :param problem: SALib problem with {num_vars, names, bounds}
    :param n_trajectories: number of Morris trajectories
    :param num_levels: e.g. 4
    :param simulate_func: function(param_dict, df_sim, df_real)->float
    :return: (morris_res, param_values, Y)
    """
    # 1) Generate samples
    param_values = morris.sample(
        problem,
        N=n_trajectories,
        num_levels=num_levels,
        optimal_trajectories=None,
        local_optimization=False
    )

    # 2) Evaluate each param set
    Y = []
    param_names = problem["names"]
    for row in param_values:
        p_dict = {param_names[i]: row[i] for i in range(len(param_names))}
        score = simulate_func(p_dict, df_sim, df_real)
        Y.append(score)
    Y = np.array(Y)

    # 3) Analyze
    morris_res = morris_analyze.analyze(
        problem,
        param_values,
        Y,
        conf_level=0.95,
        print_to_console=False
    )

    return morris_res, param_values, Y


def run_sobol_sensitivity(problem: dict,
                          n_samples: int,
                          simulate_func,
                          df_sim: pd.DataFrame,
                          df_real: pd.DataFrame):
    """
    Perform Sobol with SALib on the 'problem' definition.
    """
    param_values = saltelli.sample(
        problem,
        n_samples,
        calc_second_order=True
    )

    Y = []
    param_names = problem["names"]
    for row in param_values:
        p_dict = {param_names[i]: row[i] for i in range(len(param_names))}
        score = simulate_func(p_dict, df_sim, df_real)
        Y.append(score)
    Y = np.array(Y)

    sobol_res = sobol.analyze(
        problem,
        Y,
        calc_second_order=True,
        print_to_console=False
    )
    return sobol_res, param_values, Y

###############################################################################
# 6) MAIN
###############################################################################

def main():
    # Paths to scenario params, sim results, real data
    scenario_dir = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    sim_csv = r"D:\Documents\E_Plus_2030_py\output\results\merged_daily_mean_mocked.csv"
    real_csv = r"D:\Documents\E_Plus_2030_py\output\results\mock_merged_daily_mean.csv"

    # --------------------------------------------------------------------------
    # A) Load scenario parameter data
    # --------------------------------------------------------------------------
    df_params_long = load_scenario_parameter_csvs(scenario_dir)
    print("[INFO] Loaded scenario param rows:", df_params_long.shape)

    # Pivot so each scenario_index => one row
    df_pivot = pivot_scenario_params(df_params_long)
    print("[INFO] Pivoted shape:", df_pivot.shape)
    print(df_pivot.head(5))

    # --------------------------------------------------------------------------
    # B) Build SALib problem, skipping or expanding zero-variance params
    # --------------------------------------------------------------------------
    problem = build_salib_problem_from_pivoted(df_pivot)
    print("\n[SALib Problem Created]")
    print("  num_vars:", problem["num_vars"])
    print("  names:", problem["names"])

    # --------------------------------------------------------------------------
    # C) Load simulation & real data
    # --------------------------------------------------------------------------
    df_sim = load_sim_results(sim_csv)
    df_real = load_real_data(real_csv)

    # --------------------------------------------------------------------------
    # D) Run Morris (or Sobol) Sensitivity
    # --------------------------------------------------------------------------
    n_trajectories = 10
    num_levels = 4

    print("\n=== Running Morris Sensitivity ===")
    morris_res, X_morris, Y_morris = run_morris_sensitivity(
        problem,
        n_trajectories=n_trajectories,
        num_levels=num_levels,
        simulate_func=simulate_scenario_mismatch,
        df_sim=df_sim,
        df_real=df_real
    )

    # Print Morris results
    print("[Morris] mu_star by param:")
    for i, pname in enumerate(problem["names"]):
        mu_star = morris_res["mu_star"][i]
        sigma = morris_res["sigma"][i]
        print(f"  Param: {pname:30s}  mu_star={mu_star:.4f}  sigma={sigma:.4f}")

    # ------------------
    # Optionally run Sobol if desired:
    # n_samples_sobol = 128
    # print("\n=== Running Sobol Sensitivity ===")
    # sobol_res, X_sobol, Y_sobol = run_sobol_sensitivity(
    #     problem,
    #     n_samples=n_samples_sobol,
    #     simulate_func=simulate_scenario_mismatch,
    #     df_sim=df_sim,
    #     df_real=df_real
    # )
    # for i, pname in enumerate(problem["names"]):
    #     st = sobol_res["ST"][i]
    #     print(f"[Sobol] {pname}: ST={st:.4f}")

    print("\n[INFO] Done with sensitivity analysis.")
    print("  - Inspect mu_star (or S1, ST) to see which parameters matter most.")
    print("  - Possibly refine param ranges or fix less influential params.")


if __name__ == "__main__":
    main()

  --- File Contents End ---

  File: man.py
  --- File Contents Start ---
#!/usr/bin/env python3
# man.py

"""
Simple runner script that imports and runs the main function
from main_sensitivity.py.
"""

import cal_2.main_sensitivity

if __name__ == "__main__":
    cal_2.main_sensitivity.main()



#!/usr/bin/env python3
# man.py

import cal_2.train_surrogate

if __name__ == "__main__":
    cal_2.train_surrogate.main()

  --- File Contents End ---

  File: train_surrogate.py
  --- File Contents Start ---
#!/usr/bin/env python3
# train_surrogate.py

"""
Example script to build and save a surrogate model 
based on scenario parameters and a numeric target (e.g., mismatch or total energy).

Requirements:
  pip install pandas numpy scikit-learn joblib
"""

import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import joblib

###############################################################################
# 1) Load scenario parameters
###############################################################################

def load_scenario_parameter_csvs(scenario_dir: str) -> pd.DataFrame:
    """
    Same logic as before: read scenario_params_* CSVs, combine.
    Expect columns: [scenario_index, ogc_fid, param_name, assigned_value].
    """
    dfs = []
    for fname in os.listdir(scenario_dir):
        if fname.startswith("scenario_params_") and fname.endswith(".csv"):
            fpath = os.path.join(scenario_dir, fname)
            df_temp = pd.read_csv(fpath)
            # Minimal check
            req_cols = {"scenario_index", "param_name", "assigned_value"}
            if not req_cols.issubset(df_temp.columns):
                continue
            dfs.append(df_temp)
    if not dfs:
        raise FileNotFoundError("No scenario_params_*.csv found.")
    df_merged = pd.concat(dfs, ignore_index=True)
    return df_merged


def pivot_scenario_params(df_params: pd.DataFrame) -> pd.DataFrame:
    """
    pivot => one row per scenario_index, columns=param_name, values=assigned_value
    """
    df_pivot = df_params.pivot_table(
        index="scenario_index",
        columns="param_name",
        values="assigned_value",
        aggfunc="first"
    ).reset_index()
    df_pivot.columns.name = None
    return df_pivot


###############################################################################
# 2) Load or define the target (mismatch or consumption)
###############################################################################

def load_scenario_targets(target_csv: str) -> pd.DataFrame:
    """
    Suppose we have a CSV that has at least:
       scenario_index, mismatch_value
    If you have a different column name, adjust accordingly.
    """
    df = pd.read_csv(target_csv)
    # E.g. columns = ["scenario_index", "mismatch_value"]
    if "scenario_index" not in df.columns:
        raise ValueError("target_csv must have 'scenario_index' column.")
    if "mismatch_value" not in df.columns:
        raise ValueError("target_csv must have 'mismatch_value' column.")
    return df


###############################################################################
# 3) Merge parameters + target
###############################################################################

def merge_params_and_target(df_params: pd.DataFrame, df_target: pd.DataFrame) -> pd.DataFrame:
    """
    Join on 'scenario_index' so each scenario row has param columns + target column.
    """
    df_merged = pd.merge(df_params, df_target, on="scenario_index", how="inner")
    return df_merged


###############################################################################
# 4) Choose which columns to keep (all or top sensitive)
###############################################################################

def select_parameter_columns(df: pd.DataFrame,
                            top_sensitive: list = None) -> pd.DataFrame:
    """
    If top_sensitive is None => use all param columns except scenario_index + mismatch_value.
    Otherwise, only keep the columns in top_sensitive.
    Return the sub-DataFrame.
    """
    # We'll exclude scenario_index, mismatch_value from the input features
    exclude = {"scenario_index", "mismatch_value"}

    # Figure out param cols
    all_cols = set(df.columns)
    param_cols = [c for c in all_cols if c not in exclude]

    # If a top-sensitive param list is given, keep only those intersection
    if top_sensitive is not None:
        param_cols = [p for p in param_cols if p in top_sensitive]

    # We want a stable column order, so let's sort
    param_cols = sorted(param_cols)

    return df[list(param_cols)]  # reindex

###############################################################################
# 5) Train a surrogate model
###############################################################################

def train_surrogate_model(X: pd.DataFrame, y: pd.Series) -> RandomForestRegressor:
    """
    Example: a random forest regressor. 
    You can replace with other models (XGBRegressor, MLPRegressor, etc.).
    """
    # Some default hyperparams
    rf = RandomForestRegressor(
        n_estimators=50,
        max_depth=10,
        random_state=42
    )
    rf.fit(X, y)
    return rf

###############################################################################
# 6) Evaluate & Save the model
###############################################################################

def evaluate_model(model, X_test, y_test):
    preds = model.predict(X_test)
    r2 = r2_score(y_test, preds)
    mse = mean_squared_error(y_test, preds)
    rmse = np.sqrt(mse)
    return r2, rmse


def save_model(model, output_path: str):
    joblib.dump(model, output_path)
    print(f"[INFO] Saved model to {output_path}")


###############################################################################
# 7) MAIN
###############################################################################

def main():
    scenario_dir = r"D:\Documents\E_Plus_2030_py\output\scenarios"
    target_csv = r"D:\Documents\E_Plus_2030_py\output\results\scenario_mismatch_values.csv"
    model_output_path = "surrogate_rf_model.joblib"

    # 1) Load param data
    df_params_long = load_scenario_parameter_csvs(scenario_dir)
    df_params_pivot = pivot_scenario_params(df_params_long)
    print("[INFO] scenario param pivot shape:", df_params_pivot.shape)
    print(df_params_pivot.head(3))

    # 2) Load target (mismatch or total consumption)
    df_target = load_scenario_targets(target_csv)
    print("[INFO] target shape:", df_target.shape)
    print(df_target.head(3))

    # 3) Merge
    df_merged = merge_params_and_target(df_params_pivot, df_target)
    print("[INFO] merged shape:", df_merged.shape)
    print(df_merged.head(3))

    # 4) Choose param columns
    # Option A: Use all
    # Option B: Use top few from sensitivity, e.g. ["infiltration_base", "occupant_density", "u_value_wall"]
    top_sensitive = None  # or ["infiltration_base", "occupant_density_m2_per_person"]
    X = select_parameter_columns(df_merged, top_sensitive=top_sensitive)

    # 5) The target is mismatch_value
    y = df_merged["mismatch_value"]

    # 6) Train/test split for evaluation
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42
    )

    # 7) Train
    model = train_surrogate_model(X_train, y_train)

    # 8) Evaluate
    r2, rmse = evaluate_model(model, X_test, y_test)
    print(f"[DEBUG] Surrogate performance on test: R^2={r2:.3f}, RMSE={rmse:.3f}")

    # 9) Save the model
    save_model(model, model_output_path)


if __name__ == "__main__":
    main()

  --- File Contents End ---

================================================================================

Folder: D:\Documents\E_Plus_2030_py\aiaia\cal_2\__pycache__

================================================================================

