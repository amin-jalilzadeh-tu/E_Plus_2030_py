{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merged daily mean mocked simulations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/results/merged_daily_mean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m fluctuation_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# Â±20%\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Read the existing CSV\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Ensure 'BuildingID' is numeric\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# This will convert the column to numeric, setting errors='coerce' will turn non-convertible values to NaN\u001b[39;00m\n\u001b[0;32m     15\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuildingID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuildingID\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\New folder (2)\\Anaconda\\DDsaie\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/results/merged_daily_mean.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "input_file = 'output/results/merged_daily_mean.csv'  # Path to your input CSV\n",
    "output_file = 'output/results/merged_daily_mean_mocked.csv'  # Path for the output CSV\n",
    "num_new_buildings = 5  # Number of new BuildingIDs to generate\n",
    "fluctuation_percentage = 0.2  # Â±20%\n",
    "\n",
    "# Read the existing CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure 'BuildingID' is numeric\n",
    "# This will convert the column to numeric, setting errors='coerce' will turn non-convertible values to NaN\n",
    "df['BuildingID'] = pd.to_numeric(df['BuildingID'], errors='coerce')\n",
    "\n",
    "# Check for any NaN values in 'BuildingID' after conversion\n",
    "if df['BuildingID'].isnull().any():\n",
    "    raise ValueError(\"Some BuildingID values could not be converted to numbers. Please check your data.\")\n",
    "\n",
    "# If 'BuildingID' should be integer, convert it\n",
    "df['BuildingID'] = df['BuildingID'].astype(int)\n",
    "\n",
    "# Identify existing BuildingIDs\n",
    "existing_buildings = df['BuildingID'].unique()\n",
    "max_building_id = existing_buildings.max()\n",
    "\n",
    "# Generate new BuildingIDs\n",
    "new_building_ids = range(max_building_id + 1, max_building_id + 1 + num_new_buildings)\n",
    "\n",
    "# Identify date columns (assuming they start from the third column)\n",
    "date_columns = df.columns[2:]\n",
    "\n",
    "# Create a list to hold new rows\n",
    "new_rows = []\n",
    "\n",
    "for new_id in new_building_ids:\n",
    "    for _, row in df.iterrows():\n",
    "        new_row = row.copy()\n",
    "        new_row['BuildingID'] = new_id\n",
    "        # Apply Â±20% fluctuation to each date column\n",
    "        for date in date_columns:\n",
    "            original_value = row[date]\n",
    "            if pd.isna(original_value):\n",
    "                # If the original value is NaN, keep it as NaN\n",
    "                new_value = original_value\n",
    "            else:\n",
    "                # Generate a random fluctuation factor between -20% and +20%\n",
    "                factor = np.random.uniform(1 - fluctuation_percentage, 1 + fluctuation_percentage)\n",
    "                new_value = original_value * factor\n",
    "            new_row[date] = new_value\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Create a DataFrame for new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Append the new rows to the original DataFrame\n",
    "combined_df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# Optionally, sort the DataFrame by BuildingID and VariableName\n",
    "combined_df.sort_values(by=['BuildingID', 'VariableName'], inplace=True)\n",
    "\n",
    "# Save to a new CSV\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Mock data generated and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock data generated and saved to D:\\Documents\\E_Plus_2030_py\\output\\results\\merged_daily_mean_mocked.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "\n",
    "input_file  = r'D:\\Documents\\E_Plus_2030_py\\output\\results\\merged_daily_mean.csv'  # Path to your input CSV\n",
    "output_file = r'D:\\Documents\\E_Plus_2030_py\\output\\results\\merged_daily_mean_mocked.csv'  # Path for the output CSV\n",
    "\n",
    "num_new_buildings = 200  # Number of new BuildingIDs to generate\n",
    "fluctuation_percentage = 0.2  # Â±20%\n",
    "\n",
    "# 1. Read the existing CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FIX: Remove leading/trailing spaces from VariableName\n",
    "# ------------------------------------------------------------------------------\n",
    "df['VariableName'] = df['VariableName'].astype(str).str.strip()\n",
    "\n",
    "# 2. Define a mapping from month abbreviations to numbers\n",
    "month_mapping = {\n",
    "    'Jan': '01',\n",
    "    'Feb': '02',\n",
    "    'Mar': '03',\n",
    "    'Apr': '04',\n",
    "    'May': '05',\n",
    "    'Jun': '06',\n",
    "    'Jul': '07',\n",
    "    'Aug': '08',\n",
    "    'Sep': '09',\n",
    "    'Oct': '10',\n",
    "    'Nov': '11',\n",
    "    'Dec': '12'\n",
    "}\n",
    "\n",
    "# 3. Identify date columns (assuming they start from the third column)\n",
    "date_columns = df.columns[2:]\n",
    "\n",
    "# 4. Function to convert 'DD-MMM' to 'MM/DD'\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        day, month_abbr = date_str.split('-')\n",
    "        month_num = month_mapping.get(month_abbr, '00')  # Default to '00' if month not found\n",
    "        return f\"{month_num}/{day}\"\n",
    "    except ValueError:\n",
    "        # If the format doesn't match, return the original string\n",
    "        return date_str\n",
    "\n",
    "# 5. Rename the date columns\n",
    "new_date_columns = [convert_date_format(col) for col in date_columns]\n",
    "rename_dict = dict(zip(date_columns, new_date_columns))\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Update date_columns to the new names\n",
    "date_columns = new_date_columns\n",
    "\n",
    "# 6. Ensure 'BuildingID' is numeric\n",
    "df['BuildingID'] = pd.to_numeric(df['BuildingID'], errors='coerce')\n",
    "\n",
    "# 7. Check for any NaN values in 'BuildingID' after conversion\n",
    "if df['BuildingID'].isnull().any():\n",
    "    raise ValueError(\"Some BuildingID values could not be converted to numbers. Please check your data.\")\n",
    "\n",
    "# 8. If 'BuildingID' should be integer, convert it\n",
    "df['BuildingID'] = df['BuildingID'].astype(int)\n",
    "\n",
    "# 9. Identify existing BuildingIDs\n",
    "existing_buildings = df['BuildingID'].unique()\n",
    "max_building_id = existing_buildings.max()\n",
    "\n",
    "# 10. Generate new BuildingIDs\n",
    "new_building_ids = range(max_building_id + 1, max_building_id + 1 + num_new_buildings)\n",
    "\n",
    "# 11. Create a list to hold new rows\n",
    "new_rows = []\n",
    "\n",
    "for new_id in new_building_ids:\n",
    "    for _, row in df.iterrows():\n",
    "        new_row = row.copy()\n",
    "        new_row['BuildingID'] = new_id\n",
    "        # Apply Â±20% fluctuation to each date column\n",
    "        for date in date_columns:\n",
    "            original_value = row[date]\n",
    "            if pd.isna(original_value):\n",
    "                # If the original value is NaN, keep it as NaN\n",
    "                new_value = original_value\n",
    "            else:\n",
    "                # Generate a random fluctuation factor between -20% and +20%\n",
    "                factor = np.random.uniform(1 - fluctuation_percentage, 1 + fluctuation_percentage)\n",
    "                new_value = original_value * factor\n",
    "            new_row[date] = new_value\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# 12. Create a DataFrame for new rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# 13. Append the new rows to the original DataFrame\n",
    "combined_df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# 14. Optionally, sort the DataFrame by BuildingID and VariableName\n",
    "combined_df.sort_values(by=['BuildingID', 'VariableName'], inplace=True)\n",
    "\n",
    "# 15. Save to a new CSV\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Mock data generated and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 new sets for each BuildingID. Saved to D:\\Documents\\E_Plus_2030_py\\output\\assigned\\master_parameters_mock.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Input file\n",
    "input_csv = r\"D:\\Documents\\E_Plus_2030_py\\output\\assigned\\master_parameters.csv\" # r'output\\assigned\\master_parameters.csv'\n",
    "# Output file for the new, mocked rows\n",
    "output_csv = r'D:\\Documents\\E_Plus_2030_py\\output\\assigned\\master_parameters_mock.csv'\n",
    "\n",
    "# Number of new âcopiesâ per unique BuildingID\n",
    "N = 200\n",
    "\n",
    "def is_float(val):\n",
    "    \"\"\"Check if a string can be cast to float.\"\"\"\n",
    "    try:\n",
    "        float(val)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Read the original dataset\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# List to accumulate all newly generated rows\n",
    "mocked_rows = []\n",
    "\n",
    "# Get the unique BuildingIDs\n",
    "unique_bldg_ids = df['BuildingID'].unique()\n",
    "\n",
    "for bldg_id in unique_bldg_ids:\n",
    "    # Extract rows for this building ID\n",
    "    subset = df[df['BuildingID'] == bldg_id]\n",
    "\n",
    "    # Generate N new sets of rows for this building\n",
    "    for i in range(1, N+1):\n",
    "        # Define a new building ID\n",
    "        # Option 1: create a string like \"4136730_1\"\n",
    "        # new_bldg_id = f\"{bldg_id}_{i}\"\n",
    "        \n",
    "        # Option 2: just do bldg_id*100 + i (if bldg_id is numeric)\n",
    "        new_bldg_id = bldg_id*100 + i\n",
    "        \n",
    "        for _, row in subset.iterrows():\n",
    "            new_row = row.copy()\n",
    "            \n",
    "            # Assign the new BuildingID\n",
    "            new_row['BuildingID'] = new_bldg_id\n",
    "\n",
    "            # 1) Parse assigned_value, min_value, max_value\n",
    "            assigned_str = str(new_row['assigned_value']).strip()\n",
    "            min_str = str(new_row['min_value']).strip()\n",
    "            max_str = str(new_row['max_value']).strip()\n",
    "\n",
    "            assigned_is_float = is_float(assigned_str)\n",
    "            min_is_float = is_float(min_str)\n",
    "            max_is_float = is_float(max_str)\n",
    "\n",
    "            if assigned_is_float:\n",
    "                assigned_val = float(assigned_str)\n",
    "            else:\n",
    "                assigned_val = assigned_str  # keep as string if not numeric\n",
    "\n",
    "            if min_is_float:\n",
    "                min_val = float(min_str)\n",
    "            else:\n",
    "                min_val = None\n",
    "\n",
    "            if max_is_float:\n",
    "                max_val = float(max_str)\n",
    "            else:\n",
    "                max_val = None\n",
    "\n",
    "            # 2) Check for a valid numeric range\n",
    "            has_valid_range = False\n",
    "            if (min_val is not None and max_val is not None \n",
    "                and min_val < max_val and assigned_is_float):\n",
    "                has_valid_range = True\n",
    "\n",
    "            # 3) Mock the assigned_value\n",
    "            if has_valid_range:\n",
    "                # 60% chance keep same, 40% random in [min_val, max_val]\n",
    "                if np.random.rand() < 0.6:\n",
    "                    new_assigned = assigned_val\n",
    "                else:\n",
    "                    new_assigned = np.random.uniform(min_val, max_val)\n",
    "            else:\n",
    "                # No valid range\n",
    "                if not assigned_is_float:\n",
    "                    # Keep string values as is\n",
    "                    new_assigned = assigned_val\n",
    "                else:\n",
    "                    # assigned_val is numeric\n",
    "                    if assigned_val >= 100:\n",
    "                        # vary by Â±100\n",
    "                        offset = np.random.randint(-100, 101)\n",
    "                        new_assigned = assigned_val + offset\n",
    "                    else:\n",
    "                        # vary by Â±1\n",
    "                        offset = np.random.uniform(-1, 1)\n",
    "                        new_assigned = assigned_val + offset\n",
    "\n",
    "            # Update new_row\n",
    "            new_row['assigned_value'] = new_assigned\n",
    "\n",
    "            # Accumulate\n",
    "            mocked_rows.append(new_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "mocked_df = pd.DataFrame(mocked_rows)\n",
    "\n",
    "# If you want to include original data + new data, uncomment the next line:\n",
    "# mocked_df = pd.concat([df, mocked_df], ignore_index=True)\n",
    "\n",
    "# Write out to CSV\n",
    "mocked_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Generated {N} new sets for each BuildingID. Saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mimicking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new, mimicked data to: D:\\Documents\\E_Plus_2030_py\\output\\results\\mock_merged_daily_mean.csv\n",
      "   BuildingID                                       VariableName  \\\n",
      "0           0                 Cooling:EnergyTransfer [J](Hourly)   \n",
      "1           0                   Electricity:Facility [J](Hourly)   \n",
      "2           0  Environment:Site Diffuse Solar Radiation Rate ...   \n",
      "3           0  Environment:Site Direct Solar Radiation Rate p...   \n",
      "4           0  Environment:Site Outdoor Air Dewpoint Temperat...   \n",
      "5           0  Environment:Site Outdoor Air Drybulb Temperatu...   \n",
      "6           0  Environment:Site Outdoor Air Wetbulb Temperatu...   \n",
      "7           0  Environment:Site Solar Altitude Angle [deg](Da...   \n",
      "8           0  Environment:Site Solar Azimuth Angle [deg](Daily)   \n",
      "9           0                 Heating:EnergyTransfer [J](Hourly)   \n",
      "\n",
      "          01/01         01/02         01/03         01/04         01/05  \\\n",
      "0  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "1  4.790622e+06  1.855840e+06  2.124216e+06  2.175319e+06  4.437611e+06   \n",
      "2  1.047428e+01  2.357845e+01  1.125576e+01  7.368200e+00  1.879355e+01   \n",
      "3  1.250364e+01  3.770362e+01  3.314799e+01  1.796656e+02  1.836193e+02   \n",
      "4  2.425260e-01 -5.285033e-02  4.944331e-01 -3.444218e-01 -2.168687e+00   \n",
      "5  1.653573e+00  3.866074e+00  2.336552e+00  6.133259e-01 -1.635174e+00   \n",
      "6  5.215231e-01  9.386522e-01  1.883936e+00  5.682345e-01 -3.856081e+00   \n",
      "7 -1.254747e+01 -2.957479e+01 -1.291854e+01 -1.337099e+01 -2.977035e+01   \n",
      "8  1.104714e+02  8.804078e+01  2.345312e+02  2.538699e+02  2.313029e+02   \n",
      "9  1.161894e+07  9.868545e+06  3.496298e+06  1.098446e+07  1.264554e+07   \n",
      "\n",
      "          01/06         01/07         01/08  ...         12/22         12/23  \\\n",
      "0  0.000000e+00  0.000000e+00  0.000000e+00  ...  0.000000e+00  0.000000e+00   \n",
      "1  2.069249e+06  4.676777e+06  2.043734e+06  ...  1.826302e+06  1.866410e+06   \n",
      "2  2.167110e+01  2.434756e+01  2.371694e+01  ...  8.976949e+00  2.096836e+01   \n",
      "3  8.910044e+01  2.745687e+00  4.157686e+01  ...  2.481956e+01  4.611470e+01   \n",
      "4 -9.687235e+00 -6.149204e+00 -8.853642e+00  ...  3.909251e+00 -2.252376e+00   \n",
      "5 -5.407892e+00 -4.937338e+00 -2.789582e+00  ...  3.855117e+00  3.577777e+00   \n",
      "6 -2.648048e+00 -2.522109e+00 -2.949200e+00  ...  2.810525e+00  5.347928e-01   \n",
      "7 -1.049162e+01 -1.263783e+01 -2.911926e+01  ... -1.249936e+01 -2.793402e+01   \n",
      "8  2.317928e+02  2.484008e+02  1.098174e+02  ...  2.408107e+02  2.269008e+02   \n",
      "9  1.497391e+07  1.416483e+07  1.340463e+07  ...  1.212708e+07  6.728255e+06   \n",
      "\n",
      "          12/24         12/25         12/26         12/27         12/28  \\\n",
      "0  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "1  2.165455e+06  2.066981e+06  4.907774e+06  2.199801e+06  4.820940e+06   \n",
      "2  1.049531e+01  9.132282e+00  1.323875e+01  1.913482e+01  1.789312e+01   \n",
      "3  1.965875e+00  6.931085e-01  5.209603e-01  6.833404e+00  1.105721e+02   \n",
      "4  6.970224e-01  4.610172e+00  3.438181e+00  3.513257e+00  5.424672e+00   \n",
      "5  2.089405e+00  2.335859e+00  1.183258e+01  9.780083e+00  4.549493e+00   \n",
      "6  7.887385e-01  1.993582e+00  3.882173e+00  3.375633e+00  3.093162e+00   \n",
      "7 -1.283737e+01 -1.127785e+01 -2.739167e+01 -2.968607e+01 -1.209815e+01   \n",
      "8  2.310774e+02  2.566023e+02  2.274631e+02  1.166879e+02  2.270357e+02   \n",
      "9  1.309583e+07  8.482679e+06  4.936239e+06  4.245327e+06  5.054710e+06   \n",
      "\n",
      "          12/29         12/30         12/31  \n",
      "0  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "1  1.981528e+06  4.427365e+06  4.530222e+06  \n",
      "2  2.691249e+00  7.676847e+00  2.193308e+01  \n",
      "3  4.978541e-01  4.423522e+01  2.607794e+01  \n",
      "4  5.427713e+00  2.935750e+00  1.215507e+00  \n",
      "5  7.229694e+00  2.801018e+00  1.826814e+00  \n",
      "6  3.005453e+00  4.555460e+00  1.671769e+00  \n",
      "7 -2.901391e+01 -1.311850e+01 -1.229014e+01  \n",
      "8  2.398418e+02  2.564252e+02  2.355928e+02  \n",
      "9  1.104730e+07  1.180846e+07  6.195016e+06  \n",
      "\n",
      "[10 rows x 367 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def mimic_merged_daily_mean(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    rename_dates: bool = True,\n",
    "    method: str = \"scale\",    # \"scale\" or \"random\"\n",
    "    lower_bound: float = 0.3, # For \"scale\": Â±30â50%\n",
    "    upper_bound: float = 0.5,\n",
    "    seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads `merged_daily_mean.csv`, creates new data that mimics the original,\n",
    "    optionally renames columns like '01-Jan' -> '01/01',\n",
    "    and saves a new CSV file with the modified values.\n",
    "\n",
    "    :param input_path: Full path to the original merged_daily_mean.csv.\n",
    "    :param output_path: Where the new CSV should be written.\n",
    "    :param rename_dates: Whether to attempt converting 'DD-Mmm' -> 'MM/DD' columns.\n",
    "    :param method: \"scale\" to multiply each numeric cell by a random factor; \n",
    "                   \"random\" to generate random values in original minâmax range.\n",
    "    :param lower_bound: Lower bound of the random factor (e.g. 0.3 -> Â±30%).\n",
    "    :param upper_bound: Upper bound of the random factor (e.g. 0.5 -> Â±50%).\n",
    "    :param seed: Random seed for reproducibility.\n",
    "    :return: The new pandas DataFrame.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1. Read the original CSV\n",
    "    df_original = pd.read_csv(input_path)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # FIX: Remove leading/trailing spaces from VariableName to avoid mismatches\n",
    "    # --------------------------------------------------------------------------\n",
    "    df_original[\"VariableName\"] = df_original[\"VariableName\"].astype(str).str.strip()\n",
    "\n",
    "    # 2. Make a copy so we don't overwrite the original\n",
    "    df_new = df_original.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2A. Optionally rename date-like columns from 'DD-Mon' -> 'MM/DD'\n",
    "    # -------------------------------------------------------------------------\n",
    "    if rename_dates:\n",
    "        renamed_cols = {}\n",
    "        for col in df_new.columns:\n",
    "            # Look for 'DD-Mmm' format\n",
    "            match = re.match(r\"^(\\d{2})-(\\w{3})$\", col)\n",
    "            if match:\n",
    "                day_str, month_str = match.groups()\n",
    "                try:\n",
    "                    dt = datetime.strptime(f\"{day_str}-{month_str}-2025\", \"%d-%b-%Y\")\n",
    "                    new_col_name = dt.strftime(\"%m/%d\")  # e.g. 01-Jan -> 01/01\n",
    "                    renamed_cols[col] = new_col_name\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        \n",
    "        # Actually rename the columns in df_new\n",
    "        df_new.rename(columns=renamed_cols, inplace=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. Mimic numeric data\n",
    "    # -------------------------------------------------------------------------\n",
    "    skip_cols = [\"BuildingID\", \"VariableName\"]\n",
    "    cols_to_modify = [\n",
    "        c for c in df_new.columns\n",
    "        if c not in skip_cols\n",
    "    ]\n",
    "\n",
    "    for col in cols_to_modify:\n",
    "        # Convert to numeric (coercing errors to NaN, though hopefully none)\n",
    "        df_new[col] = pd.to_numeric(df_new[col], errors=\"coerce\")\n",
    "\n",
    "        # Skip columns that are entirely NaN or non-numeric\n",
    "        if df_new[col].notna().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Two approaches:\n",
    "        # ---------------------------------------------------\n",
    "        # A) Scale the existing data by a random factor: Â±(30%â50%).\n",
    "        # ---------------------------------------------------\n",
    "        if method == \"scale\":\n",
    "            # +1 or -1 direction\n",
    "            scale_direction = np.random.choice([-1, 1], size=len(df_new))\n",
    "            # random magnitude in [lower_bound..upper_bound]\n",
    "            scale_pct = np.random.uniform(lower_bound, upper_bound, size=len(df_new))\n",
    "            factor = 1 + (scale_direction * scale_pct)\n",
    "            df_new[col] = df_new[col] * factor\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # B) Generate brand-new random data in [min..max] range\n",
    "        # ---------------------------------------------------\n",
    "        elif method == \"random\":\n",
    "            old_min, old_max = df_new[col].min(), df_new[col].max()\n",
    "            if old_min == old_max:\n",
    "                # If there's no range, give a small offset\n",
    "                old_min -= 1.0\n",
    "                old_max += 1.0\n",
    "            df_new[col] = np.random.uniform(old_min, old_max, size=len(df_new))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown method. Choose 'scale' or 'random'.\")\n",
    "\n",
    "    # 4. Save the result\n",
    "    df_new.to_csv(output_path, index=False)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    input_file = r\"D:\\Documents\\E_Plus_2030_py\\output\\results\\merged_daily_mean.csv\"\n",
    "    output_file = r\"D:\\Documents\\E_Plus_2030_py\\output\\results\\mock_merged_daily_mean.csv\"\n",
    "\n",
    "    df_mocked = mimic_merged_daily_mean(\n",
    "        input_path=input_file,\n",
    "        output_path=output_file,\n",
    "        rename_dates=True,         # Will rename '01-Jan' -> '01/01'\n",
    "        method=\"scale\",            # or \"random\"\n",
    "        lower_bound=0.3,\n",
    "        upper_bound=0.5,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    print(\"Saved new, mimicked data to:\", output_file)\n",
    "    print(df_mocked.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDsaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
