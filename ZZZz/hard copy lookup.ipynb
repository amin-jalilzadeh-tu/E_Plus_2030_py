{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Global] geometry_lookup dictionary has been updated with Excel rules.\n",
      "[Global] geometry_lookup dictionary has been updated with Excel rules. (Loaded 32 rules)\n",
      "[Global] Wrote updated geometry lookup to geometry_lookup_autogen.py\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from geomeppy import IDF\n",
    "\n",
    "# Geometry modules\n",
    "from geomz.building import create_building_with_roof_type\n",
    "from geomz.geometry_overrides_from_excel import read_geometry_overrides_excel\n",
    "\n",
    "from geomz.override_geometry_lookup_from_excel import override_geometry_lookup_from_excel\n",
    "from geomz.geometry_lookup import geometry_lookup as default_geometry_lookup\n",
    "\n",
    "# main.py\n",
    "import pprint\n",
    "from geomz.override_geometry_lookup_from_excel import override_geometry_lookup_from_excel\n",
    "from geomz.geometry_lookup import geometry_lookup as default_geometry_lookup\n",
    "# from path.to.your.helper import save_geometry_lookup_to_pyfile  # import your function\n",
    "\n",
    "def save_geometry_lookup_to_pyfile(geometry_lookup, out_py_file=\"geometry_lookup_autogen.py\"):\n",
    "    \"\"\"\n",
    "    Writes the provided geometry_lookup dictionary to a Python file,\n",
    "    with valid Python syntax. That file can then be imported as a module\n",
    "    to reuse or inspect the updated dictionary.\n",
    "\n",
    "    Args:\n",
    "      geometry_lookup (dict): The in-memory dictionary you want to save.\n",
    "      out_py_file (str): The path/name of the Python file to create.\n",
    "    \"\"\"\n",
    "    # Use 'pprint' to get a nicely formatted string representation of the dictionary\n",
    "    pretty_dict_str = pprint.pformat(geometry_lookup, width=120, sort_dicts=False)\n",
    "\n",
    "    # Write that out to a .py file with a variable assignment\n",
    "    with open(out_py_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# This file was auto-generated from Excel overrides.\\n\")\n",
    "        f.write(\"# You can import this file as a Python module to get 'geometry_lookup'.\\n\\n\")\n",
    "        f.write(\"geometry_lookup = \")\n",
    "        f.write(pretty_dict_str)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# ...\n",
    "# after reading excel_rules:\n",
    "override_geom_from_excel = True\n",
    "geom_excel_path = r\"D:\\Documents\\E_Plus_2029_py\\geometry_lookup.xlsx\"\n",
    "\n",
    "excel_rules = read_geometry_overrides_excel(geom_excel_path)\n",
    "override_geometry_lookup_from_excel(default_geometry_lookup, excel_rules)\n",
    "print(\"[Global] geometry_lookup dictionary has been updated with Excel rules.\")\n",
    "\n",
    "\n",
    "# 1) Load Excel & override\n",
    "if override_geom_from_excel and os.path.isfile(geom_excel_path):\n",
    "    excel_rules = read_geometry_overrides_excel(geom_excel_path)\n",
    "    if excel_rules:\n",
    "        override_geometry_lookup_from_excel(default_geometry_lookup, excel_rules)\n",
    "        print(f\"[Global] geometry_lookup dictionary has been updated with Excel rules. (Loaded {len(excel_rules)} rules)\")\n",
    "        # 2) Now save it to a new .py file\n",
    "        out_py = \"geometry_lookup_autogen.py\"\n",
    "        save_geometry_lookup_to_pyfile(default_geometry_lookup, out_py)\n",
    "        print(f\"[Global] Wrote updated geometry lookup to {out_py}\")\n",
    "    else:\n",
    "        print(\"[Global] Excel file found but no valid rows. Using default geometry_lookup.\")\n",
    "else:\n",
    "    print(\"[Global] Using default geometry_lookup (no Excel override).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geometry_lookup dictionary has been successfully written to D:\\Documents\\E_Plus_2029_py\\geomz\\geometry_lookup2.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Path to your Excel file\n",
    "excel_file_path = r\"D:\\Documents\\E_Plus_2029_py\\geometry_lookup.xlsx\"\n",
    "\n",
    "# Read the Excel file\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {excel_file_path} does not exist.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize the main dictionary\n",
    "geometry_lookup = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    building_function = row['building_function']\n",
    "    building_type = row['building_type']\n",
    "    calibration_stage = row['calibration_stage'].strip()  # Remove any leading/trailing whitespace\n",
    "    perimeter_depth_min = row['perimeter_depth_min']\n",
    "    perimeter_depth_max = row['perimeter_depth_max']\n",
    "    has_core_value = row['has_core_value']\n",
    "    \n",
    "    # Convert has_core_value to boolean if it's not already\n",
    "    if isinstance(has_core_value, str):\n",
    "        has_core = has_core_value.strip().lower() == 'true'\n",
    "    else:\n",
    "        has_core = bool(has_core_value)\n",
    "    \n",
    "    # Define the calibration data\n",
    "    calibration_data = {\n",
    "        \"perimeter_depth_range\": (perimeter_depth_min, perimeter_depth_max),  # Using tuple\n",
    "        \"has_core\": has_core\n",
    "    }\n",
    "    \n",
    "    # Assign the calibration data to the appropriate place in the dictionary\n",
    "    geometry_lookup[building_function][building_type][calibration_stage] = calibration_data\n",
    "\n",
    "# Convert defaultdict to regular dict for serialization\n",
    "geometry_lookup = dict(geometry_lookup)\n",
    "\n",
    "# Optionally, sort the dictionary for better readability\n",
    "def sort_dict(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {k: sort_dict(v) for k, v in sorted(d.items())}\n",
    "    return d\n",
    "\n",
    "geometry_lookup = sort_dict(geometry_lookup)\n",
    "\n",
    "# Custom function to serialize the dictionary with double quotes and proper booleans\n",
    "def serialize_dict(d, indent=0):\n",
    "    indent_space = '    ' * indent\n",
    "    if isinstance(d, dict):\n",
    "        items = []\n",
    "        for i, (k, v) in enumerate(d.items()):\n",
    "            key = f'\"{k}\"'\n",
    "            value = serialize_dict(v, indent + 1)\n",
    "            items.append(f'{indent_space}    {key}: {value}')\n",
    "        return \"{\\n\" + \",\\n\".join(items) + f\"\\n{indent_space}}}\"\n",
    "    elif isinstance(d, list):\n",
    "        items = [serialize_dict(item, indent + 1) for item in d]\n",
    "        return \"[ \" + \", \".join(items) + \" ]\"\n",
    "    elif isinstance(d, tuple):\n",
    "        items = \", \".join([serialize_dict(item, indent) for item in d])\n",
    "        return f\"({items})\"\n",
    "    elif isinstance(d, str):\n",
    "        # Escape any existing double quotes in the string\n",
    "        escaped_str = d.replace('\"', '\\\\\"')\n",
    "        return f'\"{escaped_str}\"'\n",
    "    elif isinstance(d, bool):\n",
    "        return \"True\" if d else \"False\"\n",
    "    elif d is None:\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return str(d)\n",
    "\n",
    "# Serialize the dictionary\n",
    "serialized_geometry_lookup = serialize_dict(geometry_lookup, indent=0)\n",
    "\n",
    "# Path to the output Python file\n",
    "output_file_path = r\"D:\\Documents\\E_Plus_2029_py\\geomz\\geometry_lookup2.py\"\n",
    "\n",
    "# Create the content to write\n",
    "output_content = f\"# geomz/geometry_lookup.py\\n\\ngeometry_lookup = {serialized_geometry_lookup}\\n\"\n",
    "\n",
    "# Write the dictionary to the Python file\n",
    "try:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(output_content)\n",
    "    print(f\"geometry_lookup dictionary has been successfully written to {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while writing to the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E envelop Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Sheet Names:\n",
      "1: Sheet1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to your Excel file\n",
    "excel_file_path = r\"D:\\envelop_res2.xlsx\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "# List all sheet names in the Excel file\n",
    "try:\n",
    "    xls = pd.ExcelFile(excel_file_path)\n",
    "    sheet_names = xls.sheet_names\n",
    "    print(\"Available Sheet Names:\")\n",
    "    for idx, sheet in enumerate(sheet_names):\n",
    "        print(f\"{idx + 1}: {sheet}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Column Names:\n",
      "['building_function', 'building_type', 'year_range', 'scenario', 'calibration_stage', 'element', 'area_m2', 'R_value_min', 'R_value_max', 'U_value_min', 'U_value_max', 'roughness', 'material_opaque_lookup', 'material_window_lookup', 'min_wwr', 'max_wwr']\n",
      "\n",
      "Cleaned Column Names:\n",
      "['building_function', 'building_type', 'year_range', 'scenario', 'calibration_stage', 'element', 'area_m2', 'r_value_min', 'r_value_max', 'u_value_min', 'u_value_max', 'roughness', 'material_opaque_lookup', 'material_window_lookup', 'min_wwr', 'max_wwr']\n",
      "\n",
      "First 5 Rows of the DataFrame:\n",
      "  building_function     building_type      year_range   scenario  \\\n",
      "0   Non-Residential  Meeting Function  2015 and later  scenario1   \n",
      "1   Non-Residential  Meeting Function  2015 and later  scenario1   \n",
      "2   Non-Residential  Meeting Function  2015 and later  scenario1   \n",
      "3   Non-Residential  Meeting Function  2015 and later  scenario1   \n",
      "4   Non-Residential  Meeting Function  2015 and later  scenario1   \n",
      "\n",
      "  calibration_stage            element  area_m2 r_value_min r_value_max  \\\n",
      "0   pre_calibration              doors     10.0           -           -   \n",
      "1   pre_calibration       ground_floor     80.0         3.5         3.5   \n",
      "2   pre_calibration  sloping_flat_roof     95.0         3.5         3.5   \n",
      "3   pre_calibration         solid_wall    120.0         3.5         3.5   \n",
      "4   pre_calibration            windows     35.0           -           -   \n",
      "\n",
      "   u_value_min  u_value_max    roughness      material_opaque_lookup  \\\n",
      "0         3.50         3.50  MediumRough             DoorPanel_Range   \n",
      "1         0.28         0.28  MediumRough  GroundContactFloor_Generic   \n",
      "2         0.32         0.32  MediumRough       Ceiling_Insulation_R3   \n",
      "3         0.32         0.32  MediumRough              Concrete_200mm   \n",
      "4         3.50         3.50  MediumRough             DoorPanel_Range   \n",
      "\n",
      "   material_window_lookup  min_wwr  max_wwr  \n",
      "0       Glazing_Clear_3mm     0.20     0.25  \n",
      "1  Glazing_Clear_3mm_Post     0.18     0.18  \n",
      "2       Glazing_Clear_3mm     0.20     0.25  \n",
      "3  Glazing_Clear_3mm_Post     0.18     0.18  \n",
      "4       Glazing_Clear_3mm     0.20     0.25  \n",
      "\n",
      "All required columns are present.\n",
      "\n",
      "residential_materials_data successfully exported to fenez\\data_materials_residential2.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to your Excel file\n",
    "#excel_file_path = r\"D:\\envelop_res2.xlsx\"\n",
    "excel_file_path = r\"D:\\envelop_nonres3.xlsx\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Display the original column names\n",
    "print(\"Original Column Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Clean column names: strip spaces and convert to lowercase\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Display the cleaned column names\n",
    "print(\"\\nCleaned Column Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display the first few rows to inspect the data\n",
    "print(\"\\nFirst 5 Rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check if all required columns are present\n",
    "required_columns = [\n",
    "    'building_function', 'building_type', 'year_range', 'scenario',\n",
    "    'calibration_stage', 'element', 'area_m2', 'r_value_min',\n",
    "    'r_value_max', 'u_value_min', 'u_value_max', 'roughness',\n",
    "    'material_opaque_lookup', 'material_window_lookup',\n",
    "    'min_wwr', 'max_wwr'\n",
    "]\n",
    "\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"\\nError: The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"\\nAll required columns are present.\")\n",
    "\n",
    "# Replace '-' with NaN\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "\n",
    "# Initialize the dictionary\n",
    "residential_materials_data = {}\n",
    "\n",
    "# Group the DataFrame by the desired keys\n",
    "try:\n",
    "    grouped = df.groupby(['building_function', 'building_type', 'year_range', 'scenario', 'calibration_stage'])\n",
    "except KeyError as e:\n",
    "    print(f\"\\nKeyError: {e}. Please check if all required columns are present and correctly named.\")\n",
    "    exit()\n",
    "\n",
    "for group_keys, group_df in grouped:\n",
    "    building_function, building_type, year_range, scenario, calibration_stage = group_keys\n",
    "    \n",
    "    # Define the dictionary key tuple\n",
    "    dict_key = (building_type, year_range, scenario, calibration_stage)\n",
    "    \n",
    "    # Initialize the sub-dictionary\n",
    "    sub_dict = {}\n",
    "    \n",
    "    # Extract 'roughness' - assuming it's consistent within the group\n",
    "    roughness_series = group_df['roughness'].dropna()\n",
    "    roughness = roughness_series.iloc[0] if not roughness_series.empty else None\n",
    "    sub_dict['roughness'] = roughness\n",
    "    \n",
    "    # Extract 'wwr_range' from 'min_wwr' and 'max_wwr'\n",
    "    min_wwr = group_df['min_wwr'].dropna().iloc[0] if not group_df['min_wwr'].dropna().empty else None\n",
    "    max_wwr = group_df['max_wwr'].dropna().iloc[0] if not group_df['max_wwr'].dropna().empty else None\n",
    "    if min_wwr is not None and max_wwr is not None:\n",
    "        sub_dict['wwr_range'] = (float(min_wwr), float(max_wwr))\n",
    "    else:\n",
    "        sub_dict['wwr_range'] = (None, None)\n",
    "    \n",
    "    # Assign top-level material lookups\n",
    "    # Attempt to get from 'ground_floor', else first available\n",
    "    ground_floor_row = group_df[group_df['element'] == 'ground_floor']\n",
    "    if not ground_floor_row.empty:\n",
    "        material_opaque_lookup = ground_floor_row['material_opaque_lookup'].dropna().iloc[0]\n",
    "        material_window_lookup = ground_floor_row['material_window_lookup'].dropna().iloc[0] if not ground_floor_row['material_window_lookup'].dropna().empty else None\n",
    "    else:\n",
    "        # Take from the first row\n",
    "        first_row = group_df.iloc[0]\n",
    "        material_opaque_lookup = first_row['material_opaque_lookup'] if pd.notna(first_row['material_opaque_lookup']) else None\n",
    "        material_window_lookup = first_row['material_window_lookup'] if pd.notna(first_row['material_window_lookup']) else None\n",
    "    \n",
    "    sub_dict['material_opaque_lookup'] = material_opaque_lookup\n",
    "    sub_dict['material_window_lookup'] = material_window_lookup\n",
    "    \n",
    "    # Iterate through each element in the group to add sub-elements\n",
    "    for _, row in group_df.iterrows():\n",
    "        element = row['element']\n",
    "        \n",
    "        # Initialize the element's sub-dictionary\n",
    "        element_dict = {}\n",
    "        \n",
    "        # Assign area_m2\n",
    "        area_m2 = row['area_m2']\n",
    "        if pd.notna(area_m2):\n",
    "            element_dict['area_m2'] = float(area_m2)\n",
    "        else:\n",
    "            element_dict['area_m2'] = None\n",
    "        \n",
    "        # Assign R_value_range if available\n",
    "        R_min = row['r_value_min']\n",
    "        R_max = row['r_value_max']\n",
    "        if pd.notna(R_min) and pd.notna(R_max):\n",
    "            element_dict['R_value_range'] = (float(R_min), float(R_max))\n",
    "        elif pd.notna(R_min):\n",
    "            element_dict['R_value_range'] = (float(R_min), float(R_min))\n",
    "        else:\n",
    "            element_dict['R_value_range'] = (None, None)\n",
    "        \n",
    "        # Assign U_value_range if available\n",
    "        U_min = row['u_value_min']\n",
    "        U_max = row['u_value_max']\n",
    "        if pd.notna(U_min) and pd.notna(U_max):\n",
    "            element_dict['U_value_range'] = (float(U_min), float(U_max))\n",
    "        elif pd.notna(U_min):\n",
    "            element_dict['U_value_range'] = (float(U_min), float(U_min))\n",
    "        else:\n",
    "            element_dict['U_value_range'] = (None, None)\n",
    "        \n",
    "        # Assign material lookups based on the element\n",
    "        material_opaque = row['material_opaque_lookup'] if pd.notna(row['material_opaque_lookup']) else None\n",
    "        material_window = row['material_window_lookup'] if pd.notna(row['material_window_lookup']) else None\n",
    "        \n",
    "        if material_opaque:\n",
    "            element_dict['material_opaque_lookup'] = material_opaque\n",
    "        if material_window:\n",
    "            element_dict['material_window_lookup'] = material_window\n",
    "        \n",
    "        # Assign the element dictionary to the sub_dict\n",
    "        sub_dict[element] = element_dict\n",
    "    \n",
    "    # Assign the sub_dict to the main dictionary\n",
    "    residential_materials_data[dict_key] = sub_dict\n",
    "\n",
    "# Now, residential_materials_data is populated with all the data\n",
    "\n",
    "# Optionally, you can pretty-print the dictionary to verify\n",
    "# import pprint\n",
    "# pprint.pprint(residential_materials_data)\n",
    "\n",
    "# Define the path to save the Python file\n",
    "#output_python_file = r\"fenez\\data_materials_residential2.py\"\n",
    "output_python_file = r\"fenez\\data_materials_residential2.py\"\n",
    "\n",
    "# Prepare the content to write\n",
    "with open(output_python_file, 'w') as f:\n",
    "    f.write(\"residential_materials_data = {\\n\")\n",
    "    for key, value in residential_materials_data.items():\n",
    "        f.write(f\"    {key}: {{\\n\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            if isinstance(sub_value, dict):\n",
    "                # For nested dictionaries\n",
    "                f.write(f\"        \\\"{sub_key}\\\": {{\\n\")\n",
    "                for inner_key, inner_val in sub_value.items():\n",
    "                    if isinstance(inner_val, tuple):\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": {inner_val},\\n\")\n",
    "                    elif isinstance(inner_val, (float, int)):\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": {inner_val},\\n\")\n",
    "                    elif inner_val is None:\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": None,\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": \\\"{inner_val}\\\",\\n\")\n",
    "                f.write(\"        },\\n\")\n",
    "            elif isinstance(sub_value, tuple):\n",
    "                f.write(f\"        \\\"{sub_key}\\\": {sub_value},\\n\")\n",
    "            elif isinstance(sub_value, (float, int)):\n",
    "                f.write(f\"        \\\"{sub_key}\\\": {sub_value},\\n\")\n",
    "            elif sub_value is None:\n",
    "                f.write(f\"        \\\"{sub_key}\\\": None,\\n\")\n",
    "            else:\n",
    "                f.write(f\"        \\\"{sub_key}\\\": \\\"{sub_value}\\\",\\n\")\n",
    "        f.write(\"    },\\n\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"\\nresidential_materials_data successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "material_lookup dictionary successfully exported to fenez\\materials_lookup_generated.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "excel_file_path = r\"materials.xlsx\"\n",
    "output_python_file = r\"fenez\\materials_lookup_generated.py\"\n",
    "\n",
    "# 1) Read the Excel\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Clean columns\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Replace '-' or '' with NaN\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "# Initialize the final dictionary\n",
    "material_lookup = {}\n",
    "\n",
    "def read_range(row, col_min, col_max):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - (val_min, val_max) if at least one is not null,\n",
    "      - None if both are blank.\n",
    "    \"\"\"\n",
    "    # Safety check: only attempt if columns exist\n",
    "    if col_min not in row or col_max not in row:\n",
    "        return None\n",
    "    \n",
    "    val_min = row[col_min]\n",
    "    val_max = row[col_max]\n",
    "    \n",
    "    # If both are NaN -> skip (None return)\n",
    "    if pd.isna(val_min) and pd.isna(val_max):\n",
    "        return None\n",
    "    \n",
    "    # Convert numeric if not None\n",
    "    val_min = float(val_min) if pd.notna(val_min) else None\n",
    "    val_max = float(val_max) if pd.notna(val_max) else None\n",
    "    \n",
    "    # If one is None, it means only the other is set\n",
    "    # You could store (val_min, val_min) if you prefer that behavior\n",
    "    return (val_min, val_max)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # The material name becomes the key\n",
    "    mat_name = str(row['name']).strip()\n",
    "    \n",
    "    mat_dict = {}\n",
    "\n",
    "    # -----------------------\n",
    "    # SINGLE-VALUE PROPERTIES\n",
    "    # -----------------------\n",
    "    # For single columns, only add if not blank.\n",
    "    if 'obj_type' in row and pd.notna(row['obj_type']):\n",
    "        mat_dict['obj_type'] = str(row['obj_type'])\n",
    "\n",
    "    if 'roughness' in row and pd.notna(row['roughness']):\n",
    "        mat_dict['Roughness'] = str(row['roughness'])\n",
    "    \n",
    "    # IR_Transmittance (example numeric field):\n",
    "    if 'ir_transmittance' in row and pd.notna(row['ir_transmittance']):\n",
    "        # Attempt numeric conversion if appropriate\n",
    "        try:\n",
    "            mat_dict['IR_Transmittance'] = float(row['ir_transmittance'])\n",
    "        except ValueError:\n",
    "            mat_dict['IR_Transmittance'] = str(row['ir_transmittance'])\n",
    "    \n",
    "    # Optical_Data_Type (string, e.g. \"SpectralAverage\"):\n",
    "    if 'optical_data_type' in row and pd.notna(row['optical_data_type']):\n",
    "        mat_dict['Optical_Data_Type'] = str(row['optical_data_type'])\n",
    "\n",
    "    # Solar_Diffusing (could be \"No\", \"Yes\", etc.)\n",
    "    if 'solar_diffusing' in row and pd.notna(row['solar_diffusing']):\n",
    "        mat_dict['Solar_Diffusing'] = str(row['solar_diffusing'])\n",
    "\n",
    "    # -----------------------\n",
    "    # RANGE-BASED PROPERTIES\n",
    "    # -----------------------\n",
    "    # Example: Thickness_range\n",
    "    thickness_val = read_range(row, 'thickness_min', 'thickness_max')\n",
    "    if thickness_val is not None:\n",
    "        mat_dict['Thickness_range'] = thickness_val\n",
    "    \n",
    "    # Example: Conductivity_range\n",
    "    conductivity_val = read_range(row, 'conductivity_min', 'conductivity_max')\n",
    "    if conductivity_val is not None:\n",
    "        mat_dict['Conductivity_range'] = conductivity_val\n",
    "    \n",
    "    # Repeat for others: Density, Specific_Heat, etc.\n",
    "    density_val = read_range(row, 'density_min', 'density_max')\n",
    "    if density_val is not None:\n",
    "        mat_dict['Density_range'] = density_val\n",
    "\n",
    "    specific_heat_val = read_range(row, 'specific_heat_min', 'specific_heat_max')\n",
    "    if specific_heat_val is not None:\n",
    "        mat_dict['Specific_Heat_range'] = specific_heat_val\n",
    "\n",
    "    thermal_abs_val = read_range(row, 'thermal_absorptance_min', 'thermal_absorptance_max')\n",
    "    if thermal_abs_val is not None:\n",
    "        mat_dict['Thermal_Absorptance_range'] = thermal_abs_val\n",
    "\n",
    "    solar_abs_val = read_range(row, 'solar_absorptance_min', 'solar_absorptance_max')\n",
    "    if solar_abs_val is not None:\n",
    "        mat_dict['Solar_Absorptance_range'] = solar_abs_val\n",
    "\n",
    "    visible_abs_val = read_range(row, 'visible_absorptance_min', 'visible_absorptance_max')\n",
    "    if visible_abs_val is not None:\n",
    "        mat_dict['Visible_Absorptance_range'] = visible_abs_val\n",
    "\n",
    "    # Example for glazing properties:\n",
    "    solar_trans_val = read_range(row, 'solar_transmittance_min', 'solar_transmittance_max')\n",
    "    if solar_trans_val is not None:\n",
    "        mat_dict['Solar_Transmittance_range'] = solar_trans_val\n",
    "    \n",
    "    front_solar_ref_val = read_range(row, 'front_solar_reflectance_min', 'front_solar_reflectance_max')\n",
    "    if front_solar_ref_val is not None:\n",
    "        mat_dict['Front_Solar_Reflectance_range'] = front_solar_ref_val\n",
    "    \n",
    "    back_solar_ref_val = read_range(row, 'back_solar_reflectance_min', 'back_solar_reflectance_max')\n",
    "    if back_solar_ref_val is not None:\n",
    "        mat_dict['Back_Solar_Reflectance_range'] = back_solar_ref_val\n",
    "\n",
    "    visible_trans_val = read_range(row, 'visible_transmittance_min', 'visible_transmittance_max')\n",
    "    if visible_trans_val is not None:\n",
    "        mat_dict['Visible_Transmittance_range'] = visible_trans_val\n",
    "\n",
    "    front_vis_ref_val = read_range(row, 'front_visible_reflectance_min', 'front_visible_reflectance_max')\n",
    "    if front_vis_ref_val is not None:\n",
    "        mat_dict['Front_Visible_Reflectance_range'] = front_vis_ref_val\n",
    "\n",
    "    back_vis_ref_val = read_range(row, 'back_visible_reflectance_min', 'back_visible_reflectance_max')\n",
    "    if back_vis_ref_val is not None:\n",
    "        mat_dict['Back_Visible_Reflectance_range'] = back_vis_ref_val\n",
    "\n",
    "    front_ir_val = read_range(row, 'front_ir_emissivity_min', 'front_ir_emissivity_max')\n",
    "    if front_ir_val is not None:\n",
    "        mat_dict['Front_IR_Emissivity_range'] = front_ir_val\n",
    "\n",
    "    back_ir_val = read_range(row, 'back_ir_emissivity_min', 'back_ir_emissivity_max')\n",
    "    if back_ir_val is not None:\n",
    "        mat_dict['Back_IR_Emissivity_range'] = back_ir_val\n",
    "\n",
    "    dirt_factor_val = read_range(row, 'dirt_correction_factor_min', 'dirt_correction_factor_max')\n",
    "    if dirt_factor_val is not None:\n",
    "        mat_dict['Dirt_Correction_Factor_range'] = dirt_factor_val\n",
    "    \n",
    "    # Finally, assign to the main dictionary\n",
    "    # Skip entirely if mat_name is blank or if there's no data\n",
    "    if mat_name and mat_dict:\n",
    "        material_lookup[mat_name] = mat_dict\n",
    "\n",
    "# Write to Python file\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# This file is automatically generated. Do not edit manually.\\n\")\n",
    "    f.write(\"material_lookup = {\\n\")\n",
    "    for mat_key, mat_val in material_lookup.items():\n",
    "        f.write(f'    \"{mat_key}\": {{\\n')\n",
    "        for prop_key, prop_val in mat_val.items():\n",
    "            if isinstance(prop_val, tuple):\n",
    "                f.write(f'        \"{prop_key}\": {prop_val},\\n')\n",
    "            elif prop_val is None:\n",
    "                f.write(f'        \"{prop_key}\": None,\\n')\n",
    "            elif isinstance(prop_val, str):\n",
    "                f.write(f'        \"{prop_key}\": \"{prop_val}\",\\n')\n",
    "            else:\n",
    "                f.write(f'        \"{prop_key}\": {prop_val},\\n')\n",
    "        f.write(\"    },\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"material_lookup dictionary successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E Elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEDULE_DEFINITIONS dictionary successfully exported to elec\\schedules_lookup2.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Input Excel and Output Python file\n",
    "# ---------------------------------------------------------\n",
    "excel_file_path = r\"elec_schedules.xlsx\"\n",
    "output_python_file = r\"elec\\schedules_lookup2.py\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Read the Excel, do some checks\n",
    "# ---------------------------------------------------------\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Replace '-' or '' with NaN if needed\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Validate that the essential columns are present\n",
    "# ---------------------------------------------------------\n",
    "required_cols = ['building_category', 'building_subtype', 'day_type', 'start_hour', 'end_hour', 'fraction']\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    print(f\"Error: Missing required columns in the Excel: {missing}\")\n",
    "    exit()\n",
    "\n",
    "# We can fill NaN or skip rows that are incomplete, depending on your preference:\n",
    "df.dropna(subset=required_cols, inplace=True)  # remove rows missing any required col\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Build the SCHEDULE_DEFINITIONS dictionary\n",
    "# ---------------------------------------------------------\n",
    "SCHEDULE_DEFINITIONS = {}\n",
    "\n",
    "# Group by building_category + building_subtype\n",
    "grouped = df.groupby(['building_category', 'building_subtype'])\n",
    "\n",
    "for (category, subtype), group_df in grouped:\n",
    "    # If this category is not yet in the dictionary, initialize it\n",
    "    if category not in SCHEDULE_DEFINITIONS:\n",
    "        SCHEDULE_DEFINITIONS[category] = {}\n",
    "    \n",
    "    # We'll collect schedules for each day_type in a sub_dict\n",
    "    sub_dict = {}\n",
    "    \n",
    "    # Group again by day_type (weekday, weekend, etc.)\n",
    "    for day_type, day_df in group_df.groupby('day_type'):\n",
    "        # We'll build a list of (start_hour, end_hour, fraction)\n",
    "        schedule_list = []\n",
    "        \n",
    "        # Sort day_df by start_hour if you want them in chronological order\n",
    "        day_df = day_df.sort_values(by='start_hour')\n",
    "        \n",
    "        for _, row in day_df.iterrows():\n",
    "            start_hr = float(row['start_hour'])\n",
    "            end_hr   = float(row['end_hour'])\n",
    "            fract    = float(row['fraction'])\n",
    "            \n",
    "            # You can skip rows with fraction=0 or negative hours if needed\n",
    "            # We'll assume all valid if they exist:\n",
    "            schedule_list.append((start_hr, end_hr, fract))\n",
    "        \n",
    "        sub_dict[day_type] = schedule_list\n",
    "    \n",
    "    # Assign to SCHEDULE_DEFINITIONS\n",
    "    SCHEDULE_DEFINITIONS[category][subtype] = sub_dict\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Write to a Python file\n",
    "# ---------------------------------------------------------\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# This file is automatically generated. Do not edit manually.\\n\")\n",
    "    f.write(\"SCHEDULE_DEFINITIONS = {\\n\")\n",
    "    for cat_key, cat_val in SCHEDULE_DEFINITIONS.items():\n",
    "        f.write(f'    \"{cat_key}\": {{\\n')\n",
    "        \n",
    "        for subtype_key, subtype_val in cat_val.items():\n",
    "            f.write(f'        \"{subtype_key}\": {{\\n')\n",
    "            \n",
    "            # subtype_val is a dict of day_types -> list of tuples\n",
    "            for day_key, day_list in subtype_val.items():\n",
    "                f.write(f'            \"{day_key}\": [\\n')\n",
    "                \n",
    "                # Write each tuple, e.g., (0, 6, 0.05)\n",
    "                for sched_tuple in day_list:\n",
    "                    f.write(f'                {sched_tuple},\\n')\n",
    "                \n",
    "                f.write(\"            ],\\n\")  # end of day_key list\n",
    "            f.write(\"        },\\n\")  # end of subtype\n",
    "        f.write(\"    },\\n\")  # end of category\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"SCHEDULE_DEFINITIONS dictionary successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lighting_lookup dictionary successfully exported to Elec\\lighting_lookup_generated.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your Excel file\n",
    "excel_file_path = r\"lighting_lookup.xlsx\"\n",
    "\n",
    "# Path where you want to output the generated .py file\n",
    "output_python_file = r\"Elec\\lighting_lookup_generated.py\"\n",
    "\n",
    "def read_range(row, min_col, max_col):\n",
    "    \"\"\"\n",
    "    Returns a tuple (val_min, val_max) if either is not blank;\n",
    "    returns None if both are blank.\n",
    "    \"\"\"\n",
    "    val_min = row[min_col] if min_col in row and pd.notna(row[min_col]) else None\n",
    "    val_max = row[max_col] if max_col in row and pd.notna(row[max_col]) else None\n",
    "    \n",
    "    if val_min is None and val_max is None:\n",
    "        return None  # skip if both blank\n",
    "    \n",
    "    # Convert to float if not None\n",
    "    val_min = float(val_min) if val_min is not None else None\n",
    "    val_max = float(val_max) if val_max is not None else None\n",
    "    return (val_min, val_max)\n",
    "\n",
    "# 1) Read the Excel file\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Replace '-' or '' with NaN\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "# 2) Build the lighting_lookup dictionary\n",
    "lighting_lookup = {}\n",
    "\n",
    "# We expect each row to have 'scenario', 'building_function', 'building_subtype'\n",
    "# Then the numeric ranges:\n",
    "#   lights_wm2_min / lights_wm2_max\n",
    "#   parasitic_wm2_min / parasitic_wm2_max\n",
    "#   td_min / td_max\n",
    "#   tn_min / tn_max\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for idx, row in df.iterrows():\n",
    "    # Scenario\n",
    "    scenario = str(row['scenario']).strip() if pd.notna(row['scenario']) else None\n",
    "    if not scenario:\n",
    "        continue  # skip if no scenario found\n",
    "    \n",
    "    # Building Function\n",
    "    building_func = str(row['building_function']).strip() if pd.notna(row['building_function']) else None\n",
    "    if not building_func:\n",
    "        continue  # skip if no building_function found\n",
    "    \n",
    "    # Building Subtype\n",
    "    building_subtype = str(row['building_subtype']).strip() if pd.notna(row['building_subtype']) else None\n",
    "    if not building_subtype:\n",
    "        continue  # skip if no building_subtype found\n",
    "    \n",
    "    # Initialize the dictionary placeholders if they don't exist\n",
    "    if scenario not in lighting_lookup:\n",
    "        lighting_lookup[scenario] = {}\n",
    "    if building_func not in lighting_lookup[scenario]:\n",
    "        lighting_lookup[scenario][building_func] = {}\n",
    "    \n",
    "    # Create a sub-dict for this building_subtype\n",
    "    sub_dict = {}\n",
    "    \n",
    "    # read_range for the 4 possible ranges\n",
    "    lw_range = read_range(row, 'lights_wm2_min', 'lights_wm2_max')\n",
    "    if lw_range is not None:\n",
    "        sub_dict['LIGHTS_WM2_range'] = lw_range\n",
    "    \n",
    "    pwr_range = read_range(row, 'parasitic_wm2_min', 'parasitic_wm2_max')\n",
    "    if pwr_range is not None:\n",
    "        sub_dict['PARASITIC_WM2_range'] = pwr_range\n",
    "\n",
    "    td_range = read_range(row, 'td_min', 'td_max')\n",
    "    if td_range is not None:\n",
    "        sub_dict['tD_range'] = td_range\n",
    "\n",
    "    tn_range = read_range(row, 'tn_min', 'tn_max')\n",
    "    if tn_range is not None:\n",
    "        sub_dict['tN_range'] = tn_range\n",
    "    \n",
    "    # Assign this sub_dict if it has anything\n",
    "    if sub_dict:\n",
    "        lighting_lookup[scenario][building_func][building_subtype] = sub_dict\n",
    "\n",
    "# 3) Write the dictionary to a Python file\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\\"\\\"\\\"Automatically generated lighting_lookup. Do not edit manually.\\\"\\\"\\\"\\n\\n\")\n",
    "    f.write(\"lighting_lookup = {\\n\")\n",
    "    \n",
    "    for scenario_key, scenario_val in lighting_lookup.items():\n",
    "        f.write(f'    \"{scenario_key}\": {{\\n')\n",
    "        \n",
    "        for func_key, func_val in scenario_val.items():\n",
    "            f.write(f'        \"{func_key}\": {{\\n')\n",
    "            \n",
    "            for subtype_key, subtype_val in func_val.items():\n",
    "                f.write(f'            \"{subtype_key}\": {{\\n')\n",
    "                \n",
    "                for prop_key, prop_val in subtype_val.items():\n",
    "                    # If it's a tuple (range)\n",
    "                    if isinstance(prop_val, tuple):\n",
    "                        f.write(f'                \"{prop_key}\": {prop_val},\\n')\n",
    "                    else:\n",
    "                        f.write(f'                \"{prop_key}\": {prop_val},\\n')\n",
    "                \n",
    "                f.write(\"            },\\n\")\n",
    "            \n",
    "            f.write(\"        },\\n\")\n",
    "        \n",
    "        f.write(\"    },\\n\")\n",
    "    \n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"lighting_lookup dictionary successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epw_lookup list successfully exported to epw\\epw_lookup_generated.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your Excel file\n",
    "excel_file_path = r\"epw_lookup.xlsx\"\n",
    "\n",
    "# Path where you want to output the generated Python file\n",
    "output_python_file = r\"epw\\epw_lookup_generated.py\"\n",
    "\n",
    "# 1) Read the Excel\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Replace '-' or '' with NaN\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "# 2) Build the epw_lookup list\n",
    "epw_lookup = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Make sure we have a valid file_path\n",
    "    file_path = row['file_path'] if 'file_path' in row and pd.notna(row['file_path']) else None\n",
    "    if not file_path:\n",
    "        # Skip row if no file_path\n",
    "        continue\n",
    "    \n",
    "    # year, lat, lon (optional numeric conversions)\n",
    "    year_val = None\n",
    "    if 'year' in row and pd.notna(row['year']):\n",
    "        # Attempt to cast to int\n",
    "        try:\n",
    "            year_val = int(row['year'])\n",
    "        except ValueError:\n",
    "            pass  # If invalid, stays None or skip\n",
    "    \n",
    "    lat_val = None\n",
    "    if 'lat' in row and pd.notna(row['lat']):\n",
    "        try:\n",
    "            lat_val = float(row['lat'])\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    lon_val = None\n",
    "    if 'lon' in row and pd.notna(row['lon']):\n",
    "        try:\n",
    "            lon_val = float(row['lon'])\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Create a dictionary for this row\n",
    "    epw_entry = {}\n",
    "    epw_entry[\"file_path\"] = str(file_path)\n",
    "    \n",
    "    if year_val is not None:\n",
    "        epw_entry[\"year\"] = year_val\n",
    "    if lat_val is not None:\n",
    "        epw_entry[\"lat\"] = lat_val\n",
    "    if lon_val is not None:\n",
    "        epw_entry[\"lon\"] = lon_val\n",
    "    \n",
    "    # If you have more columns (e.g., \"climate_zone\"), handle them similarly:\n",
    "    # climate_zone = row['climate_zone'] if 'climate_zone' in row and pd.notna(row['climate_zone']) else None\n",
    "    # if climate_zone:\n",
    "    #     epw_entry[\"climate_zone\"] = str(climate_zone)\n",
    "    \n",
    "    # Append to epw_lookup if something is valid\n",
    "    if epw_entry:\n",
    "        epw_lookup.append(epw_entry)\n",
    "\n",
    "# 3) Write the list to a Python file\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# Automatically generated epw_lookup. Do not edit manually.\\n\\n\")\n",
    "    f.write(\"epw_lookup = [\\n\")\n",
    "    \n",
    "    for entry in epw_lookup:\n",
    "        f.write(\"    {\\n\")\n",
    "        for k, v in entry.items():\n",
    "            # If string, wrap in quotes\n",
    "            if isinstance(v, str):\n",
    "                f.write(f'        \"{k}\": \"{v}\",\\n')\n",
    "            else:\n",
    "                f.write(f'        \"{k}\": {v},\\n')\n",
    "        f.write(\"    },\\n\")\n",
    "    \n",
    "    f.write(\"]\\n\")\n",
    "\n",
    "print(f\"epw_lookup list successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ventilation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ventilation_lookup dictionary successfully exported to ventilation\\ventilation_lookup_generated.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "excel_file_path = r\"ventilation_lookup_data.xlsx\"\n",
    "output_python_file = r\"ventilation\\ventilation_lookup_generated.py\"\n",
    "\n",
    "def read_range(row):\n",
    "    \"\"\"\n",
    "    Reads range_min, range_max from a row.\n",
    "    Returns:\n",
    "      - (val_min, val_max) if at least one is not blank,\n",
    "      - None if both are blank.\n",
    "    If only one is present, returns (val, val).\n",
    "    \"\"\"\n",
    "    val_min = row.get('range_min', None)\n",
    "    val_max = row.get('range_max', None)\n",
    "    \n",
    "    # If both are completely blank, return None\n",
    "    if pd.isna(val_min) and pd.isna(val_max):\n",
    "        return None\n",
    "    \n",
    "    # Convert to float if not blank\n",
    "    min_float = float(val_min) if pd.notna(val_min) else None\n",
    "    max_float = float(val_max) if pd.notna(val_max) else None\n",
    "    \n",
    "    # If only one is provided, use the same value for both\n",
    "    if min_float is not None and max_float is None:\n",
    "        max_float = min_float\n",
    "    elif max_float is not None and min_float is None:\n",
    "        min_float = max_float\n",
    "    \n",
    "    return (min_float, max_float)\n",
    "\n",
    "# 1) Read the Excel\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Clean columns\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Replace '-' or '' with NaN\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "# 2) Build the ventilation_lookup dictionary\n",
    "ventilation_lookup = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # scenario\n",
    "    scenario = row.get('scenario', None)\n",
    "    if pd.isna(scenario):\n",
    "        continue\n",
    "    scenario = str(scenario).strip()\n",
    "    \n",
    "    # calibration_stage\n",
    "    stage = row.get('calibration_stage', None)\n",
    "    if pd.isna(stage):\n",
    "        continue\n",
    "    stage = str(stage).strip()\n",
    "    \n",
    "    # param_category (e.g. \"residential_infiltration_range\", \"fan_pressure_range\", etc.)\n",
    "    category = row.get('param_category', None)\n",
    "    if pd.isna(category):\n",
    "        continue\n",
    "    category = str(category).strip()\n",
    "    \n",
    "    # read_range\n",
    "    rng = read_range(row)\n",
    "    if rng is None:\n",
    "        # skip if both min & max blank\n",
    "        continue\n",
    "    \n",
    "    # Create the scenario/stage if not exist\n",
    "    if scenario not in ventilation_lookup:\n",
    "        ventilation_lookup[scenario] = {}\n",
    "    if stage not in ventilation_lookup[scenario]:\n",
    "        ventilation_lookup[scenario][stage] = {}\n",
    "    \n",
    "    # param_subkey (e.g. \"two_and_a_half_story_house\", \"A\", \"1900-2000\", or blank)\n",
    "    subkey = row.get('param_subkey', None)\n",
    "    subkey = None if pd.isna(subkey) else str(subkey).strip()\n",
    "    \n",
    "    # param_nested_key (e.g. \"f_ctrl_range\"), or blank\n",
    "    nested_key = row.get('param_nested_key', None)\n",
    "    nested_key = None if pd.isna(nested_key) else str(nested_key).strip()\n",
    "    \n",
    "    # Access the dictionary for this category (create if not exist)\n",
    "    if category not in ventilation_lookup[scenario][stage]:\n",
    "        # If we suspect its a dictionary of subkeys, we store {} initially\n",
    "        ventilation_lookup[scenario][stage][category] = {}\n",
    "    \n",
    "    # We have 3 possible ways to store the range:\n",
    "    # 1) If subkey is None -> store the tuple directly: ventilation_lookup[scenario][stage][category] = (..,..)\n",
    "    # 2) If subkey is present but nested_key is None -> store the tuple at that subkey\n",
    "    #    ventilation_lookup[scenario][stage][category][subkey] = (.., ..)\n",
    "    # 3) If subkey AND nested_key are present -> store a dictionary { nested_key: (.., ..) }\n",
    "    #    ventilation_lookup[scenario][stage][category][subkey][nested_key] = (.., ..)\n",
    "\n",
    "    if subkey is None:\n",
    "        # Means we directly attach the tuple to param_category\n",
    "        ventilation_lookup[scenario][stage][category] = rng\n",
    "    else:\n",
    "        # We have a subkey, ensure its a dictionary\n",
    "        if not isinstance(ventilation_lookup[scenario][stage][category], dict):\n",
    "            # If category was previously set to a tuple, thats a conflict\n",
    "            # If needed, you could handle or raise an error. Here we just override.\n",
    "            ventilation_lookup[scenario][stage][category] = {}\n",
    "        \n",
    "        if nested_key is None:\n",
    "            # Just store the tuple at [subkey]\n",
    "            ventilation_lookup[scenario][stage][category][subkey] = rng\n",
    "        else:\n",
    "            # We store at [subkey][nested_key] => rng\n",
    "            if subkey not in ventilation_lookup[scenario][stage][category]:\n",
    "                ventilation_lookup[scenario][stage][category][subkey] = {}\n",
    "            \n",
    "            ventilation_lookup[scenario][stage][category][subkey][nested_key] = rng\n",
    "\n",
    "# 3) Write the dictionary to a Python file\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\\"\\\"\\\"Automatically generated ventilation_lookup. Do not edit manually.\\\"\\\"\\\"\\n\\n\")\n",
    "    f.write(\"ventilation_lookup = {\\n\")\n",
    "    \n",
    "    # First-level: scenario\n",
    "    for scenario_key, scenario_val in ventilation_lookup.items():\n",
    "        f.write(f'    \"{scenario_key}\": {{\\n')\n",
    "        \n",
    "        # second-level: calibration_stage\n",
    "        for stage_key, stage_val in scenario_val.items():\n",
    "            f.write(f'        \"{stage_key}\": {{\\n')\n",
    "            \n",
    "            # third-level: param_category (string -> could be dict or tuple)\n",
    "            for cat_key, cat_val in stage_val.items():\n",
    "                f.write(f'            \"{cat_key}\": ')\n",
    "                \n",
    "                # If its a tuple, we can write it directly. If its a dict, we must recurse or do a manual dict format.\n",
    "                if isinstance(cat_val, tuple):\n",
    "                    # e.g. (0.7, 0.8)\n",
    "                    f.write(f\"{cat_val},\\n\")\n",
    "                else:\n",
    "                    # We assume its a dict of subkeys => can be nested\n",
    "                    f.write(\"{\\n\")\n",
    "                    \n",
    "                    for subkey_key, subkey_val in cat_val.items():\n",
    "                        # subkey_val could be a tuple or another dict\n",
    "                        f.write(f'                \"{subkey_key}\": ')\n",
    "                        if isinstance(subkey_val, tuple):\n",
    "                            f.write(f\"{subkey_val},\\n\")\n",
    "                        elif isinstance(subkey_val, dict):\n",
    "                            # e.g. \"A\": {\"f_ctrl_range\": (0.9, 1.0)}\n",
    "                            f.write(\"{\\n\")\n",
    "                            for nk, nk_val in subkey_val.items():\n",
    "                                f.write(f'                    \"{nk}\": {nk_val},\\n')\n",
    "                            f.write(\"                },\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{subkey_val},\\n\")  # fallback\n",
    "                    f.write(\"            },\\n\")\n",
    "            \n",
    "            f.write(\"        },\\n\")\n",
    "        \n",
    "        f.write(\"    },\\n\")\n",
    "    \n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"ventilation_lookup dictionary successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groundtemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtemp_lookup dictionary successfully exported to tempground\\groundtemp_lookup_generated.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your Excel file\n",
    "excel_file_path = r\"groundtemp_lookup.xlsx\"\n",
    "\n",
    "# Path to the Python file you want to generate\n",
    "output_python_file = r\"tempground\\groundtemp_lookup_generated.py\"\n",
    "\n",
    "def read_range(row):\n",
    "    \"\"\"\n",
    "    Reads temp_min, temp_max from the row.\n",
    "    Returns:\n",
    "      - (val_min, val_max) if at least one is not blank,\n",
    "      - None if both are blank.\n",
    "    If only one is provided, uses the same value for both.\n",
    "    \"\"\"\n",
    "    tmin = row.get('temp_min', None)\n",
    "    tmax = row.get('temp_max', None)\n",
    "    \n",
    "    # If both blank => skip\n",
    "    if pd.isna(tmin) and pd.isna(tmax):\n",
    "        return None\n",
    "    \n",
    "    # Convert to float if not None\n",
    "    min_val = float(tmin) if pd.notna(tmin) else None\n",
    "    max_val = float(tmax) if pd.notna(tmax) else None\n",
    "    \n",
    "    # If only one is provided, set both to the same\n",
    "    if min_val is not None and max_val is None:\n",
    "        max_val = min_val\n",
    "    elif max_val is not None and min_val is None:\n",
    "        min_val = max_val\n",
    "    \n",
    "    return (min_val, max_val)\n",
    "\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Clean columns\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Replace '-' or '' with NaN\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "# 1) Build the groundtemp_lookup dictionary\n",
    "groundtemp_lookup = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    scenario = row.get('scenario', None)\n",
    "    month = row.get('month', None)\n",
    "    \n",
    "    if pd.isna(scenario) or pd.isna(month):\n",
    "        continue  # skip if either is blank\n",
    "    \n",
    "    # Convert them to strings\n",
    "    scenario = str(scenario).strip()\n",
    "    month = str(month).strip()\n",
    "    \n",
    "    # read_range for the temperature\n",
    "    temp_tuple = read_range(row)\n",
    "    if temp_tuple is None:\n",
    "        continue  # skip if both temp_min and temp_max are blank\n",
    "    \n",
    "    # Create the scenario dict if not present\n",
    "    if scenario not in groundtemp_lookup:\n",
    "        groundtemp_lookup[scenario] = {}\n",
    "    \n",
    "    # Assign the temperature tuple to that month\n",
    "    groundtemp_lookup[scenario][month] = temp_tuple\n",
    "\n",
    "# 2) Write to a Python file\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\\"\\\"\\\"Automatically generated groundtemp_lookup. Do not edit manually.\\\"\\\"\\\"\\n\\n\")\n",
    "    f.write(\"groundtemp_lookup = {\\n\")\n",
    "    \n",
    "    for scenario_key, months_dict in groundtemp_lookup.items():\n",
    "        f.write(f'    \"{scenario_key}\": {{\\n')\n",
    "        for month_key, temp_val in months_dict.items():\n",
    "            f.write(f'        \"{month_key}\": {temp_val},\\n')\n",
    "        f.write(\"    },\\n\")\n",
    "    \n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"groundtemp_lookup dictionary successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dhw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhw_lookup dictionary successfully exported to DHW\\dhw_lookup_generated.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "excel_file_path = r\"dhw_lookup.xlsx\"\n",
    "output_python_file = r\"DHW\\dhw_lookup_generated.py\"\n",
    "\n",
    "def read_range_or_value(row):\n",
    "    \"\"\"\n",
    "    Checks if there's a single_value present.\n",
    "    Else attempts to read (range_min, range_max).\n",
    "    Returns one of:\n",
    "      - float (if single_value is present),\n",
    "      - tuple (val_min, val_max) if at least one is present,\n",
    "      - None if everything is blank.\n",
    "    \"\"\"\n",
    "    single_val = row.get('single_value', None)\n",
    "    if pd.notna(single_val):\n",
    "        # convert to float\n",
    "        try:\n",
    "            return float(single_val)\n",
    "        except ValueError:\n",
    "            return single_val  # fallback if it's string\n",
    "    # else check range_min / range_max\n",
    "    min_val = row.get('range_min', None)\n",
    "    max_val = row.get('range_max', None)\n",
    "    \n",
    "    if pd.isna(min_val) and pd.isna(max_val):\n",
    "        return None\n",
    "    \n",
    "    # Convert if not None\n",
    "    min_float = float(min_val) if pd.notna(min_val) else None\n",
    "    max_float = float(max_val) if pd.notna(max_val) else None\n",
    "    \n",
    "    # If only one is present, replicate\n",
    "    if min_float is not None and max_float is None:\n",
    "        max_float = min_float\n",
    "    elif max_float is not None and min_float is None:\n",
    "        min_float = max_float\n",
    "    \n",
    "    return (min_float, max_float)\n",
    "\n",
    "# 1) Read Excel\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: {excel_file_path} not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "# 2) Initialize the final dictionary\n",
    "dhw_lookup = {\n",
    "    \"TABLE_13_1_KWH_PER_M2\": {},\n",
    "    \"pre_calibration\": {},\n",
    "    \"post_calibration\": {}\n",
    "}\n",
    "\n",
    "# 3) Populate from Excel\n",
    "for _, row in df.iterrows():\n",
    "    section_type = row.get('section_type', None)\n",
    "    if pd.isna(section_type):\n",
    "        continue\n",
    "    section_type = str(section_type).strip()\n",
    "\n",
    "    # 'key_name': for TABLE_13_1_KWH_PER_M2 => \"Meeting Function\"\n",
    "    # for pre/post_calibration => \"Residential_SingleFamily_Small\", etc.\n",
    "    key_name = row.get('key_name', None)\n",
    "    if pd.isna(key_name):\n",
    "        continue\n",
    "    key_name = str(key_name).strip()\n",
    "\n",
    "    # 'subkey_name': usually the property name if in pre/post_cal, blank if in TABLE_13_1_KWH_PER_M2\n",
    "    subkey_name = row.get('subkey_name', None)\n",
    "    subkey_name = None if pd.isna(subkey_name) else str(subkey_name).strip()\n",
    "    \n",
    "    val = read_range_or_value(row)\n",
    "    if val is None:\n",
    "        # skip blank row\n",
    "        continue\n",
    "    \n",
    "    # Insert into dhw_lookup\n",
    "    if section_type == \"TABLE_13_1_KWH_PER_M2\":\n",
    "        # We expect val to be a single float\n",
    "        # e.g. dhw_lookup[\"TABLE_13_1_KWH_PER_M2\"][\"Meeting Function\"] = 2.8\n",
    "        if not isinstance(val, (float, int)):\n",
    "            # if it's not numeric, store anyway or skip?\n",
    "            pass\n",
    "        dhw_lookup[\"TABLE_13_1_KWH_PER_M2\"][key_name] = val\n",
    "    \n",
    "    elif section_type in (\"pre_calibration\", \"post_calibration\"):\n",
    "        # We expect a dictionary of subkeys at dhw_lookup[\"pre_calibration\"][key_name]\n",
    "        # If that key_name dict doesn't exist, create it\n",
    "        if key_name not in dhw_lookup[section_type]:\n",
    "            dhw_lookup[section_type][key_name] = {}\n",
    "        \n",
    "        # If subkey_name is None => skip or decide what to do\n",
    "        # but in your example, we always have a property name\n",
    "        if subkey_name is None:\n",
    "            # Means we directly store the val in the dictionary?\n",
    "            # E.g. dhw_lookup[\"pre_calibration\"][\"Residential_SingleFamily_Small\"] = (..)\n",
    "            # But typically you want a name, so let's just skip if no subkey_name\n",
    "            continue\n",
    "        \n",
    "        # Store the value\n",
    "        dhw_lookup[section_type][key_name][subkey_name] = val\n",
    "\n",
    "# 4) Write out the dictionary\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# Automatically generated from dhw_lookup_data.xlsx\\n\\n\")\n",
    "    f.write(\"dhw_lookup = {\\n\")\n",
    "    \n",
    "    # A) TABLE_13_1_KWH_PER_M2\n",
    "    f.write('    \"TABLE_13_1_KWH_PER_M2\": {\\n')\n",
    "    for func_key, func_val in dhw_lookup[\"TABLE_13_1_KWH_PER_M2\"].items():\n",
    "        # Numeric single value\n",
    "        f.write(f'        \"{func_key}\": {func_val},\\n')\n",
    "    f.write(\"    },\\n\\n\")\n",
    "    \n",
    "    # B) pre_calibration\n",
    "    f.write('    \"pre_calibration\": {\\n')\n",
    "    for bld_key, bld_val in dhw_lookup[\"pre_calibration\"].items():\n",
    "        f.write(f'        \"{bld_key}\": {{\\n')\n",
    "        for prop_key, prop_val in bld_val.items():\n",
    "            if isinstance(prop_val, tuple):\n",
    "                f.write(f'            \"{prop_key}\": {prop_val},\\n')\n",
    "            elif isinstance(prop_val, (float, int)):\n",
    "                f.write(f'            \"{prop_key}\": {prop_val},\\n')\n",
    "            else:\n",
    "                # fallback if its a string or something else\n",
    "                f.write(f'            \"{prop_key}\": \"{prop_val}\",\\n')\n",
    "        f.write(\"        },\\n\")\n",
    "    f.write(\"    },\\n\\n\")\n",
    "    \n",
    "    # C) post_calibration\n",
    "    f.write('    \"post_calibration\": {\\n')\n",
    "    for bld_key, bld_val in dhw_lookup[\"post_calibration\"].items():\n",
    "        f.write(f'        \"{bld_key}\": {{\\n')\n",
    "        for prop_key, prop_val in bld_val.items():\n",
    "            if isinstance(prop_val, tuple):\n",
    "                f.write(f'            \"{prop_key}\": {prop_val},\\n')\n",
    "            elif isinstance(prop_val, (float, int)):\n",
    "                f.write(f'            \"{prop_key}\": {prop_val},\\n')\n",
    "            else:\n",
    "                f.write(f'            \"{prop_key}\": \"{prop_val}\",\\n')\n",
    "        f.write(\"        },\\n\")\n",
    "    f.write(\"    },\\n\")\n",
    "    \n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"dhw_lookup dictionary successfully exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometry with new intersssssssssssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Added new 'interior_wall' and 'inter_floor' rows where they didn't exist.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Read the original Excel file into a DataFrame\n",
    "# =============================================================================\n",
    "df = pd.read_excel(\"D:/envelop_nonres3.xlsx\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Define the columns that define unique groups\n",
    "#    For each unique combo of these, we'll add new rows\n",
    "# =============================================================================\n",
    "group_cols = [\n",
    "    'building_function',\n",
    "    'building_type',\n",
    "    'year_range',\n",
    "    'scenario',\n",
    "    'calibration_stage'\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# 3) Identify all unique combinations of those columns\n",
    "# =============================================================================\n",
    "unique_groups = df[group_cols].drop_duplicates()\n",
    "\n",
    "# =============================================================================\n",
    "# 4) Define the new elements (rows) you want to add, with default values\n",
    "#    Adjust values as needed for your scenario\n",
    "# =============================================================================\n",
    "new_elements = [\n",
    "    {\n",
    "        'element': 'interior_wall',\n",
    "        'area_m2': 50,                 # Example default area\n",
    "        'R_value_min': 1.5,            # Example default R-value\n",
    "        'R_value_max': 1.5,\n",
    "        'U_value_min': 0.67,\n",
    "        'U_value_max': 0.67,\n",
    "        'roughness': 'MediumRough',\n",
    "        'material_opaque_lookup': 'Gypsum_12mm',\n",
    "        'material_window_lookup': '',  # interior walls usually have no window\n",
    "        'min_wwr': 0.0,                # interior walls usually no fenestration\n",
    "        'max_wwr': 0.0\n",
    "    },\n",
    "    {\n",
    "        'element': 'inter_floor',\n",
    "        'area_m2': 60,                 # Example default area\n",
    "        'R_value_min': 2.0,            # Example default R-value\n",
    "        'R_value_max': 2.0,\n",
    "        'U_value_min': 0.5,\n",
    "        'U_value_max': 0.5,\n",
    "        'roughness': 'MediumRough',\n",
    "        'material_opaque_lookup': 'Concrete_100mm',\n",
    "        'material_window_lookup': '',  # interior floor usually no window\n",
    "        'min_wwr': 0.0,\n",
    "        'max_wwr': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# 5) Build a DataFrame (df_new) with the new rows for each unique group\n",
    "# =============================================================================\n",
    "new_rows = []\n",
    "for _, row in unique_groups.iterrows():\n",
    "    for elem_dict in new_elements:\n",
    "        new_row = {}\n",
    "        # Copy over the group columns\n",
    "        for col in group_cols:\n",
    "            new_row[col] = row[col]\n",
    "        # Add the new element defaults\n",
    "        for key, val in elem_dict.items():\n",
    "            new_row[key] = val\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "df_new = pd.DataFrame(new_rows)\n",
    "\n",
    "# =============================================================================\n",
    "# 6) Check for duplicates using only the identifying columns \n",
    "#    (group_cols + ['element'])\n",
    "#    so we only add rows if that group+element combo does not exist in original\n",
    "# =============================================================================\n",
    "\n",
    "# (a) Create a \"reduced\" version of the original DataFrame to detect duplicates\n",
    "group_cols_with_element = group_cols + ['element']\n",
    "df_reduced = df[group_cols_with_element].drop_duplicates()\n",
    "\n",
    "# (b) Merge df_new with df_reduced on these identifying columns\n",
    "df_merged = pd.merge(\n",
    "    df_new,\n",
    "    df_reduced,\n",
    "    on=group_cols_with_element,\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# (c) Keep only the new rows that aren't found in df_reduced\n",
    "#     (i.e., rows that are 'left_only')\n",
    "df_new_unique = df_merged[df_merged['_merge'] == 'left_only']\n",
    "\n",
    "# Now df_new_unique has only the truly new combos\n",
    "# It retains all the columns from df_new (area_m2, R_value_min, etc.)\n",
    "\n",
    "# =============================================================================\n",
    "# 7) Append these unique new rows to the original DataFrame\n",
    "# =============================================================================\n",
    "df_combined = pd.concat([df, df_new_unique.drop(columns='_merge')], ignore_index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 8) Write the final DataFrame to a new Excel file\n",
    "# =============================================================================\n",
    "df_combined.to_excel(\"D:/envelop_nonres5.xlsx\", index=False)\n",
    "\n",
    "print(\"Done! Added new 'interior_wall' and 'inter_floor' rows where they didn't exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Column Names:\n",
      "['building_function', 'building_type', 'year_range', 'scenario', 'calibration_stage', 'element', 'area_m2', 'R_value_min', 'R_value_max', 'U_value_min', 'U_value_max', 'roughness', 'material_opaque_lookup', 'material_window_lookup', 'min_wwr', 'max_wwr']\n",
      "\n",
      "Cleaned Column Names:\n",
      "['building_function', 'building_type', 'year_range', 'scenario', 'calibration_stage', 'element', 'area_m2', 'r_value_min', 'r_value_max', 'u_value_min', 'u_value_max', 'roughness', 'material_opaque_lookup', 'material_window_lookup', 'min_wwr', 'max_wwr']\n",
      "\n",
      "First 5 Rows of the DataFrame:\n",
      "  building_function building_type year_range   scenario calibration_stage  \\\n",
      "0       residential     Apartment      <1946  scenario1   pre_calibration   \n",
      "1       residential     Apartment      <1946  scenario1  post_calibration   \n",
      "2       residential     Apartment      <1946  scenario1   pre_calibration   \n",
      "3       residential     Apartment      <1946  scenario1  post_calibration   \n",
      "4       residential     Apartment      <1946  scenario1   pre_calibration   \n",
      "\n",
      "         element  area_m2 r_value_min r_value_max  u_value_min  u_value_max  \\\n",
      "0          doors     4.82           -           -         3.40         3.40   \n",
      "1          doors     4.82           -           -         3.40         3.40   \n",
      "2  exterior_wall    19.91        0.19        0.35         2.78         1.92   \n",
      "3  exterior_wall    19.91        0.19        0.35         2.78         1.92   \n",
      "4      flat_roof    57.25        0.22        2.56         2.37         2.56   \n",
      "\n",
      "     roughness material_opaque_lookup material_window_lookup  min_wwr  max_wwr  \n",
      "0  MediumRough        DoorPanel_Range      Glazing_Clear_3mm     0.20     0.25  \n",
      "1  MediumRough        DoorPanel_Range      Glazing_Clear_3mm     0.20     0.25  \n",
      "2  MediumRough         Concrete_200mm      Glazing_Clear_3mm     0.22     0.22  \n",
      "3  MediumRough         Concrete_200mm      Glazing_Clear_3mm     0.22     0.22  \n",
      "4  MediumRough      Insulated_Roof_R5      Glazing_Clear_3mm     0.20     0.25  \n",
      "\n",
      "All required columns are present.\n",
      "\n",
      "Dictionary successfully created and exported to fenez\\data_materials_residential3.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your Excel file\n",
    "excel_file_path = r\"D:\\envelop_res5.xlsx\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file at {excel_file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Display the original column names\n",
    "print(\"Original Column Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Clean column names: strip spaces and convert to lowercase\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Display the cleaned column names\n",
    "print(\"\\nCleaned Column Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display the first few rows to inspect the data\n",
    "print(\"\\nFirst 5 Rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check if all required columns are present\n",
    "required_columns = [\n",
    "    'building_function', 'building_type', 'year_range', 'scenario',\n",
    "    'calibration_stage', 'element', 'area_m2', 'r_value_min',\n",
    "    'r_value_max', 'u_value_min', 'u_value_max', 'roughness',\n",
    "    'material_opaque_lookup', 'material_window_lookup',\n",
    "    'min_wwr', 'max_wwr'\n",
    "]\n",
    "\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"\\nError: The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"\\nAll required columns are present.\")\n",
    "\n",
    "# Replace '-' (or any other placeholder) with NaN\n",
    "df.replace('-', pd.NA, inplace=True)\n",
    "\n",
    "# Initialize the main dictionary\n",
    "residential_materials_data = {}\n",
    "\n",
    "# Group the DataFrame by the primary keys\n",
    "grouped = df.groupby(['building_function', 'building_type', 'year_range', 'scenario', 'calibration_stage'])\n",
    "\n",
    "for group_keys, group_df in grouped:\n",
    "    building_function, building_type, year_range, scenario, calibration_stage = group_keys\n",
    "    \n",
    "    # Dictionary key: (building_type, year_range, scenario, calibration_stage)\n",
    "    dict_key = (building_type, year_range, scenario, calibration_stage)\n",
    "    \n",
    "    # Initialize the sub-dictionary\n",
    "    sub_dict = {}\n",
    "    \n",
    "    #\n",
    "    # 1) Get top-level values (roughness, wwr_range, fallback materials)\n",
    "    #\n",
    "    # We'll use the first row of this group to grab fallback values\n",
    "    first_row = group_df.iloc[0]\n",
    "    \n",
    "    # a) roughness\n",
    "    roughness = first_row['roughness'] if pd.notna(first_row['roughness']) else None\n",
    "    sub_dict['roughness'] = roughness\n",
    "    \n",
    "    # b) wwr_range\n",
    "    min_wwr = first_row['min_wwr'] if pd.notna(first_row['min_wwr']) else None\n",
    "    max_wwr = first_row['max_wwr'] if pd.notna(first_row['max_wwr']) else None\n",
    "    sub_dict['wwr_range'] = (\n",
    "        float(min_wwr) if min_wwr else None,\n",
    "        float(max_wwr) if max_wwr else None\n",
    "    )\n",
    "    \n",
    "    # c) material_opaque_lookup (fallback)\n",
    "    fallback_opaque = first_row['material_opaque_lookup'] if pd.notna(first_row['material_opaque_lookup']) else None\n",
    "    sub_dict['material_opaque_lookup'] = fallback_opaque\n",
    "    \n",
    "    # d) material_window_lookup (fallback)\n",
    "    fallback_window = first_row['material_window_lookup'] if pd.notna(first_row['material_window_lookup']) else None\n",
    "    sub_dict['material_window_lookup'] = fallback_window\n",
    "    \n",
    "    #\n",
    "    # 2) Create entry for each element row in this group\n",
    "    #\n",
    "    for _, row in group_df.iterrows():\n",
    "        element_name = row['element']  # e.g. \"exterior_wall\", \"ground_floor\", etc.\n",
    "        \n",
    "        element_dict = {}\n",
    "        \n",
    "        # area_m2\n",
    "        if pd.notna(row['area_m2']):\n",
    "            element_dict['area_m2'] = float(row['area_m2'])\n",
    "        else:\n",
    "            element_dict['area_m2'] = None\n",
    "        \n",
    "        # R_value_range\n",
    "        R_min = row['r_value_min']\n",
    "        R_max = row['r_value_max']\n",
    "        if pd.notna(R_min) and pd.notna(R_max):\n",
    "            element_dict['R_value_range'] = (float(R_min), float(R_max))\n",
    "        elif pd.notna(R_min):\n",
    "            element_dict['R_value_range'] = (float(R_min), float(R_min))\n",
    "        else:\n",
    "            element_dict['R_value_range'] = (None, None)\n",
    "        \n",
    "        # U_value_range\n",
    "        U_min = row['u_value_min']\n",
    "        U_max = row['u_value_max']\n",
    "        if pd.notna(U_min) and pd.notna(U_max):\n",
    "            element_dict['U_value_range'] = (float(U_min), float(U_max))\n",
    "        elif pd.notna(U_min):\n",
    "            element_dict['U_value_range'] = (float(U_min), float(U_min))\n",
    "        else:\n",
    "            element_dict['U_value_range'] = (None, None)\n",
    "        \n",
    "        # material_opaque_lookup: use rows if present, else fallback\n",
    "        if pd.notna(row['material_opaque_lookup']):\n",
    "            element_dict['material_opaque_lookup'] = row['material_opaque_lookup']\n",
    "        elif fallback_opaque:\n",
    "            element_dict['material_opaque_lookup'] = fallback_opaque\n",
    "        \n",
    "        # material_window_lookup: use rows if present, else fallback\n",
    "        if pd.notna(row['material_window_lookup']):\n",
    "            element_dict['material_window_lookup'] = row['material_window_lookup']\n",
    "        elif fallback_window:\n",
    "            element_dict['material_window_lookup'] = fallback_window\n",
    "        \n",
    "        # Add this elements dict into the sub_dict\n",
    "        sub_dict[element_name] = element_dict\n",
    "    \n",
    "    # Assign the sub_dict to the main dictionary\n",
    "    residential_materials_data[dict_key] = sub_dict\n",
    "\n",
    "#\n",
    "# 3) (Optional) Write out to a .py file\n",
    "#\n",
    "output_python_file = r\"fenez\\data_materials_residential3.py\"\n",
    "with open(output_python_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"residential_materials_data = {\\n\")\n",
    "    for key, value in residential_materials_data.items():\n",
    "        f.write(f\"    {key}: {{\\n\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            if isinstance(sub_value, dict):\n",
    "                # Nested dictionary\n",
    "                f.write(f\"        \\\"{sub_key}\\\": {{\\n\")\n",
    "                for inner_key, inner_val in sub_value.items():\n",
    "                    if isinstance(inner_val, tuple):\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": {inner_val},\\n\")\n",
    "                    elif isinstance(inner_val, (float, int)):\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": {inner_val},\\n\")\n",
    "                    elif inner_val is None:\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": None,\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"            \\\"{inner_key}\\\": \\\"{inner_val}\\\",\\n\")\n",
    "                f.write(\"        },\\n\")\n",
    "            elif isinstance(sub_value, tuple):\n",
    "                f.write(f\"        \\\"{sub_key}\\\": {sub_value},\\n\")\n",
    "            elif isinstance(sub_value, (float, int)):\n",
    "                f.write(f\"        \\\"{sub_key}\\\": {sub_value},\\n\")\n",
    "            elif sub_value is None:\n",
    "                f.write(f\"        \\\"{sub_key}\\\": None,\\n\")\n",
    "            else:\n",
    "                f.write(f\"        \\\"{sub_key}\\\": \\\"{sub_value}\\\",\\n\")\n",
    "        f.write(\"    },\\n\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"\\nDictionary successfully created and exported to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDsaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
